{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iTeJdRXekGJ"
      },
      "source": [
        "### Group ID: HaKaPr \n",
        "### Assignment 03\n",
        "● Submitted by:\n",
        "1. Kanishk Chugh (2004215)<br>\n",
        "2. Prem Swarup (2003318)<br>\n",
        "3. Harshvardhan Gupta (2003310) <br>\n",
        "● Team Code: HaKaPr<br>\n",
        "● Course Instructor: Dr. Satyanath Bhat<br>\n",
        "● Course: Machine Learning (CS-331)<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REUURPBG8Kb6"
      },
      "source": [
        "## Question 1:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4oQRA5r8PcD"
      },
      "source": [
        "##Create a Neural Network class. This class takes in list of Layer objects. It also takes a loss object. You can also allow it to take a seed as input. The seed is used for reproduciblity across runs. Each layer is characterized by its activation function and count of output neuron. Examples of activation include linear (W x + b), sigmoid, tanh etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TFSiLbzSk7U"
      },
      "source": [
        "\n",
        "## Class Neural Network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DX9FqIRRvOiL"
      },
      "outputs": [],
      "source": [
        "class Neural_Network :\n",
        "  # Initialize the neural network with a list of layers and a loss function\n",
        "  def __init__(self, layer_list, loss_fun) :\n",
        "    # Store the list of layers in the network\n",
        "    self.layer_list = layer_list\n",
        "    # Store the loss function used to calculate the loss\n",
        "    self.loss_fun = loss_fun\n",
        "    # Initialize the output of the network as None\n",
        "    self.out = None \n",
        "\n",
        "  # Forward Pass\n",
        "  def forward(self, x, y):\n",
        "    # Store the input to the network as the output\n",
        "    self.out = x\n",
        "    # Loop through each layer in the network\n",
        "    for layer_i in self.layer_list:\n",
        "      # Pass the output from the previous layer as the input to the current layer\n",
        "      self.out = layer_i.forward(self.out)\n",
        "    # Calculate the loss using the loss function and the final output of the network\n",
        "    self.loss = self.loss_fun.forward(self.out, y)\n",
        "    # Return the calculated loss\n",
        "    return self.loss\n",
        "\n",
        "  # Backward Pass\n",
        "  def backward(self):\n",
        "    # Calculate the gradient with respect to the loss using the loss function\n",
        "    self.grad = self.loss_fun.backward()\n",
        "    # Loop through the layers in the network in reverse order\n",
        "    for i in reversed(range(len(self.layer_list))):\n",
        "      # Pass the gradient from the previous layer as the input to the current layer\n",
        "      self.grad = self.layer_list[i].backward(self.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPn-0DM2SZkj"
      },
      "source": [
        "### Dense Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NKWawvujSfzE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Dense():\n",
        "    def __init__(self, input_shape, output_size, learning_rate, activation=None, seed_W=None, seed_bias=None):\n",
        "        self.weights = np.random.randn(output_size, input_shape[0])\n",
        "        self.bias = np.random.randn(output_size, input_shape[1])\n",
        "        if(seed_W is not None):\n",
        "           self.weights = seed_W\n",
        "        if(seed_bias is not None):\n",
        "           self.bias = seed_bias\n",
        "        # Activation function used in the dense layer\n",
        "        self.activation = activation\n",
        "        # Learning rate used in the dense layer\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    # Forward Pass for the Dense Layer\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        # Calculate the output of the dense layer by dot product of weights and input, then adding the biases\n",
        "        self.output = np.dot(self.weights, self.input) + self.bias\n",
        "        # If an activation function is provided, apply it to the output\n",
        "        if(self.activation is not None):\n",
        "          self.output = self.activation.forward(self.output)\n",
        "        # Return the final output\n",
        "        return self.output\n",
        "\n",
        "    # Backward Pass for the Dense Layer\n",
        "    def backward(self, output_gradient):\n",
        "        # If an activation function is provided, apply the backward pass to it\n",
        "        if(self.activation is not None):\n",
        "          output_gradient = self.activation.backward(output_gradient)\n",
        "        # Calculate the gradient with respect to the input\n",
        "        self.d_out = np.dot(self.weights.T, output_gradient)\n",
        "        # Calculate the gradient with respect to the weights\n",
        "        weights_gradient = np.dot(output_gradient, self.input.T)\n",
        "        self.weights -= self.learning_rate * weights_gradient\n",
        "        # Update the biases using the calculated gradient and the learning rate\n",
        "        self.bias -= self.learning_rate * output_gradient\n",
        "        # Return the gradient with respect to the input\n",
        "        return self.d_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k64YT_Ai0Qok"
      },
      "source": [
        "## Linear Activation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "F1ZfcaGwKo_e"
      },
      "outputs": [],
      "source": [
        "class linear :\n",
        "  # Initialize the alpha value for linear activation\n",
        "  def __init__(self, alpha) :\n",
        "    self.alpha = alpha\n",
        "    pass\n",
        "\n",
        "  # Calculate the forward pass of linear activation\n",
        "  def forward(self, x) :\n",
        "    return self.alpha*x\n",
        "\n",
        "  # Calculate the backward pass of linear activation\n",
        "  def backward(self, y) :\n",
        "    return self.alpha*y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP6S3tkyhbYn"
      },
      "source": [
        "## Mean Square Loss Layer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lK1tiODvhd1n"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "class MeanSquaredLossLayer:\n",
        "  # Initialize y_pred and y_true to None\n",
        "    def __init__(self):\n",
        "        self.y_pred = None\n",
        "        self.y_true = None\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        # Store the values of y_pred and y_true\n",
        "        self.y_pred = y_pred\n",
        "        self.y_true = y_true\n",
        "        # Calculate the mean squared loss\n",
        "        loss = np.mean((y_pred - y_true)**2)\n",
        "        return loss\n",
        "\n",
        "    def backward(self):\n",
        "        # Calculate the gradient of the loss with respect to the prediction\n",
        "        m = self.y_true.shape[1]\n",
        "        d_L = (self.y_pred - self.y_true) / m\n",
        "        return d_L"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqRGgsHm0egs"
      },
      "source": [
        "## Softmax Layer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "u-LyPJWxiEEO"
      },
      "outputs": [],
      "source": [
        "class SoftmaxLayer():\n",
        "    def __init__(self) :\n",
        "      pass\n",
        "    \n",
        "    def forward(self, input):\n",
        "        # Subtracting the maximum value from the input to avoid numeric instability\n",
        "        mx = np.max(input, axis=0)\n",
        "        tmp = input - mx \n",
        "        \n",
        "        # Taking exponential of the input tensor\n",
        "        tmp = np.exp(tmp)\n",
        "        # Dividing the exponential tensor by the sum of exponential along the axis 0\n",
        "        self.output = tmp / np.sum(tmp, axis=0)\n",
        "        return self.output\n",
        "    \n",
        "    def backward(self, output_gradient):\n",
        "        n = len(self.output)\n",
        "        if(self.output.shape[1]>1) :\n",
        "          # Reshaping the output tensor to be of shape (number of samples, number of features)\n",
        "          self.output = self.output.reshape(self.output.shape[1], n, 1)\n",
        "          d_out = np.ndarray((self.output.shape[0], n))\n",
        "          for i in range(len(self.output)) :\n",
        "             # Tile the output tensor to be of shape (number of features, number of features)\n",
        "            tmp = np.tile(self.output[i], n)\n",
        "            d_out[i] = np.dot(tmp*(np.identity(n) - tmp.T), output_gradient[:, i:i+1]).reshape(-1)\n",
        "          return d_out.T\n",
        "        \n",
        "        # Tile the output tensor to be of shape (number of features, number of features)\n",
        "        tmp = np.tile(self.output, n)\n",
        "        return np.dot(tmp * (np.identity(n) - tmp.T), output_gradient)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7FTQHtq0hJo"
      },
      "source": [
        "## Cross-Entropy Loss Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VHo_Cn2CJW_M"
      },
      "outputs": [],
      "source": [
        "\n",
        "# The cross-entropy loss is defined as:\n",
        "\n",
        "# L = - ∑ yi * log(pi)\n",
        "\n",
        "# Where L is the loss, yi is the true label for the ith example and pi is the predicted probability of the true label for the ith example.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class CrossEntropyLayer:\n",
        "    def __init__(self):\n",
        "        self.CE_out = None\n",
        "        self.epsln = 10**(-10)\n",
        "        self.Y_true = None\n",
        "        self.s_out = None\n",
        "\n",
        "    #Forward Pass\n",
        "    def forward(self, s_out, Y_true):\n",
        "        self.Y_true = Y_true\n",
        "        self.s_out = s_out\n",
        "        self.CE_out = 0\n",
        "        \n",
        "        for i in range(len(self.Y_true)) :\n",
        "          # If true label is 0, add -log(1-pi) to the loss\n",
        "          if(Y_true[i][0]==0) :\n",
        "            self.CE_out += -1*np.log(1-self.s_out[i][0]+self.epsln)\n",
        "          else :\n",
        "            # If true label is 1, add -log(pi) to the loss\n",
        "            self.CE_out += -1*np.log(self.s_out[i][0]+self.epsln)\n",
        "\n",
        "        return self.CE_out\n",
        "\n",
        "    # Backward Pass\n",
        "    def backward(self):\n",
        "        L_out  = np.zeros((self.s_out.shape[0],1))\n",
        "        if(self.s_out.shape[1]>1) :\n",
        "          for j in range(self.s_out.shape[1]) :\n",
        "            for i in range(len(self.Y_true)) :\n",
        "              if(self.Y_true[i][j]==0) :\n",
        "                L_out[i][0] = 1/(1-self.s_out[i][j]+self.epsln)\n",
        "              else :\n",
        "                L_out[i][0] = -1/(self.s_out[i][j]+ self.epsln)\n",
        "          return L_out/self.s_out.shape[1]\n",
        "          \n",
        "        for i in range(len(self.Y_true)) :\n",
        "          if(self.Y_true[i][0]==0) :\n",
        "            L_out[i][0] = 1/(1-self.s_out[i][0]+self.epsln)\n",
        "          else :\n",
        "            L_out[i][0] = -1/(self.s_out[i][0]+self.epsln)\n",
        "\n",
        "        return L_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhZxOpTwWbvv"
      },
      "source": [
        "## Sigmoid Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zPS-e8rCWbbY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # code for sigmoid function\n",
        "        self.x = x\n",
        "        self.out = 1/(1+np.exp(-1*x))\n",
        "        return self.out\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "      \n",
        "        # grad_output is the gradient of the loss with respect to the output activations\n",
        "        return grad_output * self.out * (1 - self.out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18lYPWS4XvxO"
      },
      "source": [
        "### Tanh Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "JPi9j6mjXykS"
      },
      "outputs": [],
      "source": [
        "class Tanh:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return np.tanh(x)\n",
        "    \n",
        "    def backward(self, output_gradient):\n",
        "        x_gradient = output_gradient * (1 - np.tanh(self.x)**2)\n",
        "        return x_gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh2IPf6qlxiD"
      },
      "source": [
        "### Q.2 1st "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vSeyFjHmorZ"
      },
      "source": [
        "### Standardisation of the Input data code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jz7k0jIlmn6N"
      },
      "outputs": [],
      "source": [
        "# Function to standardize the features of a dataset. \n",
        "# It subtracts the mean and divides by the standard deviation of each feature\n",
        "def standardize(X):\n",
        "    # Calculate the mean of each feature\n",
        "    mean = np.mean(X, axis=0)\n",
        "\n",
        "    # Calculate the standard deviation of each feature\n",
        "    std = np.std(X, axis=0)\n",
        "\n",
        "    # Subtract the mean from each feature and divide by the standard deviation\n",
        "    X_standardized = (X - mean) / std\n",
        "\n",
        "    return X_standardized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb0EQ6Uh0vca"
      },
      "source": [
        "### Loading the Boston Data Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x157s524l0_L"
      },
      "outputs": [],
      "source": [
        "## Boston Data Load\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "target = raw_df.values[1::2, 2]\n",
        "\n",
        "# Standardizing data \n",
        "data = standardize(data)\n",
        "# Testing and Training data split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data, target, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d16tK0m109zF"
      },
      "source": [
        "### Testing the training data set code which will calculate the loss for the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqgFspM4aD_w"
      },
      "outputs": [],
      "source": [
        "def testing(X_test, Y_test, layer_list):\n",
        "  loss = 0\n",
        "  # Loop over each test example\n",
        "  for i in range (len(X_test)):\n",
        "    y_pred = X_test[i:i+1].T\n",
        "    # Propagate the input through all the layers in the list\n",
        "    for layer_i in layer_list:\n",
        "      y_pred = layer_i.forward(y_pred)\n",
        "    # Calculate the absolute difference between the predicted and true output and add it to the total loss\n",
        "    loss += abs(y_pred[0][0] - Y_test[i]) / (Y_test[i])\n",
        "    # Divide the total loss by the number of test examples to get the average loss\n",
        "  return loss / (len(Y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPDW2G6g8tmD"
      },
      "source": [
        "## Question 2a: just one output neural with linear activation and least mean square loss.(This is linear regression)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36cWMRhsoWZ5",
        "outputId": "6865d843-9e62-4974-df0d-3ed81516e2f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 0, loss: 297.18643482065465\n",
            "Iteration: 1000, loss: 1048.377572516459\n",
            "Iteration: 2000, loss: 1183.1733691181419\n",
            "Iteration: 3000, loss: 473.19028425258375\n",
            "Iteration: 4000, loss: 461.45101419948185\n",
            "Iteration: 5000, loss: 585.3897088292057\n",
            "Iteration: 6000, loss: 683.3509217903048\n",
            "Iteration: 7000, loss: 277.93370165630074\n",
            "Iteration: 8000, loss: 169.7853032105024\n",
            "Iteration: 9000, loss: 134.02821169251743\n",
            "Iteration: 10000, loss: 193.59017258855405\n",
            "Iteration: 11000, loss: 326.31556477534303\n",
            "Iteration: 12000, loss: 287.6466286008161\n",
            "Iteration: 13000, loss: 506.3578685701268\n",
            "Iteration: 14000, loss: 279.9374961460736\n",
            "Iteration: 15000, loss: 343.97676891856327\n",
            "Iteration: 16000, loss: 126.30219541417836\n",
            "Iteration: 17000, loss: 157.9081476868801\n",
            "Iteration: 18000, loss: 296.516015993371\n",
            "Iteration: 19000, loss: 358.20935934014585\n",
            "Iteration: 20000, loss: 217.96213145502992\n",
            "Iteration: 21000, loss: 674.2130931989757\n",
            "Iteration: 22000, loss: 330.4309063771777\n",
            "Iteration: 23000, loss: 63.010686362737616\n",
            "Iteration: 24000, loss: 212.62322787109883\n",
            "Iteration: 25000, loss: 417.5485488882348\n",
            "Iteration: 26000, loss: 191.27050815985535\n",
            "Iteration: 27000, loss: 93.94810037970208\n",
            "Iteration: 28000, loss: 82.66423905073161\n",
            "Iteration: 29000, loss: 153.12920932629967\n",
            "Iteration: 30000, loss: 154.06793391008566\n",
            "Iteration: 31000, loss: 366.91118177132756\n",
            "Iteration: 32000, loss: 216.85749252164\n",
            "Iteration: 33000, loss: 156.3827991129645\n",
            "Iteration: 34000, loss: 323.5609655750722\n",
            "Iteration: 35000, loss: 173.9327729364604\n",
            "Iteration: 36000, loss: 112.09530189098916\n",
            "Iteration: 37000, loss: 333.5891287635832\n",
            "Iteration: 38000, loss: 43.20048811245223\n",
            "Iteration: 39000, loss: 266.076972094141\n",
            "Iteration: 40000, loss: 157.52143675961264\n",
            "Iteration: 41000, loss: 117.32449492345879\n",
            "Iteration: 42000, loss: 106.30774509531142\n",
            "Iteration: 43000, loss: 105.82862149144624\n",
            "Iteration: 44000, loss: 699.472534248931\n",
            "Iteration: 45000, loss: 85.32434924467684\n",
            "Iteration: 46000, loss: 254.09502981458\n",
            "Iteration: 47000, loss: 131.0037361615033\n",
            "Iteration: 48000, loss: 141.05706877385612\n",
            "Iteration: 49000, loss: 106.50582775837812\n",
            "Iteration: 50000, loss: 263.800833435218\n",
            "Iteration: 51000, loss: 314.98565843352435\n",
            "Iteration: 52000, loss: 184.75783796207975\n",
            "Iteration: 53000, loss: 110.94209453849547\n",
            "Iteration: 54000, loss: 734.8021164903048\n",
            "Iteration: 55000, loss: 77.43659920731886\n",
            "Iteration: 56000, loss: 178.61543781179049\n",
            "Iteration: 57000, loss: 20.122200487744525\n",
            "Iteration: 58000, loss: 244.4101676278607\n",
            "Iteration: 59000, loss: 85.65954283620795\n",
            "Iteration: 60000, loss: 301.98646517624417\n",
            "Iteration: 61000, loss: 110.33382832714591\n",
            "Iteration: 62000, loss: 81.3803002941063\n",
            "Iteration: 63000, loss: 70.15817923634526\n",
            "Iteration: 64000, loss: 15.245221582299887\n",
            "Iteration: 65000, loss: 79.36217983426835\n",
            "Iteration: 66000, loss: 29.579815667093616\n",
            "Iteration: 67000, loss: 65.29276907493278\n",
            "Iteration: 68000, loss: 145.36499308600915\n",
            "Iteration: 69000, loss: 488.1187625887227\n",
            "Iteration: 70000, loss: 79.08708790399287\n",
            "Iteration: 71000, loss: 25.0375883258674\n",
            "Iteration: 72000, loss: 51.06675720821806\n",
            "Iteration: 73000, loss: 38.63427621856051\n",
            "Iteration: 74000, loss: 915.3099773897743\n",
            "Iteration: 75000, loss: 63.06655712794643\n",
            "Iteration: 76000, loss: 252.2151090889837\n",
            "Iteration: 77000, loss: 95.13950466251241\n",
            "Iteration: 78000, loss: 227.0175390696974\n",
            "Iteration: 79000, loss: 44.899479020898944\n",
            "Iteration: 80000, loss: 107.12510429772027\n",
            "Iteration: 81000, loss: 71.07198530016878\n",
            "Iteration: 82000, loss: 96.83642612715545\n",
            "Iteration: 83000, loss: 483.2023629526619\n",
            "Iteration: 84000, loss: 119.78672364315142\n",
            "Iteration: 85000, loss: 42.147583703769335\n",
            "Iteration: 86000, loss: 95.72693359484332\n",
            "Iteration: 87000, loss: 163.31332945682567\n",
            "Iteration: 88000, loss: 68.33291811852612\n",
            "Iteration: 89000, loss: 53.713066354808355\n",
            "Iteration: 90000, loss: 81.13264727556405\n",
            "Iteration: 91000, loss: 63.76343602286083\n",
            "Iteration: 92000, loss: 64.94905641250944\n",
            "Iteration: 93000, loss: 12.328419653698083\n",
            "Iteration: 94000, loss: 10.449885312513173\n",
            "Iteration: 95000, loss: 681.6472327073803\n",
            "Iteration: 96000, loss: 446.17986875828694\n",
            "Iteration: 97000, loss: 2.8736540603900274\n",
            "Iteration: 98000, loss: 171.61647596713897\n",
            "Iteration: 99000, loss: 30.35839756909831\n",
            "Iteration: 100000, loss: 318.80623463297286\n",
            "Iteration: 101000, loss: 23.9844610326292\n",
            "Iteration: 102000, loss: 16.623560472426824\n",
            "Iteration: 103000, loss: 2.380294738618098\n",
            "Iteration: 104000, loss: 50.56364899877028\n",
            "Iteration: 105000, loss: 45.96669484651374\n",
            "Iteration: 106000, loss: 26.804215058968154\n",
            "Iteration: 107000, loss: 47.89809439858167\n",
            "Iteration: 108000, loss: 19.039452830735453\n",
            "Iteration: 109000, loss: 66.27093079240335\n",
            "Iteration: 110000, loss: 63.65996630051609\n",
            "Iteration: 111000, loss: 107.63587485196767\n",
            "Iteration: 112000, loss: 93.35794458074182\n",
            "Iteration: 113000, loss: 5.242668057362503\n",
            "Iteration: 114000, loss: 2.1250627580693466\n",
            "Iteration: 115000, loss: 8.957830110655403\n",
            "Iteration: 116000, loss: 79.34313754920554\n",
            "Iteration: 117000, loss: 40.953074802818584\n",
            "Iteration: 118000, loss: 27.692661947453775\n",
            "Iteration: 119000, loss: 3.840700044967408\n",
            "Iteration: 120000, loss: 14.773768196927898\n",
            "Iteration: 121000, loss: 33.535541476617915\n",
            "Iteration: 122000, loss: 2.9800014391094964\n",
            "Iteration: 123000, loss: 48.250423440591\n",
            "Iteration: 124000, loss: 4.45992558477606\n",
            "Iteration: 125000, loss: 19.901354012883623\n",
            "Iteration: 126000, loss: 25.25146993213058\n",
            "Iteration: 127000, loss: 38.313931337511974\n",
            "Iteration: 128000, loss: 5.114382888361163\n",
            "Iteration: 129000, loss: 8.830311005437219\n",
            "Iteration: 130000, loss: 19.20606494183454\n",
            "Iteration: 131000, loss: 31.684339785312666\n",
            "Iteration: 132000, loss: 75.81533606918998\n",
            "Iteration: 133000, loss: 42.74607386554265\n",
            "Iteration: 134000, loss: 688.8496075395826\n",
            "Iteration: 135000, loss: 124.39350948090653\n",
            "Iteration: 136000, loss: 21.127771512159633\n",
            "Iteration: 137000, loss: 1.9638191656899409\n",
            "Iteration: 138000, loss: 1.5614661346481897\n",
            "Iteration: 139000, loss: 53.257873708823446\n",
            "Iteration: 140000, loss: 288.2269645360653\n",
            "Iteration: 141000, loss: 40.79002170285365\n",
            "Iteration: 142000, loss: 29.69158945890323\n",
            "Iteration: 143000, loss: 69.71703726799393\n",
            "Iteration: 144000, loss: 43.89948830689337\n",
            "Iteration: 145000, loss: 6.695362896341479\n",
            "Iteration: 146000, loss: 0.013030412299962582\n",
            "Iteration: 147000, loss: 372.0431474616123\n",
            "Iteration: 148000, loss: 38.38363989488678\n",
            "Iteration: 149000, loss: 1.0741663591713555\n",
            "Iteration: 150000, loss: 90.73307742366138\n",
            "Iteration: 151000, loss: 75.00532173136611\n",
            "Iteration: 152000, loss: 32.66361267927962\n",
            "Iteration: 153000, loss: 0.9972253546914469\n",
            "Iteration: 154000, loss: 1.600808928294051\n",
            "Iteration: 155000, loss: 9.383909457619135\n",
            "Iteration: 156000, loss: 20.060586814188195\n",
            "Iteration: 157000, loss: 216.25165172179348\n",
            "Iteration: 158000, loss: 5.0723249096086755\n",
            "Iteration: 159000, loss: 4.201231217004841\n",
            "Iteration: 160000, loss: 5.623295844756495\n",
            "Iteration: 161000, loss: 1.3081986503875576\n",
            "Iteration: 162000, loss: 11.898795241190681\n",
            "Iteration: 163000, loss: 4.919834573383176\n",
            "Iteration: 164000, loss: 29.085866268757428\n",
            "Iteration: 165000, loss: 0.013963035564092632\n",
            "Iteration: 166000, loss: 220.64856217011345\n",
            "Iteration: 167000, loss: 1.1328654196763628\n",
            "Iteration: 168000, loss: 16.35612120949822\n",
            "Iteration: 169000, loss: 67.4991188482422\n",
            "Iteration: 170000, loss: 9.631011273373554\n",
            "Iteration: 171000, loss: 109.53238235001332\n",
            "Iteration: 172000, loss: 53.6203671920969\n",
            "Iteration: 173000, loss: 0.17042147178333988\n",
            "Iteration: 174000, loss: 21.64861544060247\n",
            "Iteration: 175000, loss: 15.459949779567769\n",
            "Iteration: 176000, loss: 6.262515590769589\n",
            "Iteration: 177000, loss: 17.188589551014722\n",
            "Iteration: 178000, loss: 0.0038206643585271772\n",
            "Iteration: 179000, loss: 63.92873931287653\n",
            "Iteration: 180000, loss: 3.3763720515826723\n",
            "Iteration: 181000, loss: 20.424143020348858\n",
            "Iteration: 182000, loss: 14.819368470366577\n",
            "Iteration: 183000, loss: 16.437974839983966\n",
            "Iteration: 184000, loss: 2.9422328841065504\n",
            "Iteration: 185000, loss: 0.7768750625114341\n",
            "Iteration: 186000, loss: 5.4473484387292315\n",
            "Iteration: 187000, loss: 1.9305731591478772\n",
            "Iteration: 188000, loss: 1.8749184017986849\n",
            "Iteration: 189000, loss: 4.057792932449014\n",
            "Iteration: 190000, loss: 4.513854873221765\n",
            "Iteration: 191000, loss: 78.54996053300779\n",
            "Iteration: 192000, loss: 0.0019431502576791166\n",
            "Iteration: 193000, loss: 36.84611553413401\n",
            "Iteration: 194000, loss: 0.16111165022894305\n",
            "Iteration: 195000, loss: 4.310786854666176\n",
            "Iteration: 196000, loss: 18.610889633121538\n",
            "Iteration: 197000, loss: 11.822003944392508\n",
            "Iteration: 198000, loss: 2.841181686950455\n",
            "Iteration: 199000, loss: 0.8331460029969907\n",
            "Iteration: 200000, loss: 2.3337647787487956\n",
            "Iteration: 201000, loss: 14.248405110685134\n",
            "Iteration: 202000, loss: 0.5480792515618453\n",
            "Iteration: 203000, loss: 1.0399982433078754\n",
            "Iteration: 204000, loss: 0.33467495840398437\n",
            "Iteration: 205000, loss: 18.807441714932718\n",
            "Iteration: 206000, loss: 0.004616388073491517\n",
            "Iteration: 207000, loss: 0.9287702153440244\n",
            "Iteration: 208000, loss: 5.512154559106225\n",
            "Iteration: 209000, loss: 1.1901371526014328\n",
            "Iteration: 210000, loss: 7.158876712978683\n",
            "Iteration: 211000, loss: 2.4847042853897494\n",
            "Iteration: 212000, loss: 0.8236195410865805\n",
            "Iteration: 213000, loss: 14.507146330734654\n",
            "Iteration: 214000, loss: 1.1042087465741945\n",
            "Iteration: 215000, loss: 2.4220062375873646\n",
            "Iteration: 216000, loss: 2.346422881446855\n",
            "Iteration: 217000, loss: 0.39524930108556794\n",
            "Iteration: 218000, loss: 0.4414847964882202\n",
            "Iteration: 219000, loss: 37.72342016414651\n",
            "Iteration: 220000, loss: 4.008410625483827\n",
            "Iteration: 221000, loss: 14.375044175692585\n",
            "Iteration: 222000, loss: 9.777514963765055\n",
            "Iteration: 223000, loss: 0.6167925375200135\n",
            "Iteration: 224000, loss: 11.83944478990342\n",
            "Iteration: 225000, loss: 1.1740389451527011\n",
            "Iteration: 226000, loss: 81.16369543176158\n",
            "Iteration: 227000, loss: 128.38028366761824\n",
            "Iteration: 228000, loss: 77.74316588605743\n",
            "Iteration: 229000, loss: 3.554955512875242\n",
            "Iteration: 230000, loss: 10.199115039391327\n",
            "Iteration: 231000, loss: 8.928947576070742\n",
            "Iteration: 232000, loss: 1.0368040464364132\n",
            "Iteration: 233000, loss: 13.101971868307833\n",
            "Iteration: 234000, loss: 0.37406587501234173\n",
            "Iteration: 235000, loss: 2.971080497613537\n",
            "Iteration: 236000, loss: 7.2301265129061\n",
            "Iteration: 237000, loss: 0.35320883093986555\n",
            "Iteration: 238000, loss: 19.541405295402548\n",
            "Iteration: 239000, loss: 1.3262825899183244\n",
            "Iteration: 240000, loss: 54.36016365359067\n",
            "Iteration: 241000, loss: 3.7313229277026547\n",
            "Iteration: 242000, loss: 28.27823246279158\n",
            "Iteration: 243000, loss: 11.210094056741456\n",
            "Iteration: 244000, loss: 4.065656422004057\n",
            "Iteration: 245000, loss: 1212.2985563840796\n",
            "Iteration: 246000, loss: 3.060637894746528\n",
            "Iteration: 247000, loss: 23.974862575882987\n",
            "Iteration: 248000, loss: 2.0921968666386004\n",
            "Iteration: 249000, loss: 6.608069084430876\n",
            "Iteration: 250000, loss: 0.13031691146691993\n",
            "Iteration: 251000, loss: 1.1676977046117085\n",
            "Iteration: 252000, loss: 31.069017616048786\n",
            "Iteration: 253000, loss: 6.077848154676609\n",
            "Iteration: 254000, loss: 0.8881016035627309\n",
            "Iteration: 255000, loss: 22.943133412213363\n",
            "Iteration: 256000, loss: 12.793318064754056\n",
            "Iteration: 257000, loss: 0.4107721407580515\n",
            "Iteration: 258000, loss: 12.630727227846732\n",
            "Iteration: 259000, loss: 1.077807496244136\n",
            "Iteration: 260000, loss: 0.6799406801027997\n",
            "Iteration: 261000, loss: 5.279109010653897\n",
            "Iteration: 262000, loss: 0.4893311884059786\n",
            "Iteration: 263000, loss: 1.3867029208245378\n",
            "Iteration: 264000, loss: 0.0011289818682814418\n",
            "Iteration: 265000, loss: 12.50714947303582\n",
            "Iteration: 266000, loss: 0.5136404402109134\n",
            "Iteration: 267000, loss: 8.684942811988307\n",
            "Iteration: 268000, loss: 0.054668773724186576\n",
            "Iteration: 269000, loss: 3.3752089325252608\n",
            "Iteration: 270000, loss: 1.3242103176694062\n",
            "Iteration: 271000, loss: 5.07188530642079\n",
            "Iteration: 272000, loss: 3.8560211073765625\n",
            "Iteration: 273000, loss: 6.764625474945991\n",
            "Iteration: 274000, loss: 17.356120448904424\n",
            "Iteration: 275000, loss: 2.955687279332399\n",
            "Iteration: 276000, loss: 12.031105304621601\n",
            "Iteration: 277000, loss: 5.5007733815667095\n",
            "Iteration: 278000, loss: 44.83866506674195\n",
            "Iteration: 279000, loss: 22.818884139284645\n",
            "Iteration: 280000, loss: 32.89755759008226\n",
            "Iteration: 281000, loss: 0.006627599064288621\n",
            "Iteration: 282000, loss: 1.4711646731056711\n",
            "Iteration: 283000, loss: 10.151350542524696\n",
            "Iteration: 284000, loss: 871.8171097228907\n",
            "Iteration: 285000, loss: 33.663349903590266\n",
            "Iteration: 286000, loss: 84.49280703620505\n",
            "Iteration: 287000, loss: 2.7331963807838036\n",
            "Iteration: 288000, loss: 17.31559043805786\n",
            "Iteration: 289000, loss: 0.0006794296239434768\n",
            "Iteration: 290000, loss: 37.479796104972976\n",
            "Iteration: 291000, loss: 0.0007310754421166327\n",
            "Iteration: 292000, loss: 19.642632467701567\n",
            "Iteration: 293000, loss: 65.72621338691789\n",
            "Iteration: 294000, loss: 2.1416135782281787\n",
            "Iteration: 295000, loss: 1.0632758917803322\n",
            "Iteration: 296000, loss: 21.286295016975213\n",
            "Iteration: 297000, loss: 0.9904324928605198\n",
            "Iteration: 298000, loss: 16.30941052440052\n",
            "Iteration: 299000, loss: 14.044079552218223\n",
            "Iteration: 300000, loss: 7.85621896446321\n",
            "Iteration: 301000, loss: 0.4866943377450031\n",
            "Iteration: 302000, loss: 4.343887395260982\n",
            "Iteration: 303000, loss: 59.61553493633074\n",
            "Iteration: 304000, loss: 4.066998898330868\n",
            "Iteration: 305000, loss: 0.0004637130538133899\n",
            "Iteration: 306000, loss: 0.05967058362068075\n",
            "Iteration: 307000, loss: 0.018361379173008986\n",
            "Iteration: 308000, loss: 1.5863625835977506\n",
            "Iteration: 309000, loss: 9.90300337315521\n",
            "Iteration: 310000, loss: 48.54839555265163\n",
            "Iteration: 311000, loss: 0.1501289319601313\n",
            "Iteration: 312000, loss: 11.723187092299549\n",
            "Iteration: 313000, loss: 8.912677162895045\n",
            "Iteration: 314000, loss: 7.830504082394099\n",
            "Iteration: 315000, loss: 0.4487779251443937\n",
            "Iteration: 316000, loss: 3.648081903341968\n",
            "Iteration: 317000, loss: 0.7702671314616675\n",
            "Iteration: 318000, loss: 4.758918597384703\n",
            "Iteration: 319000, loss: 23.60730051782171\n",
            "Iteration: 320000, loss: 0.10091980930670633\n",
            "Iteration: 321000, loss: 0.23744317085255406\n",
            "Iteration: 322000, loss: 26.142250581122262\n",
            "Iteration: 323000, loss: 420.3255743561635\n",
            "Iteration: 324000, loss: 26.141154306555098\n",
            "Iteration: 325000, loss: 3.63576917336288\n",
            "Iteration: 326000, loss: 4.027010690968647\n",
            "Iteration: 327000, loss: 24.686410212083942\n",
            "Iteration: 328000, loss: 1.0450879725790265\n",
            "Iteration: 329000, loss: 2.2492713863373015\n",
            "Iteration: 330000, loss: 11.007189273663284\n",
            "Iteration: 331000, loss: 34.27305764981465\n",
            "Iteration: 332000, loss: 42.71230782435342\n",
            "Iteration: 333000, loss: 26.807842586028215\n",
            "Iteration: 334000, loss: 0.4034786057461253\n",
            "Iteration: 335000, loss: 5.025835826160892\n",
            "Iteration: 336000, loss: 0.6622710235746008\n",
            "Iteration: 337000, loss: 0.9648488880440453\n",
            "Iteration: 338000, loss: 0.7107949739497775\n",
            "Iteration: 339000, loss: 126.63817777590825\n",
            "Iteration: 340000, loss: 0.08508061093468168\n",
            "Iteration: 341000, loss: 0.06095333201583111\n",
            "Iteration: 342000, loss: 0.9180974187212735\n",
            "Iteration: 343000, loss: 11.493484370822669\n",
            "Iteration: 344000, loss: 7.661769718342038\n",
            "Iteration: 345000, loss: 2.043651533193152\n",
            "Iteration: 346000, loss: 0.1717148726195746\n",
            "Iteration: 347000, loss: 97.71828714977443\n",
            "Iteration: 348000, loss: 6.607146275046941\n",
            "Iteration: 349000, loss: 4.253119650583051\n",
            "Iteration: 350000, loss: 40.22027923384378\n",
            "Iteration: 351000, loss: 4.875242326496391\n",
            "Iteration: 352000, loss: 12.301258813261471\n",
            "Iteration: 353000, loss: 8.042873819084225\n",
            "Iteration: 354000, loss: 405.2160748235197\n",
            "Iteration: 355000, loss: 89.7397163546552\n",
            "Iteration: 356000, loss: 2.2780546849171897\n",
            "Iteration: 357000, loss: 3.264873504782223\n",
            "Iteration: 358000, loss: 8.916173990050407\n",
            "Iteration: 359000, loss: 6.973218880093668\n",
            "Iteration: 360000, loss: 9.027605938402244\n",
            "Iteration: 361000, loss: 2.9998273830168802\n",
            "Iteration: 362000, loss: 10.75450477288121\n",
            "Iteration: 363000, loss: 160.5541444223922\n",
            "Iteration: 364000, loss: 0.018120995628938177\n",
            "Iteration: 365000, loss: 78.77706656967469\n",
            "Iteration: 366000, loss: 1.206037736703295\n",
            "Iteration: 367000, loss: 0.6918094685478895\n",
            "Iteration: 368000, loss: 28.96283001993931\n",
            "Iteration: 369000, loss: 6.782851604875464\n",
            "Iteration: 370000, loss: 9.061587859220387\n",
            "Iteration: 371000, loss: 24.888882386948467\n",
            "Iteration: 372000, loss: 0.0038222363410961233\n",
            "Iteration: 373000, loss: 0.8872617378256373\n",
            "Iteration: 374000, loss: 12.61625485122607\n",
            "Iteration: 375000, loss: 12.594502508131903\n",
            "Iteration: 376000, loss: 9.524297188919235\n",
            "Iteration: 377000, loss: 0.3073147389922967\n",
            "Iteration: 378000, loss: 0.6789721780938001\n",
            "Iteration: 379000, loss: 17.141408408796483\n",
            "Iteration: 380000, loss: 4.77690747100591\n",
            "Iteration: 381000, loss: 2.9295056702321745\n",
            "Iteration: 382000, loss: 27.62566308053963\n",
            "Iteration: 383000, loss: 6.81112468750443\n",
            "Iteration: 384000, loss: 48.33452870116179\n",
            "Iteration: 385000, loss: 1.601994608793427\n",
            "Iteration: 386000, loss: 392.18185162048104\n",
            "Iteration: 387000, loss: 1.3563841170505588\n",
            "Iteration: 388000, loss: 0.14655072162010624\n",
            "Iteration: 389000, loss: 1.42872506549799\n",
            "Iteration: 390000, loss: 0.7170443120295883\n",
            "Iteration: 391000, loss: 2.81503450033107\n",
            "Iteration: 392000, loss: 0.1847033645746046\n",
            "Iteration: 393000, loss: 0.03113470943752146\n",
            "Iteration: 394000, loss: 0.08400193021551013\n",
            "Iteration: 395000, loss: 2.4444487115572655\n",
            "Iteration: 396000, loss: 50.438870807335206\n",
            "Iteration: 397000, loss: 49.07147874925186\n",
            "Iteration: 398000, loss: 8.256539264856496\n",
            "Iteration: 399000, loss: 2.048255618830479\n",
            "Iteration: 400000, loss: 31.352611998844854\n",
            "Iteration: 401000, loss: 1.4351571097199507\n",
            "Iteration: 402000, loss: 7.851002205957893\n",
            "Iteration: 403000, loss: 6.420506226121904\n",
            "Iteration: 404000, loss: 1.5457605482886279\n",
            "Iteration: 405000, loss: 0.08962968134881424\n",
            "Iteration: 406000, loss: 5.384216752298982\n",
            "Iteration: 407000, loss: 33.56860897268976\n",
            "Iteration: 408000, loss: 9.066454178470146\n",
            "Iteration: 409000, loss: 1.0124644800106486\n",
            "Iteration: 410000, loss: 1.536285401325619\n",
            "Iteration: 411000, loss: 0.027261264174540126\n",
            "Iteration: 412000, loss: 98.88454786461104\n",
            "Iteration: 413000, loss: 5.561315811524015\n",
            "Iteration: 414000, loss: 6.805438471769613\n",
            "Iteration: 415000, loss: 0.2613057212469742\n",
            "Iteration: 416000, loss: 14.655002669929319\n",
            "Iteration: 417000, loss: 4.054362381963881\n",
            "Iteration: 418000, loss: 43.38660975030891\n",
            "Iteration: 419000, loss: 14.749643113550576\n",
            "Iteration: 420000, loss: 0.13085528929162446\n",
            "Iteration: 421000, loss: 44.822578281569385\n",
            "Iteration: 422000, loss: 0.6771490191172216\n",
            "Iteration: 423000, loss: 51.834182717432775\n",
            "Iteration: 424000, loss: 2.326050189316863\n",
            "Iteration: 425000, loss: 14.254773220485232\n",
            "Iteration: 426000, loss: 28.951891991800224\n",
            "Iteration: 427000, loss: 5.672647689653541\n",
            "Iteration: 428000, loss: 3.013922103151007\n",
            "Iteration: 429000, loss: 47.52703897165421\n",
            "Iteration: 430000, loss: 30.80592067449901\n",
            "Iteration: 431000, loss: 0.41679323419660547\n",
            "Iteration: 432000, loss: 307.9693290428419\n",
            "Iteration: 433000, loss: 47.27632731786368\n",
            "Iteration: 434000, loss: 2.5301818524196467\n",
            "Iteration: 435000, loss: 1.200762600227091\n",
            "Iteration: 436000, loss: 20.428108258753255\n",
            "Iteration: 437000, loss: 14.611155380078337\n",
            "Iteration: 438000, loss: 61.9550007846701\n",
            "Iteration: 439000, loss: 39.34458399375858\n",
            "Iteration: 440000, loss: 15.624852453258594\n",
            "Iteration: 441000, loss: 0.5028225370576905\n",
            "Iteration: 442000, loss: 0.17241227813086843\n",
            "Iteration: 443000, loss: 31.15054826739601\n",
            "Iteration: 444000, loss: 73.78059964495972\n",
            "Iteration: 445000, loss: 0.3983010710020784\n",
            "Iteration: 446000, loss: 15.17025886900881\n",
            "Iteration: 447000, loss: 3.8948005057854767\n",
            "Iteration: 448000, loss: 3.97763360718038\n",
            "Iteration: 449000, loss: 9.35852306754672\n",
            "Iteration: 450000, loss: 0.014924738503014078\n",
            "Iteration: 451000, loss: 37.62759359692198\n",
            "Iteration: 452000, loss: 0.6610120278417879\n",
            "Iteration: 453000, loss: 1.5514863140842554\n",
            "Iteration: 454000, loss: 1.904936942794541\n",
            "Iteration: 455000, loss: 43.316965321798506\n",
            "Iteration: 456000, loss: 1.4982894012270505\n",
            "Iteration: 457000, loss: 5.236498115278559\n",
            "Iteration: 458000, loss: 0.004684747338906151\n",
            "Iteration: 459000, loss: 5.350388469116662\n",
            "Iteration: 460000, loss: 2.064119361740559\n",
            "Iteration: 461000, loss: 74.32354333472595\n",
            "Iteration: 462000, loss: 5.864905894630481\n",
            "Iteration: 463000, loss: 7.413453113558748\n",
            "Iteration: 464000, loss: 35.24914624973431\n",
            "Iteration: 465000, loss: 15.448066031628624\n",
            "Iteration: 466000, loss: 8.283612973254334\n",
            "Iteration: 467000, loss: 47.89010224391613\n",
            "Iteration: 468000, loss: 2.0967542523642315\n",
            "Iteration: 469000, loss: 11.889281057013747\n",
            "Iteration: 470000, loss: 12.720639643703732\n",
            "Iteration: 471000, loss: 1.0244122432944598\n",
            "Iteration: 472000, loss: 0.34716334317601444\n",
            "Iteration: 473000, loss: 14.214207766870222\n",
            "Iteration: 474000, loss: 0.10225704200610373\n",
            "Iteration: 475000, loss: 15.654194514822374\n",
            "Iteration: 476000, loss: 17.869481481641877\n",
            "Iteration: 477000, loss: 31.27731574466099\n",
            "Iteration: 478000, loss: 0.23768101570326902\n",
            "Iteration: 479000, loss: 11.51998410740983\n",
            "Iteration: 480000, loss: 0.07008204339615497\n",
            "Iteration: 481000, loss: 36.927903199683634\n",
            "Iteration: 482000, loss: 0.4514323588928007\n",
            "Iteration: 483000, loss: 3.5427615769507925\n",
            "Iteration: 484000, loss: 11.395912450099244\n",
            "Iteration: 485000, loss: 0.25326843778182984\n",
            "Iteration: 486000, loss: 1.4540424881103013\n",
            "Iteration: 487000, loss: 5.414609741731605\n",
            "Iteration: 488000, loss: 4.229833774050664\n",
            "Iteration: 489000, loss: 0.08522613981862207\n",
            "Iteration: 490000, loss: 43.816954302841076\n",
            "Iteration: 491000, loss: 12.83540483582826\n",
            "Iteration: 492000, loss: 51.672364825632265\n",
            "Iteration: 493000, loss: 24.743904560753\n",
            "Iteration: 494000, loss: 82.82854790546574\n",
            "Iteration: 495000, loss: 41.10169533306709\n",
            "Iteration: 496000, loss: 5.466056295759202\n",
            "Iteration: 497000, loss: 4.513906050042056\n",
            "Iteration: 498000, loss: 10.389581724516244\n",
            "Iteration: 499000, loss: 60.48032095419307\n",
            "Iteration: 500000, loss: 160.61281420630033\n",
            "Iteration: 501000, loss: 52.685588846182355\n",
            "Iteration: 502000, loss: 10.04656954882511\n",
            "Iteration: 503000, loss: 1.5620759601081693\n",
            "Iteration: 504000, loss: 0.5242242955461577\n",
            "Iteration: 505000, loss: 1.5419069538602865\n",
            "Iteration: 506000, loss: 22.652133736648288\n",
            "Iteration: 507000, loss: 0.8578444526193868\n",
            "Iteration: 508000, loss: 1.1762462577725084\n",
            "Iteration: 509000, loss: 33.67537020427888\n",
            "Iteration: 510000, loss: 31.17606568018495\n",
            "Iteration: 511000, loss: 224.1249620214\n",
            "Iteration: 512000, loss: 275.04068404939864\n",
            "Iteration: 513000, loss: 60.72837152901892\n",
            "Iteration: 514000, loss: 78.13484890693611\n",
            "Iteration: 515000, loss: 3.749356003657416\n",
            "Iteration: 516000, loss: 0.04861027234382957\n",
            "Iteration: 517000, loss: 18.383619426849545\n",
            "Iteration: 518000, loss: 4.219095226108863\n",
            "Iteration: 519000, loss: 6.397014553420139\n",
            "Iteration: 520000, loss: 0.17669219076744225\n",
            "Iteration: 521000, loss: 0.45611514303726\n",
            "Iteration: 522000, loss: 3.286518768344837\n",
            "Iteration: 523000, loss: 12.17529886360884\n",
            "Iteration: 524000, loss: 6.48946529039155\n",
            "Iteration: 525000, loss: 49.662627619405946\n",
            "Iteration: 526000, loss: 14.946400917048004\n",
            "Iteration: 527000, loss: 3.219174686089901\n",
            "Iteration: 528000, loss: 9.995669710797094\n",
            "Iteration: 529000, loss: 8.03563600229309\n",
            "Iteration: 530000, loss: 188.00138988767043\n",
            "Iteration: 531000, loss: 0.8170363181872755\n",
            "Iteration: 532000, loss: 0.4241342102422777\n",
            "Iteration: 533000, loss: 15.816338021397737\n",
            "Iteration: 534000, loss: 77.10126928898195\n",
            "Iteration: 535000, loss: 1.6190777750403218\n",
            "Iteration: 536000, loss: 4.013074230829993\n",
            "Iteration: 537000, loss: 31.958081758138533\n",
            "Iteration: 538000, loss: 0.02854088663542288\n",
            "Iteration: 539000, loss: 0.061001058952296225\n",
            "Iteration: 540000, loss: 34.191913432254005\n",
            "Iteration: 541000, loss: 0.5558356219281094\n",
            "Iteration: 542000, loss: 6.7795919347298\n",
            "Iteration: 543000, loss: 1.6073316991525055\n",
            "Iteration: 544000, loss: 81.92393524763806\n",
            "Iteration: 545000, loss: 7.859215341328189\n",
            "Iteration: 546000, loss: 98.29866015111014\n",
            "Iteration: 547000, loss: 2.4205728865495755\n",
            "Iteration: 548000, loss: 13.3292305809333\n",
            "Iteration: 549000, loss: 1.2774964764523642\n",
            "Iteration: 550000, loss: 0.004329944760412397\n",
            "Iteration: 551000, loss: 26.867804702045362\n",
            "Iteration: 552000, loss: 4.747376981341958\n",
            "Iteration: 553000, loss: 4.334073473719153\n",
            "Iteration: 554000, loss: 10.345491725674442\n",
            "Iteration: 555000, loss: 0.27923241879483\n",
            "Iteration: 556000, loss: 0.5733919605240442\n",
            "Iteration: 557000, loss: 2.876828674502342\n",
            "Iteration: 558000, loss: 4.326422475489853\n",
            "Iteration: 559000, loss: 2.906919711348692\n",
            "Iteration: 560000, loss: 9.034359340507747\n",
            "Iteration: 561000, loss: 1.0298846971600737\n",
            "Iteration: 562000, loss: 4.330918988110795\n",
            "Iteration: 563000, loss: 0.6645070571627376\n",
            "Iteration: 564000, loss: 10.722890548230632\n",
            "Iteration: 565000, loss: 17.504705660708122\n",
            "Iteration: 566000, loss: 331.6443192142878\n",
            "Iteration: 567000, loss: 3.0143497963812598\n",
            "Iteration: 568000, loss: 11.993038489463803\n",
            "Iteration: 569000, loss: 0.6205632471748741\n",
            "Iteration: 570000, loss: 6.189574549052647\n",
            "Iteration: 571000, loss: 0.36519643952874964\n",
            "Iteration: 572000, loss: 4.420638589954407\n",
            "Iteration: 573000, loss: 112.54921287428499\n",
            "Iteration: 574000, loss: 0.16045080734806805\n",
            "Iteration: 575000, loss: 1.782345133758474\n",
            "Iteration: 576000, loss: 1.832936576173579\n",
            "Iteration: 577000, loss: 10.629728206889276\n",
            "Iteration: 578000, loss: 3.7864278216993417\n",
            "Iteration: 579000, loss: 2.5132569639955866\n",
            "Iteration: 580000, loss: 0.06558491268961078\n",
            "Iteration: 581000, loss: 0.8179073067378398\n",
            "Iteration: 582000, loss: 0.11469626804874618\n",
            "Iteration: 583000, loss: 3.0602119027850234\n",
            "Iteration: 584000, loss: 0.004798398358759031\n",
            "Iteration: 585000, loss: 0.0776800934351467\n",
            "Iteration: 586000, loss: 0.08811126156037319\n",
            "Iteration: 587000, loss: 3.6374280509789445\n",
            "Iteration: 588000, loss: 2.259581983301371\n",
            "Iteration: 589000, loss: 8.780401838819413\n",
            "Iteration: 590000, loss: 1.9157616052218187\n",
            "Iteration: 591000, loss: 12.669532824058203\n",
            "Iteration: 592000, loss: 0.1455462275508762\n",
            "Iteration: 593000, loss: 9.462464387695226\n",
            "Iteration: 594000, loss: 27.559578135234457\n",
            "Iteration: 595000, loss: 9.624377647856626\n",
            "Iteration: 596000, loss: 31.22270922396645\n",
            "Iteration: 597000, loss: 31.050632661397866\n",
            "Iteration: 598000, loss: 1.830062504069261\n",
            "Iteration: 599000, loss: 0.056917841650282075\n",
            "Iteration: 600000, loss: 2.2579948317058602\n",
            "Iteration: 601000, loss: 0.9925996230727768\n",
            "Iteration: 602000, loss: 2.097261159368008\n",
            "Iteration: 603000, loss: 2.1895175475893116\n",
            "Iteration: 604000, loss: 12.066787466152995\n",
            "Iteration: 605000, loss: 41.06564201400344\n",
            "Iteration: 606000, loss: 18.55613147251081\n",
            "Iteration: 607000, loss: 0.6208539798704278\n",
            "Iteration: 608000, loss: 188.82874677375895\n",
            "Iteration: 609000, loss: 0.0019394745523456558\n",
            "Iteration: 610000, loss: 4.54902502277473\n",
            "Iteration: 611000, loss: 15.697388896442144\n",
            "Iteration: 612000, loss: 44.280778950573264\n",
            "Iteration: 613000, loss: 1.7764917641552573\n",
            "Iteration: 614000, loss: 1.9080463406475414\n",
            "Iteration: 615000, loss: 2.7983594854051015\n",
            "Iteration: 616000, loss: 2.843013988988933\n",
            "Iteration: 617000, loss: 32.56601570288677\n",
            "Iteration: 618000, loss: 93.74653437925238\n",
            "Iteration: 619000, loss: 5.895760482120441\n",
            "Iteration: 620000, loss: 35.60109686600835\n",
            "Iteration: 621000, loss: 8.154200241405919\n",
            "Iteration: 622000, loss: 3.5342486045363826\n",
            "Iteration: 623000, loss: 18.208401597027663\n",
            "Iteration: 624000, loss: 6.886057623333637\n",
            "Iteration: 625000, loss: 7.9572380966211425\n",
            "Iteration: 626000, loss: 17.2043646968388\n",
            "Iteration: 627000, loss: 48.403654537172166\n",
            "Iteration: 628000, loss: 1.6943632967485092\n",
            "Iteration: 629000, loss: 349.766125531416\n",
            "Iteration: 630000, loss: 27.38460208069136\n",
            "Iteration: 631000, loss: 52.80714201468466\n",
            "Iteration: 632000, loss: 7.042798555404581\n",
            "Iteration: 633000, loss: 1.325503993773314\n",
            "Iteration: 634000, loss: 24.086187506370656\n",
            "Iteration: 635000, loss: 19.504713017176144\n",
            "Iteration: 636000, loss: 0.560898147331934\n",
            "Iteration: 637000, loss: 0.0039131339294879545\n",
            "Iteration: 638000, loss: 1.116574130178161\n",
            "Iteration: 639000, loss: 1.0209071381065857\n",
            "Iteration: 640000, loss: 21.04607966315154\n",
            "Iteration: 641000, loss: 21.61428815228581\n",
            "Iteration: 642000, loss: 6.24436115941187\n",
            "Iteration: 643000, loss: 2.7391404272876727\n",
            "Iteration: 644000, loss: 1.0602396693365659\n",
            "Iteration: 645000, loss: 5.0132816561586555\n",
            "Iteration: 646000, loss: 10.350207889239977\n",
            "Iteration: 647000, loss: 0.01158283888207593\n",
            "Iteration: 648000, loss: 1.6755667378205075\n",
            "Iteration: 649000, loss: 7.285050679510595\n",
            "Iteration: 650000, loss: 2.2307467627795288\n",
            "Iteration: 651000, loss: 0.04108011380048374\n",
            "Iteration: 652000, loss: 0.24318077523273837\n",
            "Iteration: 653000, loss: 3.018535789370241\n",
            "Iteration: 654000, loss: 19.113536921993855\n",
            "Iteration: 655000, loss: 9.162410539462169\n",
            "Iteration: 656000, loss: 15.157017856461493\n",
            "Iteration: 657000, loss: 6.452769092454154\n",
            "Iteration: 658000, loss: 55.126052845214744\n",
            "Iteration: 659000, loss: 4.605706909414158\n",
            "Iteration: 660000, loss: 8.206710783298607\n",
            "Iteration: 661000, loss: 4.071412365584446\n",
            "Iteration: 662000, loss: 1.527658572444828\n",
            "Iteration: 663000, loss: 0.8921706526827169\n",
            "Iteration: 664000, loss: 65.1299709211239\n",
            "Iteration: 665000, loss: 5.0300659650515005\n",
            "Iteration: 666000, loss: 73.59865080042205\n",
            "Iteration: 667000, loss: 1.7606940270308986\n",
            "Iteration: 668000, loss: 3.332431657731809\n",
            "Iteration: 669000, loss: 73.55986013014734\n",
            "Iteration: 670000, loss: 80.4649049852717\n",
            "Iteration: 671000, loss: 13.470659015272327\n",
            "Iteration: 672000, loss: 15.74089226092204\n",
            "Iteration: 673000, loss: 17.796628932571842\n",
            "Iteration: 674000, loss: 2.9667280725375584\n",
            "Iteration: 675000, loss: 15.996129970535822\n",
            "Iteration: 676000, loss: 0.18173980288290056\n",
            "Iteration: 677000, loss: 18.433689146205083\n",
            "Iteration: 678000, loss: 8.560871094150599\n",
            "Iteration: 679000, loss: 17.598312646478764\n",
            "Iteration: 680000, loss: 20.775355555389194\n",
            "Iteration: 681000, loss: 10.928246016541854\n",
            "Iteration: 682000, loss: 234.04243835089378\n",
            "Iteration: 683000, loss: 3.5996078562322484\n",
            "Iteration: 684000, loss: 4.723966686374976\n",
            "Iteration: 685000, loss: 0.5102451483033503\n",
            "Iteration: 686000, loss: 2.811495276238028\n",
            "Iteration: 687000, loss: 4.655537925169293\n",
            "Iteration: 688000, loss: 60.33287070763352\n",
            "Iteration: 689000, loss: 10.214124645367667\n",
            "Iteration: 690000, loss: 2.53090551809517\n",
            "Iteration: 691000, loss: 0.5824769510873953\n",
            "Iteration: 692000, loss: 0.036383781198265075\n",
            "Iteration: 693000, loss: 0.3656304708252092\n",
            "Iteration: 694000, loss: 21.02045659376166\n",
            "Iteration: 695000, loss: 28.39568816230623\n",
            "Iteration: 696000, loss: 0.41700654953026933\n",
            "Iteration: 697000, loss: 0.08789891682910217\n",
            "Iteration: 698000, loss: 12.20382636517346\n",
            "Iteration: 699000, loss: 7.961289338391264\n",
            "Iteration: 700000, loss: 1.3203819058948794\n",
            "Iteration: 701000, loss: 2.3774498633925982\n",
            "Iteration: 702000, loss: 1.2116116468173623\n",
            "Iteration: 703000, loss: 15.905898546362744\n",
            "Iteration: 704000, loss: 8.854587225753654\n",
            "Iteration: 705000, loss: 0.22459389838549237\n",
            "Iteration: 706000, loss: 25.12938676381106\n",
            "Iteration: 707000, loss: 115.38251022409953\n",
            "Iteration: 708000, loss: 5.706814137125643\n",
            "Iteration: 709000, loss: 9.589293633013568\n",
            "Iteration: 710000, loss: 1.3880197877219242\n",
            "Iteration: 711000, loss: 2.053108666233396\n",
            "Iteration: 712000, loss: 22.233212815389585\n",
            "Iteration: 713000, loss: 1.1674459226271552\n",
            "Iteration: 714000, loss: 10.174694982486711\n",
            "Iteration: 715000, loss: 12.758665683406043\n",
            "Iteration: 716000, loss: 10.731311739065827\n",
            "Iteration: 717000, loss: 56.13332928326808\n",
            "Iteration: 718000, loss: 72.52936195185711\n",
            "Iteration: 719000, loss: 0.7330161096967827\n",
            "Iteration: 720000, loss: 8.734305224866901\n",
            "Iteration: 721000, loss: 6.836041275705291\n",
            "Iteration: 722000, loss: 12.116053415706077\n",
            "Iteration: 723000, loss: 74.27761289469919\n",
            "Iteration: 724000, loss: 1.2779576799933001\n",
            "Iteration: 725000, loss: 226.48625164269987\n",
            "Iteration: 726000, loss: 0.37486778272669985\n",
            "Iteration: 727000, loss: 37.17148583954716\n",
            "Iteration: 728000, loss: 0.40294921680365764\n",
            "Iteration: 729000, loss: 81.31399667132804\n",
            "Iteration: 730000, loss: 13.650788931796475\n",
            "Iteration: 731000, loss: 0.32696534745792616\n",
            "Iteration: 732000, loss: 2.3947339430666115\n",
            "Iteration: 733000, loss: 1.2686601274615443\n",
            "Iteration: 734000, loss: 8.411283820842044e-06\n",
            "Iteration: 735000, loss: 44.95314009759168\n",
            "Iteration: 736000, loss: 0.08953500910346694\n",
            "Iteration: 737000, loss: 0.41262531468976243\n",
            "Iteration: 738000, loss: 43.548278313545616\n",
            "Iteration: 739000, loss: 15.394745797185337\n",
            "Iteration: 740000, loss: 42.864212478247666\n",
            "Iteration: 741000, loss: 17.18532021525008\n",
            "Iteration: 742000, loss: 8.26039047665572\n",
            "Iteration: 743000, loss: 1.700739874468651\n",
            "Iteration: 744000, loss: 21.0816568661415\n",
            "Iteration: 745000, loss: 0.0701390501384162\n",
            "Iteration: 746000, loss: 1.456351868085583\n",
            "Iteration: 747000, loss: 20.747867473765954\n",
            "Iteration: 748000, loss: 321.4032447711835\n",
            "Iteration: 749000, loss: 0.26652918202936915\n",
            "Iteration: 750000, loss: 8.074916843498672\n",
            "Iteration: 751000, loss: 2.469339788006838\n",
            "Iteration: 752000, loss: 0.07249259402403589\n",
            "Iteration: 753000, loss: 31.50892064121185\n",
            "Iteration: 754000, loss: 0.5307583779110285\n",
            "Iteration: 755000, loss: 73.9553230096205\n",
            "Iteration: 756000, loss: 2.9433242959956627\n",
            "Iteration: 757000, loss: 25.789448729490196\n",
            "Iteration: 758000, loss: 0.451887987566941\n",
            "Iteration: 759000, loss: 3.9642740683344013\n",
            "Iteration: 760000, loss: 0.6792891735368134\n",
            "Iteration: 761000, loss: 9.269982045975539\n",
            "Iteration: 762000, loss: 115.16731317151296\n",
            "Iteration: 763000, loss: 9.587203242531361\n",
            "Iteration: 764000, loss: 2.155385263486975\n",
            "Iteration: 765000, loss: 0.7785353463411726\n",
            "Iteration: 766000, loss: 3.1929648637121937\n",
            "Iteration: 767000, loss: 2.1478317453981655\n",
            "Iteration: 768000, loss: 12.063740681313408\n",
            "Iteration: 769000, loss: 3.379852077086078\n",
            "Iteration: 770000, loss: 42.07573839426241\n",
            "Iteration: 771000, loss: 3.5788738363968724\n",
            "Iteration: 772000, loss: 0.11721859030548779\n",
            "Iteration: 773000, loss: 0.8107765597911839\n",
            "Iteration: 774000, loss: 11.219196777166136\n",
            "Iteration: 775000, loss: 3.1627340700629807\n",
            "Iteration: 776000, loss: 0.29397594609247873\n",
            "Iteration: 777000, loss: 2.727751570570603\n",
            "Iteration: 778000, loss: 9.360703661937215\n",
            "Iteration: 779000, loss: 0.0036317252894046076\n",
            "Iteration: 780000, loss: 0.2884539999636721\n",
            "Iteration: 781000, loss: 8.785416492356019\n",
            "Iteration: 782000, loss: 3.9352210042059146\n",
            "Iteration: 783000, loss: 33.996928138515\n",
            "Iteration: 784000, loss: 8.019995259721346\n",
            "Iteration: 785000, loss: 0.04698999538748238\n",
            "Iteration: 786000, loss: 1.0698711550300515\n",
            "Iteration: 787000, loss: 0.9981254136405168\n",
            "Iteration: 788000, loss: 14.045612064515947\n",
            "Iteration: 789000, loss: 0.16451664100793223\n",
            "Iteration: 790000, loss: 3.4227574796913154\n",
            "Iteration: 791000, loss: 7.057211344513713\n",
            "Iteration: 792000, loss: 10.643327640961896\n",
            "Iteration: 793000, loss: 0.7008476222678884\n",
            "Iteration: 794000, loss: 67.7722254920831\n",
            "Iteration: 795000, loss: 1.2933139831532392\n",
            "Iteration: 796000, loss: 15.04790100647392\n",
            "Iteration: 797000, loss: 8.235303699842762\n",
            "Iteration: 798000, loss: 18.501601971973347\n",
            "Iteration: 799000, loss: 0.42957406460182745\n",
            "Iteration: 800000, loss: 1.8495366444327563\n",
            "Iteration: 801000, loss: 10.857327318053894\n",
            "Iteration: 802000, loss: 20.897856054976167\n",
            "Iteration: 803000, loss: 0.006298926759068925\n",
            "Iteration: 804000, loss: 21.755767014303235\n",
            "Iteration: 805000, loss: 0.01592649843750273\n",
            "Iteration: 806000, loss: 58.15608939894058\n",
            "Iteration: 807000, loss: 1.482700483975616\n",
            "Iteration: 808000, loss: 1.267846070062279\n",
            "Iteration: 809000, loss: 17.37670264713489\n",
            "Iteration: 810000, loss: 13.64685639988488\n",
            "Iteration: 811000, loss: 52.288424264470876\n",
            "Iteration: 812000, loss: 0.37176105883901\n",
            "Iteration: 813000, loss: 13.467795723088573\n",
            "Iteration: 814000, loss: 1.8692774906201617\n",
            "Iteration: 815000, loss: 19.59005559411165\n",
            "Iteration: 816000, loss: 2.3616123991081057\n",
            "Iteration: 817000, loss: 18.133759942149883\n",
            "Iteration: 818000, loss: 19.32450882840604\n",
            "Iteration: 819000, loss: 0.6255868351084339\n",
            "Iteration: 820000, loss: 7.974378994714786\n",
            "Iteration: 821000, loss: 5.598871263180848\n",
            "Iteration: 822000, loss: 0.30179906191691935\n",
            "Iteration: 823000, loss: 9.637463514879903\n",
            "Iteration: 824000, loss: 37.269314062435576\n",
            "Iteration: 825000, loss: 26.64410833408103\n",
            "Iteration: 826000, loss: 6.3120301111814685\n",
            "Iteration: 827000, loss: 69.77420297459211\n",
            "Iteration: 828000, loss: 0.15220999227781898\n",
            "Iteration: 829000, loss: 31.67133118051824\n",
            "Iteration: 830000, loss: 41.838177208652546\n",
            "Iteration: 831000, loss: 0.7796741570695835\n",
            "Iteration: 832000, loss: 9.649641733283913\n",
            "Iteration: 833000, loss: 2.1146657314574013\n",
            "Iteration: 834000, loss: 1.5180136437102916\n",
            "Iteration: 835000, loss: 6.927251761606881\n",
            "Iteration: 836000, loss: 20.424996255650075\n",
            "Iteration: 837000, loss: 16.292566418151196\n",
            "Iteration: 838000, loss: 6.491041048410458\n",
            "Iteration: 839000, loss: 19.08946853231424\n",
            "Iteration: 840000, loss: 25.21077662046544\n",
            "Iteration: 841000, loss: 0.9061055900926557\n",
            "Iteration: 842000, loss: 0.10456879481741077\n",
            "Iteration: 843000, loss: 13.09700950180748\n",
            "Iteration: 844000, loss: 1.2576793826034287\n",
            "Iteration: 845000, loss: 3.504749471871216\n",
            "Iteration: 846000, loss: 22.9118608697202\n",
            "Iteration: 847000, loss: 7.231391628196184\n",
            "Iteration: 848000, loss: 0.44982779356723007\n",
            "Iteration: 849000, loss: 17.56673238074679\n",
            "Iteration: 850000, loss: 33.884786492091614\n",
            "Iteration: 851000, loss: 8.498161069407603\n",
            "Iteration: 852000, loss: 80.53085185438792\n",
            "Iteration: 853000, loss: 23.01198558482686\n",
            "Iteration: 854000, loss: 1.990766785880803\n",
            "Iteration: 855000, loss: 7.585694713890264\n",
            "Iteration: 856000, loss: 2.0737446546959064\n",
            "Iteration: 857000, loss: 11.786202421344738\n",
            "Iteration: 858000, loss: 27.79669561361564\n",
            "Iteration: 859000, loss: 32.601521713100304\n",
            "Iteration: 860000, loss: 53.59248824618473\n",
            "Iteration: 861000, loss: 0.015281069612086709\n",
            "Iteration: 862000, loss: 1.4213545146873454\n",
            "Iteration: 863000, loss: 1.1095960363576052\n",
            "Iteration: 864000, loss: 1.686959421701992\n",
            "Iteration: 865000, loss: 17.342709882745968\n",
            "Iteration: 866000, loss: 58.40382719704576\n",
            "Iteration: 867000, loss: 165.56376843622078\n",
            "Iteration: 868000, loss: 2.1345084722145367\n",
            "Iteration: 869000, loss: 5.773098122806198\n",
            "Iteration: 870000, loss: 18.64684850708823\n",
            "Iteration: 871000, loss: 0.18109262247062827\n",
            "Iteration: 872000, loss: 0.20150350674054263\n",
            "Iteration: 873000, loss: 0.10502546027346446\n",
            "Iteration: 874000, loss: 0.052797466791247846\n",
            "Iteration: 875000, loss: 1.522777015533783\n",
            "Iteration: 876000, loss: 32.99422760494536\n",
            "Iteration: 877000, loss: 23.582555897011268\n",
            "Iteration: 878000, loss: 0.12681638343743268\n",
            "Iteration: 879000, loss: 65.6602622148712\n",
            "Iteration: 880000, loss: 171.5987140691848\n",
            "Iteration: 881000, loss: 0.9670203822559396\n",
            "Iteration: 882000, loss: 27.028406578990932\n",
            "Iteration: 883000, loss: 8.86509881731928\n",
            "Iteration: 884000, loss: 4.184578750277119\n",
            "Iteration: 885000, loss: 0.29174143516161843\n",
            "Iteration: 886000, loss: 171.54301458603493\n",
            "Iteration: 887000, loss: 0.2831223645069627\n",
            "Iteration: 888000, loss: 0.6800519562490269\n",
            "Iteration: 889000, loss: 0.3854094133901442\n",
            "Iteration: 890000, loss: 0.021371029434867034\n",
            "Iteration: 891000, loss: 54.53672866298699\n",
            "Iteration: 892000, loss: 0.0012107755540419962\n",
            "Iteration: 893000, loss: 7.847237415767935\n",
            "Iteration: 894000, loss: 0.02923144227194619\n",
            "Iteration: 895000, loss: 3.92307147337197\n",
            "Iteration: 896000, loss: 0.6695211683142884\n",
            "Iteration: 897000, loss: 0.0007375636508908409\n",
            "Iteration: 898000, loss: 1.0793748235093248\n",
            "Iteration: 899000, loss: 28.722081468339166\n",
            "Iteration: 900000, loss: 3.8382075497197596\n",
            "Iteration: 901000, loss: 3.500797568679935\n",
            "Iteration: 902000, loss: 35.30433880847362\n",
            "Iteration: 903000, loss: 0.018758056328071177\n",
            "Iteration: 904000, loss: 47.24432338190741\n",
            "Iteration: 905000, loss: 21.99638575084031\n",
            "Iteration: 906000, loss: 53.07587659419186\n",
            "Iteration: 907000, loss: 42.30788530203771\n",
            "Iteration: 908000, loss: 19.262209664110294\n",
            "Iteration: 909000, loss: 0.059982487118822106\n",
            "Iteration: 910000, loss: 0.19417573580238606\n",
            "Iteration: 911000, loss: 26.115093546441287\n",
            "Iteration: 912000, loss: 12.540524102851759\n",
            "Iteration: 913000, loss: 48.51522285826929\n",
            "Iteration: 914000, loss: 3.1997688225825986\n",
            "Iteration: 915000, loss: 0.7721192982206988\n",
            "Iteration: 916000, loss: 17.03769970521739\n",
            "Iteration: 917000, loss: 0.6276555756316681\n",
            "Iteration: 918000, loss: 3.6511628431322496\n",
            "Iteration: 919000, loss: 8.452399447884083\n",
            "Iteration: 920000, loss: 6.312865237267555\n",
            "Iteration: 921000, loss: 0.17634733592517982\n",
            "Iteration: 922000, loss: 4.085279461237926\n",
            "Iteration: 923000, loss: 2.146988939432087\n",
            "Iteration: 924000, loss: 4.753741404601028\n",
            "Iteration: 925000, loss: 1.641126801476845\n",
            "Iteration: 926000, loss: 1.2774819697984783\n",
            "Iteration: 927000, loss: 21.781003418118164\n",
            "Iteration: 928000, loss: 0.42855691780882116\n",
            "Iteration: 929000, loss: 0.6025772359834133\n",
            "Iteration: 930000, loss: 3.971757536375325\n",
            "Iteration: 931000, loss: 10.359802113338548\n",
            "Iteration: 932000, loss: 1.320850618180271\n",
            "Iteration: 933000, loss: 3.6459509898163294\n",
            "Iteration: 934000, loss: 6.218007686831256\n",
            "Iteration: 935000, loss: 1.0343759875038783\n",
            "Iteration: 936000, loss: 3.430148055095103\n",
            "Iteration: 937000, loss: 43.068262539841676\n",
            "Iteration: 938000, loss: 18.148009955612117\n",
            "Iteration: 939000, loss: 0.8343296035046393\n",
            "Iteration: 940000, loss: 16.554132638027053\n",
            "Iteration: 941000, loss: 6.457296597902482\n",
            "Iteration: 942000, loss: 0.5994577477060343\n",
            "Iteration: 943000, loss: 25.773616188923963\n",
            "Iteration: 944000, loss: 3.5537853371606603\n",
            "Iteration: 945000, loss: 1.397116169789437\n",
            "Iteration: 946000, loss: 7.347799442240838\n",
            "Iteration: 947000, loss: 5.4186023399655605\n",
            "Iteration: 948000, loss: 0.7339079471630618\n",
            "Iteration: 949000, loss: 9.780113774057591\n",
            "Iteration: 950000, loss: 1.8778445071304883\n",
            "Iteration: 951000, loss: 0.4793422019382562\n",
            "Iteration: 952000, loss: 0.5808075153667196\n",
            "Iteration: 953000, loss: 4.317342745913552\n",
            "Iteration: 954000, loss: 3.6541122948937117\n",
            "Iteration: 955000, loss: 0.0008680643981418914\n",
            "Iteration: 956000, loss: 31.57285680693858\n",
            "Iteration: 957000, loss: 0.9661644672292716\n",
            "Iteration: 958000, loss: 5.657500361533236\n",
            "Iteration: 959000, loss: 9.392839248316944\n",
            "Iteration: 960000, loss: 0.0002868969691950377\n",
            "Iteration: 961000, loss: 1.4987636811966019\n",
            "Iteration: 962000, loss: 7.435756993587074\n",
            "Iteration: 963000, loss: 2.7360829409762557\n",
            "Iteration: 964000, loss: 1.0756445286022622\n",
            "Iteration: 965000, loss: 22.949012073172096\n",
            "Iteration: 966000, loss: 56.862924685054544\n",
            "Iteration: 967000, loss: 7.896613960050437\n",
            "Iteration: 968000, loss: 0.07372232462517969\n",
            "Iteration: 969000, loss: 4.39409164622051\n",
            "Iteration: 970000, loss: 51.41428196918923\n",
            "Iteration: 971000, loss: 5.391260767160396\n",
            "Iteration: 972000, loss: 12.797128688269536\n",
            "Iteration: 973000, loss: 1.3044510065330939\n",
            "Iteration: 974000, loss: 0.08411230366601824\n",
            "Iteration: 975000, loss: 23.595419097323298\n",
            "Iteration: 976000, loss: 1.4354256476816092\n",
            "Iteration: 977000, loss: 39.74880676786726\n",
            "Iteration: 978000, loss: 26.58629322618239\n",
            "Iteration: 979000, loss: 0.5732146071261384\n",
            "Iteration: 980000, loss: 2.943320357159809\n",
            "Iteration: 981000, loss: 0.010785862803492842\n",
            "Iteration: 982000, loss: 0.9375144441354324\n",
            "Iteration: 983000, loss: 12.093855549955887\n",
            "Iteration: 984000, loss: 38.79682985005007\n",
            "Iteration: 985000, loss: 4.798197703735861\n",
            "Iteration: 986000, loss: 0.8294388762027849\n",
            "Iteration: 987000, loss: 0.03150569768479241\n",
            "Iteration: 988000, loss: 39.64464874582507\n",
            "Iteration: 989000, loss: 5.384043245161256\n",
            "Iteration: 990000, loss: 26.796307073953034\n",
            "Iteration: 991000, loss: 28.179987350643508\n",
            "Iteration: 992000, loss: 1.3958100089370051\n",
            "Iteration: 993000, loss: 27.57579464780073\n",
            "Iteration: 994000, loss: 4.587807434277925\n",
            "Iteration: 995000, loss: 12.32954701191529\n",
            "Iteration: 996000, loss: 1.5147818561034152\n",
            "Iteration: 997000, loss: 26.056404511661587\n",
            "Iteration: 998000, loss: 169.3908119839668\n",
            "Iteration: 999000, loss: 24.1066163454583\n",
            "Mean Square Loss Error: 15.588517363%\n"
          ]
        }
      ],
      "source": [
        "i_list_2a = []\n",
        "loss_list_2a = []\n",
        "\n",
        "num_iterations = 1000000\n",
        "\n",
        "def q2a(X_train, Y_train, learning_rate):\n",
        "\n",
        "  # initializing a single dense layer with 13 input features, 1 output and learning rate of 0.00001\n",
        "  layer_list = [Dense((13,1),1,learning_rate, )]\n",
        "  # creating an instance of MeanSquaredLossLayer\n",
        "  mse = MeanSquaredLossLayer()\n",
        "  nn = Neural_Network(layer_list, mse)\n",
        "  for i in range(num_iterations) :\n",
        "    size = len(X_train)\n",
        "    index = np.random.randint(low = 0,high = size)\n",
        "    _x = X_train[index:index+1].T\n",
        "    _y = Y_train[index:index+1].reshape(1,1)\n",
        "    # passing the input and target value to the forward function of Neural_Network class and storing the loss\n",
        "    loss = nn.forward(_x, _y)\n",
        "    if(i%1000==0) :\n",
        "      i_list_2a.append(i)\n",
        "      loss_list_2a.append(loss)\n",
        "      print(f\"Iteration: {i}, loss: {loss}\")\n",
        "      pass\n",
        "    # backpropagating the error and updating the weights\n",
        "    nn.backward()\n",
        "  # returning the final list of layers\n",
        "  return nn.layer_list\n",
        "\n",
        "final_layer_list = q2a(X_train, y_train, 0.00001)\n",
        "final_loss = testing(X_test, y_test, final_layer_list)\n",
        "# printing the final loss value in percentage\n",
        "print(\"Mean Square Loss Error: {:.9f}%\".format(final_loss * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgEoOKOVnQRa"
      },
      "source": [
        "## Plot of the Average Loss V/S Number of Iterations for 2.a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "jWIipLTSnW0q",
        "outputId": "d0597419-b607-4ad3-d1c2-f31bbc0a7a80"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "avg_loss_2a = []\n",
        "for i in range(0, len(i_list_2a), 10):\n",
        "    avg_2a = sum(loss_list_2a[i:i+10]) / 10\n",
        "    avg_loss_2a.append(avg_2a)\n",
        "\n",
        "# Plot the average loss on the y-axis and the iteration number on the x-axis\n",
        "plt.plot(list(range(1, len(avg_loss_2a)+1)), avg_loss_2a, color='red', linestyle='-')\n",
        "# plt.plot(i_list, avg_loss, color='red', linestyle='--')\n",
        "\n",
        "plt.xlabel('Iteration number (in thousands)')\n",
        "plt.ylabel('Average loss')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0WXMe-69AFc"
      },
      "source": [
        "## Question 2b: two layers. Layer 1 with 13 output neurons with sigmoid activation. Layer 2 with one output neuron and linear activation. use mean squared loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vexg6ag78_Xn",
        "outputId": "c4abbaaf-b15a-4644-e16a-5fcc7826be08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 0, loss: 1055.974524809792\n",
            "Iteration: 1000, loss: 216.45333992479976\n",
            "Iteration: 2000, loss: 327.5388190181447\n",
            "Iteration: 3000, loss: 644.8602643206785\n",
            "Iteration: 4000, loss: 591.1351414950495\n",
            "Iteration: 5000, loss: 205.05853614698722\n",
            "Iteration: 6000, loss: 480.58229931855516\n",
            "Iteration: 7000, loss: 400.5845852471364\n",
            "Iteration: 8000, loss: 518.616795420336\n",
            "Iteration: 9000, loss: 1795.6802152346354\n",
            "Iteration: 10000, loss: 25.25489335063644\n",
            "Iteration: 11000, loss: 274.65000450494915\n",
            "Iteration: 12000, loss: 112.6618633554106\n",
            "Iteration: 13000, loss: 3.921238496599141\n",
            "Iteration: 14000, loss: 20.85232334441548\n",
            "Iteration: 15000, loss: 21.102494013097626\n",
            "Iteration: 16000, loss: 151.225748440947\n",
            "Iteration: 17000, loss: 14.074447744530595\n",
            "Iteration: 18000, loss: 131.62491926658956\n",
            "Iteration: 19000, loss: 310.2245430634127\n",
            "Iteration: 20000, loss: 103.64086172726262\n",
            "Iteration: 21000, loss: 92.18824274434779\n",
            "Iteration: 22000, loss: 36.74539093716098\n",
            "Iteration: 23000, loss: 52.61063521909775\n",
            "Iteration: 24000, loss: 0.3321110811017473\n",
            "Iteration: 25000, loss: 8.22807316839709\n",
            "Iteration: 26000, loss: 13.973260040117509\n",
            "Iteration: 27000, loss: 86.99792099374875\n",
            "Iteration: 28000, loss: 1.0440087255370265\n",
            "Iteration: 29000, loss: 905.5045108840789\n",
            "Iteration: 30000, loss: 7.4281458269439735\n",
            "Iteration: 31000, loss: 58.64740803871404\n",
            "Iteration: 32000, loss: 91.3672368959063\n",
            "Iteration: 33000, loss: 1.3603968669382902\n",
            "Iteration: 34000, loss: 0.49299773494785787\n",
            "Iteration: 35000, loss: 241.29200303395265\n",
            "Iteration: 36000, loss: 0.14546165950748566\n",
            "Iteration: 37000, loss: 3.9760295128748804\n",
            "Iteration: 38000, loss: 11.66140919284371\n",
            "Iteration: 39000, loss: 32.42645732435678\n",
            "Iteration: 40000, loss: 1.916012580654819\n",
            "Iteration: 41000, loss: 5.7072243416829425\n",
            "Iteration: 42000, loss: 0.16459398294386143\n",
            "Iteration: 43000, loss: 0.08791872011193827\n",
            "Iteration: 44000, loss: 1.652087251122071\n",
            "Iteration: 45000, loss: 1.4768845264704769\n",
            "Iteration: 46000, loss: 0.0010991529260599702\n",
            "Iteration: 47000, loss: 12.450194576454393\n",
            "Iteration: 48000, loss: 19.97414770896617\n",
            "Iteration: 49000, loss: 51.198954320255766\n",
            "Iteration: 50000, loss: 21.90043935287338\n",
            "Iteration: 51000, loss: 89.0118072757642\n",
            "Iteration: 52000, loss: 0.4287079026926822\n",
            "Iteration: 53000, loss: 0.03272619111655345\n",
            "Iteration: 54000, loss: 0.20172005887186956\n",
            "Iteration: 55000, loss: 3.2049390941946374\n",
            "Iteration: 56000, loss: 1.2486441634481258\n",
            "Iteration: 57000, loss: 0.516459597724363\n",
            "Iteration: 58000, loss: 5.235825066185612\n",
            "Iteration: 59000, loss: 1.8536713070827653\n",
            "Iteration: 60000, loss: 7.493750090568414\n",
            "Iteration: 61000, loss: 0.08495943033993451\n",
            "Iteration: 62000, loss: 0.08924110784508905\n",
            "Iteration: 63000, loss: 404.9618749459881\n",
            "Iteration: 64000, loss: 13.359104061935195\n",
            "Iteration: 65000, loss: 810.6813203382426\n",
            "Iteration: 66000, loss: 13.450666326641707\n",
            "Iteration: 67000, loss: 10.13734002448333\n",
            "Iteration: 68000, loss: 2.3006582398985027\n",
            "Iteration: 69000, loss: 0.5688383572151984\n",
            "Iteration: 70000, loss: 47.52913895327113\n",
            "Iteration: 71000, loss: 11.657915237983826\n",
            "Iteration: 72000, loss: 24.38882422604592\n",
            "Iteration: 73000, loss: 3.249694365409736\n",
            "Iteration: 74000, loss: 28.954688156645656\n",
            "Iteration: 75000, loss: 14.77280714002974\n",
            "Iteration: 76000, loss: 46.55441878100199\n",
            "Iteration: 77000, loss: 0.00028770342859904503\n",
            "Iteration: 78000, loss: 102.74013001221492\n",
            "Iteration: 79000, loss: 6.112262402763891\n",
            "Iteration: 80000, loss: 7.948775447334929\n",
            "Iteration: 81000, loss: 25.681049917280586\n",
            "Iteration: 82000, loss: 721.3368377943036\n",
            "Iteration: 83000, loss: 14.623395330958953\n",
            "Iteration: 84000, loss: 11.627312954722033\n",
            "Iteration: 85000, loss: 3.2197382019556287\n",
            "Iteration: 86000, loss: 0.20264936865203964\n",
            "Iteration: 87000, loss: 6.158512504888621\n",
            "Iteration: 88000, loss: 212.7737531456225\n",
            "Iteration: 89000, loss: 40.00646010163969\n",
            "Iteration: 90000, loss: 17.731222666717887\n",
            "Iteration: 91000, loss: 8.121802265835695\n",
            "Iteration: 92000, loss: 0.37104928524331243\n",
            "Iteration: 93000, loss: 137.39342816689313\n",
            "Iteration: 94000, loss: 2.5017509571149827\n",
            "Iteration: 95000, loss: 23.402134119180868\n",
            "Iteration: 96000, loss: 0.38805010631973086\n",
            "Iteration: 97000, loss: 7.425971195971709\n",
            "Iteration: 98000, loss: 1.0711496615778522\n",
            "Iteration: 99000, loss: 110.93838625116452\n",
            "Iteration: 100000, loss: 9.627485919201956\n",
            "Iteration: 101000, loss: 8.003005584801546\n",
            "Iteration: 102000, loss: 8.321083196598186\n",
            "Iteration: 103000, loss: 10.360873175697177\n",
            "Iteration: 104000, loss: 19.49451372328316\n",
            "Iteration: 105000, loss: 14.853997989376966\n",
            "Iteration: 106000, loss: 0.8410149974154374\n",
            "Iteration: 107000, loss: 325.48242053062836\n",
            "Iteration: 108000, loss: 0.02024252608798977\n",
            "Iteration: 109000, loss: 1.4279172470924266\n",
            "Iteration: 110000, loss: 1.8001360374934723\n",
            "Iteration: 111000, loss: 39.59785330662428\n",
            "Iteration: 112000, loss: 20.40282921034164\n",
            "Iteration: 113000, loss: 13.84731936452345\n",
            "Iteration: 114000, loss: 29.603652508131276\n",
            "Iteration: 115000, loss: 36.73481651404137\n",
            "Iteration: 116000, loss: 57.71900434004738\n",
            "Iteration: 117000, loss: 7.522867972975878\n",
            "Iteration: 118000, loss: 0.8762060176409189\n",
            "Iteration: 119000, loss: 1.0844119375545282\n",
            "Iteration: 120000, loss: 0.01801730540446829\n",
            "Iteration: 121000, loss: 181.88839530665075\n",
            "Iteration: 122000, loss: 2.2996065658574376\n",
            "Iteration: 123000, loss: 1.181509130789642\n",
            "Iteration: 124000, loss: 5.312505727715961\n",
            "Iteration: 125000, loss: 18.425278055474546\n",
            "Iteration: 126000, loss: 9.074069108238096\n",
            "Iteration: 127000, loss: 0.0006262191997152432\n",
            "Iteration: 128000, loss: 4.249886312027149\n",
            "Iteration: 129000, loss: 0.4710572674129124\n",
            "Iteration: 130000, loss: 0.6754486252474041\n",
            "Iteration: 131000, loss: 0.12003063587797815\n",
            "Iteration: 132000, loss: 32.38131367558081\n",
            "Iteration: 133000, loss: 40.498540846964445\n",
            "Iteration: 134000, loss: 0.5473342092247525\n",
            "Iteration: 135000, loss: 0.23541106923699773\n",
            "Iteration: 136000, loss: 0.7001107867730185\n",
            "Iteration: 137000, loss: 26.796742119144085\n",
            "Iteration: 138000, loss: 0.4853295274624884\n",
            "Iteration: 139000, loss: 5.464103157273689\n",
            "Iteration: 140000, loss: 0.05538678866118838\n",
            "Iteration: 141000, loss: 103.63052504492332\n",
            "Iteration: 142000, loss: 0.86739088082145\n",
            "Iteration: 143000, loss: 6.557959680645825\n",
            "Iteration: 144000, loss: 1.9149847033359548\n",
            "Iteration: 145000, loss: 10.348897618727166\n",
            "Iteration: 146000, loss: 4.11728897074995\n",
            "Iteration: 147000, loss: 34.99807467491467\n",
            "Iteration: 148000, loss: 17.91951974611164\n",
            "Iteration: 149000, loss: 0.01346669058841707\n",
            "Iteration: 150000, loss: 0.764018942438419\n",
            "Iteration: 151000, loss: 0.2549879862080908\n",
            "Iteration: 152000, loss: 1.2579961231070358\n",
            "Iteration: 153000, loss: 2.0943687512969316\n",
            "Iteration: 154000, loss: 0.16303222510845544\n",
            "Iteration: 155000, loss: 24.116957876762317\n",
            "Iteration: 156000, loss: 4.42148612433886\n",
            "Iteration: 157000, loss: 9.912166288941707\n",
            "Iteration: 158000, loss: 56.55225986923548\n",
            "Iteration: 159000, loss: 1.7299173756353339\n",
            "Iteration: 160000, loss: 1.629825189006509\n",
            "Iteration: 161000, loss: 7.723404599847977\n",
            "Iteration: 162000, loss: 1.8523733572464056\n",
            "Iteration: 163000, loss: 119.91482358396688\n",
            "Iteration: 164000, loss: 28.990440708201444\n",
            "Iteration: 165000, loss: 21.485849976015345\n",
            "Iteration: 166000, loss: 13.526731829752887\n",
            "Iteration: 167000, loss: 1.6557188291985423\n",
            "Iteration: 168000, loss: 0.7701362310332835\n",
            "Iteration: 169000, loss: 0.08365437717482242\n",
            "Iteration: 170000, loss: 36.25700073527887\n",
            "Iteration: 171000, loss: 10.450375152836923\n",
            "Iteration: 172000, loss: 46.49383688031959\n",
            "Iteration: 173000, loss: 8.088950839261562\n",
            "Iteration: 174000, loss: 19.015926951580738\n",
            "Iteration: 175000, loss: 18.454309155357777\n",
            "Iteration: 176000, loss: 2.3563609708462767\n",
            "Iteration: 177000, loss: 3.2162878873976073\n",
            "Iteration: 178000, loss: 4.872784637078023\n",
            "Iteration: 179000, loss: 6.930601641351583\n",
            "Iteration: 180000, loss: 1.2247090833500225\n",
            "Iteration: 181000, loss: 110.44077430691627\n",
            "Iteration: 182000, loss: 2.568215114942412\n",
            "Iteration: 183000, loss: 20.591121192431906\n",
            "Iteration: 184000, loss: 0.028850068230110756\n",
            "Iteration: 185000, loss: 0.11016768702427135\n",
            "Iteration: 186000, loss: 24.980891410273486\n",
            "Iteration: 187000, loss: 34.41472254616705\n",
            "Iteration: 188000, loss: 1.0944381097417153\n",
            "Iteration: 189000, loss: 149.7403869526293\n",
            "Iteration: 190000, loss: 21.912627174215903\n",
            "Iteration: 191000, loss: 6.769600794845415\n",
            "Iteration: 192000, loss: 144.19348421694878\n",
            "Iteration: 193000, loss: 5.936825354362239\n",
            "Iteration: 194000, loss: 21.822571582323384\n",
            "Iteration: 195000, loss: 0.671736355420345\n",
            "Iteration: 196000, loss: 14.287362781359594\n",
            "Iteration: 197000, loss: 7.86638031678402\n",
            "Iteration: 198000, loss: 0.7437641556004287\n",
            "Iteration: 199000, loss: 6.160160750923647\n",
            "Iteration: 200000, loss: 21.118562420900854\n",
            "Iteration: 201000, loss: 0.338092721729159\n",
            "Iteration: 202000, loss: 2.18760643740295\n",
            "Iteration: 203000, loss: 1.1657286794292714\n",
            "Iteration: 204000, loss: 1.090030393015394\n",
            "Iteration: 205000, loss: 33.07786477434652\n",
            "Iteration: 206000, loss: 86.08641192771752\n",
            "Iteration: 207000, loss: 7.449029070663606\n",
            "Iteration: 208000, loss: 1.5195833478605145\n",
            "Iteration: 209000, loss: 51.43583887384981\n",
            "Iteration: 210000, loss: 7.5361918368718275\n",
            "Iteration: 211000, loss: 6.788496512889288\n",
            "Iteration: 212000, loss: 1.3269359346338128\n",
            "Iteration: 213000, loss: 9.128730626985593\n",
            "Iteration: 214000, loss: 6.444495219119017\n",
            "Iteration: 215000, loss: 21.591477828173748\n",
            "Iteration: 216000, loss: 0.5111576595222368\n",
            "Iteration: 217000, loss: 16.253444555965373\n",
            "Iteration: 218000, loss: 6.308606090232361\n",
            "Iteration: 219000, loss: 2.8772239893561844\n",
            "Iteration: 220000, loss: 17.09776080607383\n",
            "Iteration: 221000, loss: 85.55949644570306\n",
            "Iteration: 222000, loss: 0.027942372605995875\n",
            "Iteration: 223000, loss: 33.181352954413434\n",
            "Iteration: 224000, loss: 0.05236567573501902\n",
            "Iteration: 225000, loss: 20.46931169700711\n",
            "Iteration: 226000, loss: 1.2946459809481645\n",
            "Iteration: 227000, loss: 0.589425575679791\n",
            "Iteration: 228000, loss: 7.334362234604193\n",
            "Iteration: 229000, loss: 7.500058601059736\n",
            "Iteration: 230000, loss: 0.14215388523546152\n",
            "Iteration: 231000, loss: 7.967072499710155\n",
            "Iteration: 232000, loss: 27.03639595187666\n",
            "Iteration: 233000, loss: 0.9301982406499097\n",
            "Iteration: 234000, loss: 4.0388422165323234\n",
            "Iteration: 235000, loss: 3.549768476010488\n",
            "Iteration: 236000, loss: 31.438101731765855\n",
            "Iteration: 237000, loss: 51.61185818299696\n",
            "Iteration: 238000, loss: 35.62528925905282\n",
            "Iteration: 239000, loss: 32.046149683198735\n",
            "Iteration: 240000, loss: 2.1290713403034056\n",
            "Iteration: 241000, loss: 0.8187466389495053\n",
            "Iteration: 242000, loss: 0.23776672045266284\n",
            "Iteration: 243000, loss: 0.4126898755340034\n",
            "Iteration: 244000, loss: 20.169822775573852\n",
            "Iteration: 245000, loss: 0.05374686677786338\n",
            "Iteration: 246000, loss: 0.6473043805859875\n",
            "Iteration: 247000, loss: 1.4906256271490645\n",
            "Iteration: 248000, loss: 1.8524789580224683\n",
            "Iteration: 249000, loss: 0.7185516854643694\n",
            "Iteration: 250000, loss: 11.312765717294399\n",
            "Iteration: 251000, loss: 1.5120949590609065\n",
            "Iteration: 252000, loss: 5.404520280660077\n",
            "Iteration: 253000, loss: 3.50585884158179\n",
            "Iteration: 254000, loss: 1.3262070620220172\n",
            "Iteration: 255000, loss: 46.93616813879913\n",
            "Iteration: 256000, loss: 15.92558261970153\n",
            "Iteration: 257000, loss: 193.81584579977178\n",
            "Iteration: 258000, loss: 0.0842216851585842\n",
            "Iteration: 259000, loss: 0.9921395465953436\n",
            "Iteration: 260000, loss: 26.05363472670705\n",
            "Iteration: 261000, loss: 0.8048418978671763\n",
            "Iteration: 262000, loss: 12.298821536466258\n",
            "Iteration: 263000, loss: 7.400161521688036\n",
            "Iteration: 264000, loss: 1.6807129721178395\n",
            "Iteration: 265000, loss: 2.7327676348140084\n",
            "Iteration: 266000, loss: 82.93028287510847\n",
            "Iteration: 267000, loss: 0.12282260742057093\n",
            "Iteration: 268000, loss: 9.11207863692334\n",
            "Iteration: 269000, loss: 0.48731333350326705\n",
            "Iteration: 270000, loss: 3.7122788761438037\n",
            "Iteration: 271000, loss: 1.0354193591476817\n",
            "Iteration: 272000, loss: 34.97203371783055\n",
            "Iteration: 273000, loss: 0.22168067629646238\n",
            "Iteration: 274000, loss: 35.26714207797416\n",
            "Iteration: 275000, loss: 0.8032239906482784\n",
            "Iteration: 276000, loss: 1.6515316737674088\n",
            "Iteration: 277000, loss: 39.20481342316967\n",
            "Iteration: 278000, loss: 4.452076360340559\n",
            "Iteration: 279000, loss: 4.883681154010792\n",
            "Iteration: 280000, loss: 6.74583146468069\n",
            "Iteration: 281000, loss: 0.21346959556730655\n",
            "Iteration: 282000, loss: 5.722057294317785\n",
            "Iteration: 283000, loss: 0.12498753690089655\n",
            "Iteration: 284000, loss: 1.5315502610894742\n",
            "Iteration: 285000, loss: 6.189546456864105\n",
            "Iteration: 286000, loss: 11.257669377671032\n",
            "Iteration: 287000, loss: 0.9307823932701975\n",
            "Iteration: 288000, loss: 4.372811478415199\n",
            "Iteration: 289000, loss: 27.21580869365216\n",
            "Iteration: 290000, loss: 0.3530991213225501\n",
            "Iteration: 291000, loss: 0.018990506529264647\n",
            "Iteration: 292000, loss: 5.280429591298173\n",
            "Iteration: 293000, loss: 0.5167603937504403\n",
            "Iteration: 294000, loss: 77.5148155726357\n",
            "Iteration: 295000, loss: 2.975983325002221\n",
            "Iteration: 296000, loss: 1.4779217195504541\n",
            "Iteration: 297000, loss: 7.66701435376\n",
            "Iteration: 298000, loss: 4.653841778981605\n",
            "Iteration: 299000, loss: 21.62163894116119\n",
            "Iteration: 300000, loss: 14.690694639977686\n",
            "Iteration: 301000, loss: 7.22756755432833\n",
            "Iteration: 302000, loss: 0.5368943619325534\n",
            "Iteration: 303000, loss: 0.026975066930290986\n",
            "Iteration: 304000, loss: 5.971632427687588\n",
            "Iteration: 305000, loss: 2.8439989170465294\n",
            "Iteration: 306000, loss: 34.223716764639946\n",
            "Iteration: 307000, loss: 0.3778182827410362\n",
            "Iteration: 308000, loss: 7.392085228007915\n",
            "Iteration: 309000, loss: 43.45755104625395\n",
            "Iteration: 310000, loss: 206.24644974881122\n",
            "Iteration: 311000, loss: 6.784669505767539\n",
            "Iteration: 312000, loss: 0.17723537601492445\n",
            "Iteration: 313000, loss: 2.9147809081362195\n",
            "Iteration: 314000, loss: 58.66082021178377\n",
            "Iteration: 315000, loss: 0.2531418363163685\n",
            "Iteration: 316000, loss: 27.639730388303747\n",
            "Iteration: 317000, loss: 0.21770937734715143\n",
            "Iteration: 318000, loss: 6.223718117828771\n",
            "Iteration: 319000, loss: 0.6929269119148117\n",
            "Iteration: 320000, loss: 5.630951463338887\n",
            "Iteration: 321000, loss: 0.9024699747349713\n",
            "Iteration: 322000, loss: 5.069313926998371\n",
            "Iteration: 323000, loss: 3.99371750208799\n",
            "Iteration: 324000, loss: 7.948701254369628\n",
            "Iteration: 325000, loss: 5.000892008711262\n",
            "Iteration: 326000, loss: 910.0049069328714\n",
            "Iteration: 327000, loss: 10.14950045011639\n",
            "Iteration: 328000, loss: 7.761982069000379\n",
            "Iteration: 329000, loss: 170.81917600554078\n",
            "Iteration: 330000, loss: 2.057147918014606\n",
            "Iteration: 331000, loss: 106.2604237024041\n",
            "Iteration: 332000, loss: 1.622174337934986\n",
            "Iteration: 333000, loss: 8.959151775866763\n",
            "Iteration: 334000, loss: 0.06163679569737228\n",
            "Iteration: 335000, loss: 4.58305586439632\n",
            "Iteration: 336000, loss: 3.8504204844701566\n",
            "Iteration: 337000, loss: 17.613466904381873\n",
            "Iteration: 338000, loss: 3.8436042110536235\n",
            "Iteration: 339000, loss: 3.71973943398823\n",
            "Iteration: 340000, loss: 1.3287659974848371\n",
            "Iteration: 341000, loss: 1.6557684654064178\n",
            "Iteration: 342000, loss: 3.4341328100151247\n",
            "Iteration: 343000, loss: 35.23885973327493\n",
            "Iteration: 344000, loss: 5.2784238215927415\n",
            "Iteration: 345000, loss: 4.081193559664019\n",
            "Iteration: 346000, loss: 58.02704370074879\n",
            "Iteration: 347000, loss: 16.776884564689418\n",
            "Iteration: 348000, loss: 25.08367329345644\n",
            "Iteration: 349000, loss: 20.372617896514473\n",
            "Iteration: 350000, loss: 28.73864703688912\n",
            "Iteration: 351000, loss: 34.65206967768528\n",
            "Iteration: 352000, loss: 92.85971767622024\n",
            "Iteration: 353000, loss: 2.8298253650082357\n",
            "Iteration: 354000, loss: 0.004101520696561518\n",
            "Iteration: 355000, loss: 67.25988472942292\n",
            "Iteration: 356000, loss: 4.176115826242392\n",
            "Iteration: 357000, loss: 29.005022884520514\n",
            "Iteration: 358000, loss: 14.471825826880542\n",
            "Iteration: 359000, loss: 23.325546941061184\n",
            "Iteration: 360000, loss: 12.227877645649865\n",
            "Iteration: 361000, loss: 1.3971452381038802\n",
            "Iteration: 362000, loss: 2.5088097473918403\n",
            "Iteration: 363000, loss: 106.55421796296753\n",
            "Iteration: 364000, loss: 2.850070135418274\n",
            "Iteration: 365000, loss: 22.472610474169024\n",
            "Iteration: 366000, loss: 5.380092399734844\n",
            "Iteration: 367000, loss: 69.23821606750694\n",
            "Iteration: 368000, loss: 0.7762529368268866\n",
            "Iteration: 369000, loss: 0.06802412710434684\n",
            "Iteration: 370000, loss: 0.9797577701910214\n",
            "Iteration: 371000, loss: 0.00285572011061189\n",
            "Iteration: 372000, loss: 13.519788734351415\n",
            "Iteration: 373000, loss: 25.244861237936867\n",
            "Iteration: 374000, loss: 1.4987461050149027\n",
            "Iteration: 375000, loss: 4.770441875132855\n",
            "Iteration: 376000, loss: 8.973109885224648\n",
            "Iteration: 377000, loss: 0.7448520571281121\n",
            "Iteration: 378000, loss: 1.645414351972812\n",
            "Iteration: 379000, loss: 0.09337874970636578\n",
            "Iteration: 380000, loss: 110.60225403822145\n",
            "Iteration: 381000, loss: 0.4340700612997082\n",
            "Iteration: 382000, loss: 17.604584685255933\n",
            "Iteration: 383000, loss: 10.881950678545252\n",
            "Iteration: 384000, loss: 17.87991037006387\n",
            "Iteration: 385000, loss: 22.89899048153661\n",
            "Iteration: 386000, loss: 0.8229365617271652\n",
            "Iteration: 387000, loss: 36.34636622365465\n",
            "Iteration: 388000, loss: 6.301970143543186\n",
            "Iteration: 389000, loss: 286.5428448070431\n",
            "Iteration: 390000, loss: 14.899585898960035\n",
            "Iteration: 391000, loss: 25.44402397974479\n",
            "Iteration: 392000, loss: 9.47574819562305\n",
            "Iteration: 393000, loss: 21.159104260967954\n",
            "Iteration: 394000, loss: 0.10349765124167355\n",
            "Iteration: 395000, loss: 3.934296962954743\n",
            "Iteration: 396000, loss: 38.11338546852724\n",
            "Iteration: 397000, loss: 29.78122003228884\n",
            "Iteration: 398000, loss: 0.5664853959855366\n",
            "Iteration: 399000, loss: 30.94496160490848\n",
            "Iteration: 400000, loss: 28.178328899931063\n",
            "Iteration: 401000, loss: 4.196832254397424\n",
            "Iteration: 402000, loss: 50.67638069542123\n",
            "Iteration: 403000, loss: 2.209174842876982\n",
            "Iteration: 404000, loss: 97.87295166942336\n",
            "Iteration: 405000, loss: 13.667992572733887\n",
            "Iteration: 406000, loss: 7.3277436395771325\n",
            "Iteration: 407000, loss: 2.264639502347947\n",
            "Iteration: 408000, loss: 1.0674296772268965\n",
            "Iteration: 409000, loss: 0.4935497893916108\n",
            "Iteration: 410000, loss: 24.645509009936223\n",
            "Iteration: 411000, loss: 25.392380728162905\n",
            "Iteration: 412000, loss: 2.4654475395195834\n",
            "Iteration: 413000, loss: 31.608786477662242\n",
            "Iteration: 414000, loss: 28.54205131361952\n",
            "Iteration: 415000, loss: 5.427973027125073\n",
            "Iteration: 416000, loss: 1.9129315961547153\n",
            "Iteration: 417000, loss: 0.09948189355736087\n",
            "Iteration: 418000, loss: 6.594310874445195\n",
            "Iteration: 419000, loss: 13.891813587761645\n",
            "Iteration: 420000, loss: 0.016739569850075183\n",
            "Iteration: 421000, loss: 5.93959332034021\n",
            "Iteration: 422000, loss: 0.6569498204071192\n",
            "Iteration: 423000, loss: 10.183133363765956\n",
            "Iteration: 424000, loss: 264.4584307489073\n",
            "Iteration: 425000, loss: 1.7330145911888069\n",
            "Iteration: 426000, loss: 41.454016899958496\n",
            "Iteration: 427000, loss: 1.1695465774519989\n",
            "Iteration: 428000, loss: 0.21211425744323972\n",
            "Iteration: 429000, loss: 5.823702404599355\n",
            "Iteration: 430000, loss: 14.123753664294533\n",
            "Iteration: 431000, loss: 6.450945197701414\n",
            "Iteration: 432000, loss: 2.4335770661872904\n",
            "Iteration: 433000, loss: 7.122043805060698\n",
            "Iteration: 434000, loss: 9.602085813112849\n",
            "Iteration: 435000, loss: 0.8802374559857047\n",
            "Iteration: 436000, loss: 3.0208879107056563\n",
            "Iteration: 437000, loss: 59.58105172468962\n",
            "Iteration: 438000, loss: 38.84726910767906\n",
            "Iteration: 439000, loss: 34.780310569382614\n",
            "Iteration: 440000, loss: 5.19443581871644\n",
            "Iteration: 441000, loss: 18.57276608203722\n",
            "Iteration: 442000, loss: 0.12576915528066876\n",
            "Iteration: 443000, loss: 11.773213080564359\n",
            "Iteration: 444000, loss: 0.1512545171386665\n",
            "Iteration: 445000, loss: 20.669147413451675\n",
            "Iteration: 446000, loss: 2.3417526870065997\n",
            "Iteration: 447000, loss: 1.6326553196427005\n",
            "Iteration: 448000, loss: 0.24266628386835928\n",
            "Iteration: 449000, loss: 0.017319652346293304\n",
            "Iteration: 450000, loss: 28.39744548434362\n",
            "Iteration: 451000, loss: 0.6671054855276376\n",
            "Iteration: 452000, loss: 6.542837582126784\n",
            "Iteration: 453000, loss: 0.20922681514590483\n",
            "Iteration: 454000, loss: 3.5900360118508443\n",
            "Iteration: 455000, loss: 0.2504889225548405\n",
            "Iteration: 456000, loss: 0.12899722011027698\n",
            "Iteration: 457000, loss: 3.5928953151471674\n",
            "Iteration: 458000, loss: 3.2735196866582745\n",
            "Iteration: 459000, loss: 1.5530001282370243\n",
            "Iteration: 460000, loss: 25.824651204769015\n",
            "Iteration: 461000, loss: 0.5758781354527586\n",
            "Iteration: 462000, loss: 0.19606975546589675\n",
            "Iteration: 463000, loss: 5.343358553434948\n",
            "Iteration: 464000, loss: 21.283011437230215\n",
            "Iteration: 465000, loss: 3.2960254646108735\n",
            "Iteration: 466000, loss: 0.007902762390441206\n",
            "Iteration: 467000, loss: 4.41487541489113\n",
            "Iteration: 468000, loss: 1.1209652968276447\n",
            "Iteration: 469000, loss: 11.84975016862432\n",
            "Iteration: 470000, loss: 14.88716789490721\n",
            "Iteration: 471000, loss: 11.71087193449342\n",
            "Iteration: 472000, loss: 136.6764672640789\n",
            "Iteration: 473000, loss: 8.225651976249956\n",
            "Iteration: 474000, loss: 4.8444186849138635\n",
            "Iteration: 475000, loss: 28.739102314205688\n",
            "Iteration: 476000, loss: 10.781340795719963\n",
            "Iteration: 477000, loss: 1.9220017355351724\n",
            "Iteration: 478000, loss: 27.639309982464837\n",
            "Iteration: 479000, loss: 0.2935710427662728\n",
            "Iteration: 480000, loss: 36.65108028477774\n",
            "Iteration: 481000, loss: 7.848649109327231\n",
            "Iteration: 482000, loss: 7.1903783536070085\n",
            "Iteration: 483000, loss: 2.5195502531414364\n",
            "Iteration: 484000, loss: 0.19026298863812727\n",
            "Iteration: 485000, loss: 0.08478684320510786\n",
            "Iteration: 486000, loss: 24.0258322304629\n",
            "Iteration: 487000, loss: 38.97722386227633\n",
            "Iteration: 488000, loss: 11.548828637812338\n",
            "Iteration: 489000, loss: 1.682847587347189\n",
            "Iteration: 490000, loss: 1.4069182878749207\n",
            "Iteration: 491000, loss: 0.39406251778179446\n",
            "Iteration: 492000, loss: 28.446066770100412\n",
            "Iteration: 493000, loss: 0.9214016352169959\n",
            "Iteration: 494000, loss: 0.1315327884103269\n",
            "Iteration: 495000, loss: 7.13011692547606\n",
            "Iteration: 496000, loss: 3.3323474402518483\n",
            "Iteration: 497000, loss: 0.3190527009094541\n",
            "Iteration: 498000, loss: 16.30237360324749\n",
            "Iteration: 499000, loss: 2.821261474878575\n",
            "Iteration: 500000, loss: 19.510400135311333\n",
            "Iteration: 501000, loss: 75.81614943668008\n",
            "Iteration: 502000, loss: 38.60104803142521\n",
            "Iteration: 503000, loss: 0.018423203811167008\n",
            "Iteration: 504000, loss: 38.77713390696872\n",
            "Iteration: 505000, loss: 14.548271626223293\n",
            "Iteration: 506000, loss: 19.853791591955297\n",
            "Iteration: 507000, loss: 13.855664851275833\n",
            "Iteration: 508000, loss: 41.05470687524856\n",
            "Iteration: 509000, loss: 3.3590328650449126\n",
            "Iteration: 510000, loss: 7.734648079542213\n",
            "Iteration: 511000, loss: 5.631043711398961\n",
            "Iteration: 512000, loss: 5.04304517036378\n",
            "Iteration: 513000, loss: 1.867133834634272\n",
            "Iteration: 514000, loss: 14.03341808979488\n",
            "Iteration: 515000, loss: 22.01617972019868\n",
            "Iteration: 516000, loss: 0.8755923770185626\n",
            "Iteration: 517000, loss: 10.928804381255855\n",
            "Iteration: 518000, loss: 1.5911907352536594\n",
            "Iteration: 519000, loss: 6.886837466100927\n",
            "Iteration: 520000, loss: 0.010995136982467355\n",
            "Iteration: 521000, loss: 0.06833691176386489\n",
            "Iteration: 522000, loss: 15.542618352595952\n",
            "Iteration: 523000, loss: 0.06964541217330292\n",
            "Iteration: 524000, loss: 2.229891851458558\n",
            "Iteration: 525000, loss: 0.5410430089654451\n",
            "Iteration: 526000, loss: 0.6854700681763132\n",
            "Iteration: 527000, loss: 2.555215461062489\n",
            "Iteration: 528000, loss: 0.08323012403367488\n",
            "Iteration: 529000, loss: 7.471290568209635\n",
            "Iteration: 530000, loss: 4.773201318177093\n",
            "Iteration: 531000, loss: 8.436700598435761\n",
            "Iteration: 532000, loss: 3.9858905917847367\n",
            "Iteration: 533000, loss: 0.6819870134423022\n",
            "Iteration: 534000, loss: 8.533513697811948\n",
            "Iteration: 535000, loss: 0.5705599231882043\n",
            "Iteration: 536000, loss: 0.30770087493676707\n",
            "Iteration: 537000, loss: 1.900460411411361\n",
            "Iteration: 538000, loss: 29.660689049463738\n",
            "Iteration: 539000, loss: 70.26861850654007\n",
            "Iteration: 540000, loss: 0.0225213162911905\n",
            "Iteration: 541000, loss: 0.002507056317838488\n",
            "Iteration: 542000, loss: 0.03812658380092808\n",
            "Iteration: 543000, loss: 2.525588501185654\n",
            "Iteration: 544000, loss: 0.7588915691320847\n",
            "Iteration: 545000, loss: 7.597946029148605\n",
            "Iteration: 546000, loss: 753.8807959420072\n",
            "Iteration: 547000, loss: 1.4035970093229029\n",
            "Iteration: 548000, loss: 45.56220817920274\n",
            "Iteration: 549000, loss: 5.271376774474496\n",
            "Iteration: 550000, loss: 20.087065826759854\n",
            "Iteration: 551000, loss: 22.203505740634416\n",
            "Iteration: 552000, loss: 0.7145129618234557\n",
            "Iteration: 553000, loss: 0.33683149994389555\n",
            "Iteration: 554000, loss: 22.20099558803621\n",
            "Iteration: 555000, loss: 1.8262067774770978\n",
            "Iteration: 556000, loss: 0.8517118504304341\n",
            "Iteration: 557000, loss: 0.04132776890148106\n",
            "Iteration: 558000, loss: 2.2258485733143845\n",
            "Iteration: 559000, loss: 11.160251392906442\n",
            "Iteration: 560000, loss: 0.5298015583321184\n",
            "Iteration: 561000, loss: 23.441409421991253\n",
            "Iteration: 562000, loss: 0.12044602909206056\n",
            "Iteration: 563000, loss: 5.039787064122198\n",
            "Iteration: 564000, loss: 0.2193479007777154\n",
            "Iteration: 565000, loss: 2.1308270983458657\n",
            "Iteration: 566000, loss: 0.003466720391541716\n",
            "Iteration: 567000, loss: 0.03924484642295578\n",
            "Iteration: 568000, loss: 4.22330704885782\n",
            "Iteration: 569000, loss: 0.018159850766122847\n",
            "Iteration: 570000, loss: 3.7871203139059135\n",
            "Iteration: 571000, loss: 0.015154453198179402\n",
            "Iteration: 572000, loss: 15.095224834183819\n",
            "Iteration: 573000, loss: 1.8635359138145209\n",
            "Iteration: 574000, loss: 679.7941796477671\n",
            "Iteration: 575000, loss: 24.159477665299047\n",
            "Iteration: 576000, loss: 5.398419114208883\n",
            "Iteration: 577000, loss: 19.793605174181657\n",
            "Iteration: 578000, loss: 0.40111245314095006\n",
            "Iteration: 579000, loss: 18.462804423917934\n",
            "Iteration: 580000, loss: 24.98114605292562\n",
            "Iteration: 581000, loss: 0.9027837713354188\n",
            "Iteration: 582000, loss: 3.8577980561732987\n",
            "Iteration: 583000, loss: 32.12429912637322\n",
            "Iteration: 584000, loss: 4.634332876262146\n",
            "Iteration: 585000, loss: 22.437163600260536\n",
            "Iteration: 586000, loss: 0.0016018529432354188\n",
            "Iteration: 587000, loss: 0.15211306586133938\n",
            "Iteration: 588000, loss: 0.6743664161375296\n",
            "Iteration: 589000, loss: 0.7429209178990281\n",
            "Iteration: 590000, loss: 0.6523736940517738\n",
            "Iteration: 591000, loss: 8.825571070455707\n",
            "Iteration: 592000, loss: 0.0042994858908122135\n",
            "Iteration: 593000, loss: 157.4286821327104\n",
            "Iteration: 594000, loss: 0.45997115032206604\n",
            "Iteration: 595000, loss: 0.7423570727293451\n",
            "Iteration: 596000, loss: 2.446078107425546\n",
            "Iteration: 597000, loss: 0.17629972707887823\n",
            "Iteration: 598000, loss: 9.881819273041224\n",
            "Iteration: 599000, loss: 0.4903826514563322\n",
            "Iteration: 600000, loss: 27.924072997495824\n",
            "Iteration: 601000, loss: 7.6487120111203035\n",
            "Iteration: 602000, loss: 9.09775441050358\n",
            "Iteration: 603000, loss: 20.40972261017055\n",
            "Iteration: 604000, loss: 671.3392058199948\n",
            "Iteration: 605000, loss: 0.4652153644840964\n",
            "Iteration: 606000, loss: 0.242278336320694\n",
            "Iteration: 607000, loss: 5.841526264301876\n",
            "Iteration: 608000, loss: 64.34059474863662\n",
            "Iteration: 609000, loss: 10.534620684861814\n",
            "Iteration: 610000, loss: 5.488055515730574\n",
            "Iteration: 611000, loss: 9.60551980540219\n",
            "Iteration: 612000, loss: 14.915405062869883\n",
            "Iteration: 613000, loss: 0.06492573635462559\n",
            "Iteration: 614000, loss: 14.994009546345113\n",
            "Iteration: 615000, loss: 17.461675919098976\n",
            "Iteration: 616000, loss: 20.449694154811638\n",
            "Iteration: 617000, loss: 4.6971325832575275\n",
            "Iteration: 618000, loss: 7.352636953345707\n",
            "Iteration: 619000, loss: 0.14286326192569734\n",
            "Iteration: 620000, loss: 3.734565849999396\n",
            "Iteration: 621000, loss: 0.006692293248223353\n",
            "Iteration: 622000, loss: 8.321919998411285\n",
            "Iteration: 623000, loss: 20.219665802228345\n",
            "Iteration: 624000, loss: 28.50466783942575\n",
            "Iteration: 625000, loss: 2.053518532770584\n",
            "Iteration: 626000, loss: 1.9684167790812632\n",
            "Iteration: 627000, loss: 25.402759759047704\n",
            "Iteration: 628000, loss: 1.2113142354467876\n",
            "Iteration: 629000, loss: 0.14172338959125988\n",
            "Iteration: 630000, loss: 30.636592013685082\n",
            "Iteration: 631000, loss: 0.28445109235633714\n",
            "Iteration: 632000, loss: 0.01033979613411555\n",
            "Iteration: 633000, loss: 2.0926077276275663\n",
            "Iteration: 634000, loss: 2.224368638854509\n",
            "Iteration: 635000, loss: 16.675354464387933\n",
            "Iteration: 636000, loss: 0.09256866294416756\n",
            "Iteration: 637000, loss: 0.2254028366524406\n",
            "Iteration: 638000, loss: 61.53235763134256\n",
            "Iteration: 639000, loss: 0.02723560860177677\n",
            "Iteration: 640000, loss: 21.89073718806847\n",
            "Iteration: 641000, loss: 0.7287186564420594\n",
            "Iteration: 642000, loss: 45.37303083654842\n",
            "Iteration: 643000, loss: 0.6607033808868146\n",
            "Iteration: 644000, loss: 5.1918480324528895\n",
            "Iteration: 645000, loss: 14.64511897930355\n",
            "Iteration: 646000, loss: 1.6806173999063774\n",
            "Iteration: 647000, loss: 0.47926503487220135\n",
            "Iteration: 648000, loss: 0.13647850521866484\n",
            "Iteration: 649000, loss: 84.4529445370543\n",
            "Iteration: 650000, loss: 0.12510609305870407\n",
            "Iteration: 651000, loss: 0.3782169921985986\n",
            "Iteration: 652000, loss: 21.650814752477423\n",
            "Iteration: 653000, loss: 37.878706685969696\n",
            "Iteration: 654000, loss: 0.5855188639279855\n",
            "Iteration: 655000, loss: 3.338240766041194\n",
            "Iteration: 656000, loss: 0.01345884498059626\n",
            "Iteration: 657000, loss: 0.8863464386425196\n",
            "Iteration: 658000, loss: 0.6392730753303536\n",
            "Iteration: 659000, loss: 0.05092874591363111\n",
            "Iteration: 660000, loss: 32.473645578601\n",
            "Iteration: 661000, loss: 0.020696574826175117\n",
            "Iteration: 662000, loss: 7.062072008728221\n",
            "Iteration: 663000, loss: 20.134595450893098\n",
            "Iteration: 664000, loss: 7.015657312555194\n",
            "Iteration: 665000, loss: 37.6382915403914\n",
            "Iteration: 666000, loss: 0.518306137251997\n",
            "Iteration: 667000, loss: 91.18958186477306\n",
            "Iteration: 668000, loss: 10.453523835798173\n",
            "Iteration: 669000, loss: 54.18753862677232\n",
            "Iteration: 670000, loss: 0.40396542118658474\n",
            "Iteration: 671000, loss: 2.732320785274202\n",
            "Iteration: 672000, loss: 14.278037503850383\n",
            "Iteration: 673000, loss: 1.610335795528744\n",
            "Iteration: 674000, loss: 0.7900021890863829\n",
            "Iteration: 675000, loss: 1.0830854998286514\n",
            "Iteration: 676000, loss: 0.07340588181137739\n",
            "Iteration: 677000, loss: 1.5762409757224292\n",
            "Iteration: 678000, loss: 21.15161430238832\n",
            "Iteration: 679000, loss: 33.10992736100172\n",
            "Iteration: 680000, loss: 3.6436275702394227\n",
            "Iteration: 681000, loss: 15.502320565047194\n",
            "Iteration: 682000, loss: 2.5087334964895196\n",
            "Iteration: 683000, loss: 0.5249119511498029\n",
            "Iteration: 684000, loss: 0.01836345270880537\n",
            "Iteration: 685000, loss: 0.012965811450895237\n",
            "Iteration: 686000, loss: 0.078660144463556\n",
            "Iteration: 687000, loss: 5.713289007116737\n",
            "Iteration: 688000, loss: 0.3382600872583224\n",
            "Iteration: 689000, loss: 0.6625233020034929\n",
            "Iteration: 690000, loss: 3.9040900105273977\n",
            "Iteration: 691000, loss: 13.7826496674614\n",
            "Iteration: 692000, loss: 0.2323148287509047\n",
            "Iteration: 693000, loss: 1.6572258392727663\n",
            "Iteration: 694000, loss: 0.030265663504782672\n",
            "Iteration: 695000, loss: 1.0114285974132753\n",
            "Iteration: 696000, loss: 5.771539470572642\n",
            "Iteration: 697000, loss: 0.477021021705315\n",
            "Iteration: 698000, loss: 47.908396033003065\n",
            "Iteration: 699000, loss: 9.345190381608163\n",
            "Iteration: 700000, loss: 2.1461634640494376\n",
            "Iteration: 701000, loss: 13.096286886561272\n",
            "Iteration: 702000, loss: 155.57781155999928\n",
            "Iteration: 703000, loss: 0.0015814873612551962\n",
            "Iteration: 704000, loss: 90.57905573351789\n",
            "Iteration: 705000, loss: 22.509203727363055\n",
            "Iteration: 706000, loss: 0.96529117752666\n",
            "Iteration: 707000, loss: 26.263857910488017\n",
            "Iteration: 708000, loss: 0.00904553476914532\n",
            "Iteration: 709000, loss: 0.22536249175115056\n",
            "Iteration: 710000, loss: 11.877026513849232\n",
            "Iteration: 711000, loss: 1.491617136010984\n",
            "Iteration: 712000, loss: 7.572605826340315\n",
            "Iteration: 713000, loss: 5.64102858015562\n",
            "Iteration: 714000, loss: 3.7485612401005968\n",
            "Iteration: 715000, loss: 8.209274687177148\n",
            "Iteration: 716000, loss: 0.0068226125901549575\n",
            "Iteration: 717000, loss: 7.9641393120891735\n",
            "Iteration: 718000, loss: 0.5642391285662395\n",
            "Iteration: 719000, loss: 0.15683775968563501\n",
            "Iteration: 720000, loss: 40.430434629197144\n",
            "Iteration: 721000, loss: 2.2971852054197273\n",
            "Iteration: 722000, loss: 0.4760795253354982\n",
            "Iteration: 723000, loss: 6.4956798804347144\n",
            "Iteration: 724000, loss: 0.3724160783447026\n",
            "Iteration: 725000, loss: 13.560650897103256\n",
            "Iteration: 726000, loss: 19.958896780336207\n",
            "Iteration: 727000, loss: 22.907314910094126\n",
            "Iteration: 728000, loss: 0.41087992847488825\n",
            "Iteration: 729000, loss: 9.785058714481506\n",
            "Iteration: 730000, loss: 16.037480049456725\n",
            "Iteration: 731000, loss: 39.694682926008475\n",
            "Iteration: 732000, loss: 1.401464522156114\n",
            "Iteration: 733000, loss: 5.700152399621269\n",
            "Iteration: 734000, loss: 17.557842629259266\n",
            "Iteration: 735000, loss: 142.26238730249594\n",
            "Iteration: 736000, loss: 1.094302561024281\n",
            "Iteration: 737000, loss: 0.9834991506260494\n",
            "Iteration: 738000, loss: 10.655426665157997\n",
            "Iteration: 739000, loss: 7.577654819035579\n",
            "Iteration: 740000, loss: 2.5344781135979733\n",
            "Iteration: 741000, loss: 0.0003302872565687463\n",
            "Iteration: 742000, loss: 0.483400125757415\n",
            "Iteration: 743000, loss: 0.17528741006706838\n",
            "Iteration: 744000, loss: 0.01238545371698185\n",
            "Iteration: 745000, loss: 3.530986032750874\n",
            "Iteration: 746000, loss: 10.277694513074874\n",
            "Iteration: 747000, loss: 13.628246331949525\n",
            "Iteration: 748000, loss: 96.94496925104546\n",
            "Iteration: 749000, loss: 0.00010204942624149437\n",
            "Iteration: 750000, loss: 5.78074742321569\n",
            "Iteration: 751000, loss: 3.462388714698426\n",
            "Iteration: 752000, loss: 56.0362067954212\n",
            "Iteration: 753000, loss: 14.224274226209461\n",
            "Iteration: 754000, loss: 98.7621754225034\n",
            "Iteration: 755000, loss: 0.025295069984869568\n",
            "Iteration: 756000, loss: 14.29393112248065\n",
            "Iteration: 757000, loss: 2.4751224109449175\n",
            "Iteration: 758000, loss: 53.798818960368756\n",
            "Iteration: 759000, loss: 32.798074234269315\n",
            "Iteration: 760000, loss: 1.4873945238581248\n",
            "Iteration: 761000, loss: 0.7154212659309059\n",
            "Iteration: 762000, loss: 2.9578439540134176\n",
            "Iteration: 763000, loss: 0.5307643090535034\n",
            "Iteration: 764000, loss: 25.97193617648242\n",
            "Iteration: 765000, loss: 0.5901001283689867\n",
            "Iteration: 766000, loss: 6.251085420071276\n",
            "Iteration: 767000, loss: 21.345548041721237\n",
            "Iteration: 768000, loss: 7.335733734619714\n",
            "Iteration: 769000, loss: 15.165359952488258\n",
            "Iteration: 770000, loss: 38.0220271162768\n",
            "Iteration: 771000, loss: 0.0014940698620098227\n",
            "Iteration: 772000, loss: 5.576896814913083\n",
            "Iteration: 773000, loss: 1.7511589102662883\n",
            "Iteration: 774000, loss: 8.378347222159773\n",
            "Iteration: 775000, loss: 21.457738819513366\n",
            "Iteration: 776000, loss: 10.426878241949645\n",
            "Iteration: 777000, loss: 0.02261328774012847\n",
            "Iteration: 778000, loss: 3.445806545547747\n",
            "Iteration: 779000, loss: 4.483570411307489\n",
            "Iteration: 780000, loss: 5.889361824983437\n",
            "Iteration: 781000, loss: 1.558448905014391\n",
            "Iteration: 782000, loss: 30.84704087803605\n",
            "Iteration: 783000, loss: 0.3962076015023877\n",
            "Iteration: 784000, loss: 0.7857180912074841\n",
            "Iteration: 785000, loss: 9.546004524488914\n",
            "Iteration: 786000, loss: 1.638276205240064\n",
            "Iteration: 787000, loss: 1.4559286883943239\n",
            "Iteration: 788000, loss: 1.5476493264504878\n",
            "Iteration: 789000, loss: 0.06583842954007606\n",
            "Iteration: 790000, loss: 29.74267493599827\n",
            "Iteration: 791000, loss: 0.14994337100190877\n",
            "Iteration: 792000, loss: 18.744640292886928\n",
            "Iteration: 793000, loss: 20.413935748243038\n",
            "Iteration: 794000, loss: 0.004528285006535097\n",
            "Iteration: 795000, loss: 54.051491291219726\n",
            "Iteration: 796000, loss: 42.989241082053056\n",
            "Iteration: 797000, loss: 7.910034896299045\n",
            "Iteration: 798000, loss: 0.5167863305905103\n",
            "Iteration: 799000, loss: 0.45433820586850343\n",
            "Iteration: 800000, loss: 0.34546422282398986\n",
            "Iteration: 801000, loss: 0.527493900154266\n",
            "Iteration: 802000, loss: 0.476591382181204\n",
            "Iteration: 803000, loss: 6.072126007652353\n",
            "Iteration: 804000, loss: 8.390881796104054\n",
            "Iteration: 805000, loss: 25.145850660351254\n",
            "Iteration: 806000, loss: 3.7684277991489723\n",
            "Iteration: 807000, loss: 95.25792157101372\n",
            "Iteration: 808000, loss: 0.27585422708889684\n",
            "Iteration: 809000, loss: 1.9815668769272945\n",
            "Iteration: 810000, loss: 0.8247473567969624\n",
            "Iteration: 811000, loss: 3.0145849711793935\n",
            "Iteration: 812000, loss: 0.4535920595027286\n",
            "Iteration: 813000, loss: 2.3228579075459135\n",
            "Iteration: 814000, loss: 168.8892116486376\n",
            "Iteration: 815000, loss: 13.476672526341387\n",
            "Iteration: 816000, loss: 151.76327224975165\n",
            "Iteration: 817000, loss: 399.77972583336964\n",
            "Iteration: 818000, loss: 1.3383954862451726\n",
            "Iteration: 819000, loss: 5.767256791035421\n",
            "Iteration: 820000, loss: 1.925569948318681\n",
            "Iteration: 821000, loss: 0.0008705045664247902\n",
            "Iteration: 822000, loss: 15.13559406978485\n",
            "Iteration: 823000, loss: 2.128998855924445\n",
            "Iteration: 824000, loss: 0.0027334800523155225\n",
            "Iteration: 825000, loss: 1.0628173423325382\n",
            "Iteration: 826000, loss: 0.26811476525848377\n",
            "Iteration: 827000, loss: 0.5554441914577095\n",
            "Iteration: 828000, loss: 4.857848517110425\n",
            "Iteration: 829000, loss: 2.0136938978337686\n",
            "Iteration: 830000, loss: 0.3308828366087635\n",
            "Iteration: 831000, loss: 10.059682800151165\n",
            "Iteration: 832000, loss: 6.663211051792632\n",
            "Iteration: 833000, loss: 2.4732442788938593\n",
            "Iteration: 834000, loss: 0.011304681558756784\n",
            "Iteration: 835000, loss: 1.5403867702059422\n",
            "Iteration: 836000, loss: 2.498636495815403\n",
            "Iteration: 837000, loss: 7.980238689497825\n",
            "Iteration: 838000, loss: 0.5476518786776766\n",
            "Iteration: 839000, loss: 46.50292939952744\n",
            "Iteration: 840000, loss: 30.556912046654553\n",
            "Iteration: 841000, loss: 0.14064051306543565\n",
            "Iteration: 842000, loss: 1.6350953402417527\n",
            "Iteration: 843000, loss: 6.042718208501102\n",
            "Iteration: 844000, loss: 15.119852804448783\n",
            "Iteration: 845000, loss: 9.57282583074951\n",
            "Iteration: 846000, loss: 0.16102146103948584\n",
            "Iteration: 847000, loss: 1.4077324447945214\n",
            "Iteration: 848000, loss: 1.120306780849579\n",
            "Iteration: 849000, loss: 4.9795336695843515\n",
            "Iteration: 850000, loss: 13.24856070109204\n",
            "Iteration: 851000, loss: 0.18970340740445246\n",
            "Iteration: 852000, loss: 1.1902773375401332\n",
            "Iteration: 853000, loss: 18.121543502224245\n",
            "Iteration: 854000, loss: 7.510180675844108\n",
            "Iteration: 855000, loss: 203.54261916073753\n",
            "Iteration: 856000, loss: 0.5957392877771894\n",
            "Iteration: 857000, loss: 0.30744074020548295\n",
            "Iteration: 858000, loss: 0.1487060935324807\n",
            "Iteration: 859000, loss: 0.4383579254646509\n",
            "Iteration: 860000, loss: 12.30237477907933\n",
            "Iteration: 861000, loss: 5.750571306291614\n",
            "Iteration: 862000, loss: 14.216738640470279\n",
            "Iteration: 863000, loss: 32.91227522804202\n",
            "Iteration: 864000, loss: 15.94105157778256\n",
            "Iteration: 865000, loss: 0.14866834335371085\n",
            "Iteration: 866000, loss: 1.353888099786449\n",
            "Iteration: 867000, loss: 4.494099709913697\n",
            "Iteration: 868000, loss: 39.86588699697172\n",
            "Iteration: 869000, loss: 3.622441753013031\n",
            "Iteration: 870000, loss: 1.887039843087223\n",
            "Iteration: 871000, loss: 5.5790732093919475\n",
            "Iteration: 872000, loss: 22.263365525143165\n",
            "Iteration: 873000, loss: 2.542324596454409\n",
            "Iteration: 874000, loss: 0.11149362681241178\n",
            "Iteration: 875000, loss: 1.0188763921911428\n",
            "Iteration: 876000, loss: 5.653657015660944\n",
            "Iteration: 877000, loss: 1.3263884753043198\n",
            "Iteration: 878000, loss: 0.04971793867149006\n",
            "Iteration: 879000, loss: 92.96018416753901\n",
            "Iteration: 880000, loss: 0.41670806681178313\n",
            "Iteration: 881000, loss: 0.7673365130498279\n",
            "Iteration: 882000, loss: 21.14585090167352\n",
            "Iteration: 883000, loss: 11.403459334776597\n",
            "Iteration: 884000, loss: 0.9798407817438254\n",
            "Iteration: 885000, loss: 9.480869688660007\n",
            "Iteration: 886000, loss: 4.66856452798471\n",
            "Iteration: 887000, loss: 7.292434602944222\n",
            "Iteration: 888000, loss: 0.18341779519623447\n",
            "Iteration: 889000, loss: 1.586762570944281\n",
            "Iteration: 890000, loss: 1.7810157460473313\n",
            "Iteration: 891000, loss: 0.20944214594977714\n",
            "Iteration: 892000, loss: 6.33168534370706\n",
            "Iteration: 893000, loss: 9.892157067968803\n",
            "Iteration: 894000, loss: 8.392327271192757\n",
            "Iteration: 895000, loss: 2.9942019881355626\n",
            "Iteration: 896000, loss: 15.1653331512481\n",
            "Iteration: 897000, loss: 0.366241421286008\n",
            "Iteration: 898000, loss: 8.42822703546937\n",
            "Iteration: 899000, loss: 31.04163380197191\n",
            "Iteration: 900000, loss: 1.8083534578122695\n",
            "Iteration: 901000, loss: 82.71324534414597\n",
            "Iteration: 902000, loss: 0.4113326389784978\n",
            "Iteration: 903000, loss: 14.0286991075013\n",
            "Iteration: 904000, loss: 3.0721071778665214\n",
            "Iteration: 905000, loss: 3.3535158211503315\n",
            "Iteration: 906000, loss: 150.9257440857161\n",
            "Iteration: 907000, loss: 0.1829979046617679\n",
            "Iteration: 908000, loss: 4.18620998582247\n",
            "Iteration: 909000, loss: 1.368410834011471\n",
            "Iteration: 910000, loss: 3.750643820289496\n",
            "Iteration: 911000, loss: 2.5171948649805573\n",
            "Iteration: 912000, loss: 0.3403303964490612\n",
            "Iteration: 913000, loss: 0.4384531140472312\n",
            "Iteration: 914000, loss: 0.22050909569663887\n",
            "Iteration: 915000, loss: 7.1690314995503766\n",
            "Iteration: 916000, loss: 0.003398738251171952\n",
            "Iteration: 917000, loss: 0.5691764611916614\n",
            "Iteration: 918000, loss: 0.35688959335483206\n",
            "Iteration: 919000, loss: 127.45552581630109\n",
            "Iteration: 920000, loss: 17.15819326818238\n",
            "Iteration: 921000, loss: 9.827265506155346\n",
            "Iteration: 922000, loss: 22.374789747649732\n",
            "Iteration: 923000, loss: 82.22186687114133\n",
            "Iteration: 924000, loss: 0.03427813057214576\n",
            "Iteration: 925000, loss: 6.721740795886607\n",
            "Iteration: 926000, loss: 1.474710079895234\n",
            "Iteration: 927000, loss: 0.002380225409829951\n",
            "Iteration: 928000, loss: 44.06161430977584\n",
            "Iteration: 929000, loss: 0.19275005253119598\n",
            "Iteration: 930000, loss: 23.513209903038867\n",
            "Iteration: 931000, loss: 1.4399293998847105\n",
            "Iteration: 932000, loss: 4.8845606546820015\n",
            "Iteration: 933000, loss: 553.3302834326292\n",
            "Iteration: 934000, loss: 8.885114203478377\n",
            "Iteration: 935000, loss: 0.01430478832995331\n",
            "Iteration: 936000, loss: 2.9490606608693213\n",
            "Iteration: 937000, loss: 14.954220272095156\n",
            "Iteration: 938000, loss: 1.3469221597395926\n",
            "Iteration: 939000, loss: 19.141057369283665\n",
            "Iteration: 940000, loss: 0.004758780762844465\n",
            "Iteration: 941000, loss: 2.9564494931544103\n",
            "Iteration: 942000, loss: 26.33515116335923\n",
            "Iteration: 943000, loss: 18.962593632665122\n",
            "Iteration: 944000, loss: 0.19456338997659717\n",
            "Iteration: 945000, loss: 1.5942902546000324\n",
            "Iteration: 946000, loss: 0.05655866783341437\n",
            "Iteration: 947000, loss: 22.256359909248665\n",
            "Iteration: 948000, loss: 0.4699284238736733\n",
            "Iteration: 949000, loss: 32.51764362648091\n",
            "Iteration: 950000, loss: 6.953640047949412\n",
            "Iteration: 951000, loss: 2.7864616778123623\n",
            "Iteration: 952000, loss: 21.186051553765427\n",
            "Iteration: 953000, loss: 1.3239700572279358\n",
            "Iteration: 954000, loss: 1.642970476815312\n",
            "Iteration: 955000, loss: 0.019029052090103526\n",
            "Iteration: 956000, loss: 15.357245656585613\n",
            "Iteration: 957000, loss: 3.1953399363738884\n",
            "Iteration: 958000, loss: 585.9514823261756\n",
            "Iteration: 959000, loss: 39.70841170842132\n",
            "Iteration: 960000, loss: 32.3395569458161\n",
            "Iteration: 961000, loss: 0.13968556795905407\n",
            "Iteration: 962000, loss: 1.0039389839779038\n",
            "Iteration: 963000, loss: 48.188684656329684\n",
            "Iteration: 964000, loss: 0.31653180879610454\n",
            "Iteration: 965000, loss: 7.103621223475595\n",
            "Iteration: 966000, loss: 0.01228184422975373\n",
            "Iteration: 967000, loss: 0.1641162493049475\n",
            "Iteration: 968000, loss: 142.1563961438153\n",
            "Iteration: 969000, loss: 17.615214059197548\n",
            "Iteration: 970000, loss: 4.945023838044144\n",
            "Iteration: 971000, loss: 0.14822046819152787\n",
            "Iteration: 972000, loss: 33.10786634973948\n",
            "Iteration: 973000, loss: 12.94766501994792\n",
            "Iteration: 974000, loss: 1.639812882332844\n",
            "Iteration: 975000, loss: 0.9367506326607322\n",
            "Iteration: 976000, loss: 17.677351576788084\n",
            "Iteration: 977000, loss: 20.677156575283856\n",
            "Iteration: 978000, loss: 15.617509364187487\n",
            "Iteration: 979000, loss: 0.6281803331507566\n",
            "Iteration: 980000, loss: 1.3076289344750993\n",
            "Iteration: 981000, loss: 4.789234418332433\n",
            "Iteration: 982000, loss: 0.0013389141829186213\n",
            "Iteration: 983000, loss: 2.2008954654662083\n",
            "Iteration: 984000, loss: 0.5773666610416583\n",
            "Iteration: 985000, loss: 0.16623332114874037\n",
            "Iteration: 986000, loss: 95.83392065088742\n",
            "Iteration: 987000, loss: 0.027984130909087288\n",
            "Iteration: 988000, loss: 26.50707770250402\n",
            "Iteration: 989000, loss: 4.869624959177474\n",
            "Iteration: 990000, loss: 17.412866926206643\n",
            "Iteration: 991000, loss: 0.7494985924405154\n",
            "Iteration: 992000, loss: 10.520872332394976\n",
            "Iteration: 993000, loss: 0.05508746882507876\n",
            "Iteration: 994000, loss: 6.657632242138585\n",
            "Iteration: 995000, loss: 0.445754243206127\n",
            "Iteration: 996000, loss: 1.005809763081113\n",
            "Iteration: 997000, loss: 0.4509963672210654\n",
            "Iteration: 998000, loss: 0.25177584548241894\n",
            "Iteration: 999000, loss: 2.9959379552970837\n",
            "Mean Error in percentage: 9.840059468%\n"
          ]
        }
      ],
      "source": [
        "i_list_2b = []\n",
        "loss_list_2b = []\n",
        "\n",
        "num_iterations = 1000000\n",
        "def q2b(X_train, Y_train, learning_rate):\n",
        "  # Layer 1 with 13 output neurons with sigmoid activation. Layer 2 with one output neuron and linear activation. use mean squared loss\n",
        "\n",
        "  layer_list = [Dense((13,1), 13, learning_rate, Sigmoid()), Dense((13,1), 1, learning_rate)]\n",
        "\n",
        "  mse = MeanSquaredLossLayer()\n",
        "  nn = Neural_Network(layer_list, mse)\n",
        "  for i in range(num_iterations) :\n",
        "    size = len(X_train)\n",
        "    # Selecting a random training sample\n",
        "    index = np.random.randint(low = 0,high = size)\n",
        "    _x = X_train[index:index+1].T\n",
        "    _y = Y_train[index:index+1].reshape(1,1)\n",
        "    loss = nn.forward(_x, _y)\n",
        "    if(i%1000==0) :\n",
        "      i_list_2b.append(i)\n",
        "      loss_list_2b.append(loss)\n",
        "      print(f\"Iteration: {i}, loss: {loss}\")\n",
        "    # Backward propogation to update the weights and biases\n",
        "    nn.backward()\n",
        "   # Returning the list of dense layers\n",
        "  return nn.layer_list\n",
        "\n",
        "learning_rate = 0.00001\n",
        "# Getting the final list of dense layers after training\n",
        "final_layer_list = q2b(X_train, y_train, learning_rate)\n",
        "# Calculating the final loss after testing the trained network\n",
        "final_loss = testing(X_test, y_test, final_layer_list)\n",
        "# Printing the final mean squared loss\n",
        "print(\"Mean Error in percentage: {:.9f}%\".format(final_loss * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkqlZzSioGz1"
      },
      "source": [
        "## Plot of the Average Loss V/S Number of Iterations for 2.b\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "WGJgNOJAoF4N",
        "outputId": "1351c285-557e-42c7-dbf2-f143f84c428f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "avg_loss_2b = []\n",
        "for i in range(0, len(i_list_2b), 10):\n",
        "    avg_2b = sum(loss_list_2b[i:i+10]) / 10\n",
        "    avg_loss_2b.append(avg_2b)\n",
        "\n",
        "# Plot the average loss on the y-axis and the iteration number on the x-axis\n",
        "plt.plot(list(range(1, len(avg_loss_2b)+1)), avg_loss_2b, color='red', linestyle='-')\n",
        "# plt.plot(i_list, avg_loss, color='red', linestyle='--')\n",
        "\n",
        "plt.xlabel('Iteration number (in thousands)')\n",
        "plt.ylabel('Average loss')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzMRlVrk9NM4"
      },
      "source": [
        "## Question 2c: three layers. Layer 1 with 13 output neurons with sigmoid activation. Layer 2 with 13 output neurons and sigmoid activation. Layer 3 with one output neuron and linear activation. use mean squared loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa9BY15R9sJ-",
        "outputId": "ed197ebb-f1d3-44e6-e816-b645cc6e1cfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 0, loss: 394.2989439670646\n",
            "Iteration: 1000, loss: 205.2749091615667\n",
            "Iteration: 2000, loss: 398.8157331367635\n",
            "Iteration: 3000, loss: 381.2948623180626\n",
            "Iteration: 4000, loss: 198.23853225321017\n",
            "Iteration: 5000, loss: 194.38407255044976\n",
            "Iteration: 6000, loss: 136.6633497169492\n",
            "Iteration: 7000, loss: 210.82658636583685\n",
            "Iteration: 8000, loss: 99.11657011597646\n",
            "Iteration: 9000, loss: 50.3343569979929\n",
            "Iteration: 10000, loss: 133.27179511704423\n",
            "Iteration: 11000, loss: 110.51872785685843\n",
            "Iteration: 12000, loss: 44.96808896047402\n",
            "Iteration: 13000, loss: 1022.588290100034\n",
            "Iteration: 14000, loss: 15.359475249688453\n",
            "Iteration: 15000, loss: 28.14426260327689\n",
            "Iteration: 16000, loss: 140.96309401927445\n",
            "Iteration: 17000, loss: 36.58928755977067\n",
            "Iteration: 18000, loss: 19.51996707640828\n",
            "Iteration: 19000, loss: 28.94473294825413\n",
            "Iteration: 20000, loss: 7.426037912802712\n",
            "Iteration: 21000, loss: 0.6597248334756796\n",
            "Iteration: 22000, loss: 20.064634968948333\n",
            "Iteration: 23000, loss: 69.51474753904793\n",
            "Iteration: 24000, loss: 1.0748895443620599\n",
            "Iteration: 25000, loss: 899.2539018886622\n",
            "Iteration: 26000, loss: 1.9278876721603382\n",
            "Iteration: 27000, loss: 8.315473436597637\n",
            "Iteration: 28000, loss: 217.06483795866296\n",
            "Iteration: 29000, loss: 6.206245560809854\n",
            "Iteration: 30000, loss: 8.193020059375216\n",
            "Iteration: 31000, loss: 3.3366619648378038\n",
            "Iteration: 32000, loss: 74.52597668820297\n",
            "Iteration: 33000, loss: 41.275362735843586\n",
            "Iteration: 34000, loss: 97.06222657384238\n",
            "Iteration: 35000, loss: 379.4162941133862\n",
            "Iteration: 36000, loss: 0.3130072514769392\n",
            "Iteration: 37000, loss: 110.09757113582678\n",
            "Iteration: 38000, loss: 94.4333011198151\n",
            "Iteration: 39000, loss: 158.21410809761525\n",
            "Iteration: 40000, loss: 6.794671930283603\n",
            "Iteration: 41000, loss: 650.776609381145\n",
            "Iteration: 42000, loss: 216.17213951177325\n",
            "Iteration: 43000, loss: 142.60458434797337\n",
            "Iteration: 44000, loss: 5.929754682952711\n",
            "Iteration: 45000, loss: 11.057006022341657\n",
            "Iteration: 46000, loss: 10.150550424421702\n",
            "Iteration: 47000, loss: 511.11706495980576\n",
            "Iteration: 48000, loss: 0.050358125739413805\n",
            "Iteration: 49000, loss: 71.13132583047526\n",
            "Iteration: 50000, loss: 9.118941417704011\n",
            "Iteration: 51000, loss: 134.91580510258672\n",
            "Iteration: 52000, loss: 135.54748891062582\n",
            "Iteration: 53000, loss: 33.156409075006074\n",
            "Iteration: 54000, loss: 0.95042681383886\n",
            "Iteration: 55000, loss: 0.9391697316287579\n",
            "Iteration: 56000, loss: 0.07039538537058948\n",
            "Iteration: 57000, loss: 13.105404028617942\n",
            "Iteration: 58000, loss: 31.867204363691485\n",
            "Iteration: 59000, loss: 56.33306672145406\n",
            "Iteration: 60000, loss: 0.9671087610596281\n",
            "Iteration: 61000, loss: 0.0006173371958400773\n",
            "Iteration: 62000, loss: 23.80270099390431\n",
            "Iteration: 63000, loss: 7.686457276752056\n",
            "Iteration: 64000, loss: 7.456107886334049\n",
            "Iteration: 65000, loss: 1.190636569770569\n",
            "Iteration: 66000, loss: 119.5698129470156\n",
            "Iteration: 67000, loss: 22.868815208019864\n",
            "Iteration: 68000, loss: 41.93974930122874\n",
            "Iteration: 69000, loss: 9.455824184717576\n",
            "Iteration: 70000, loss: 41.97850608859805\n",
            "Iteration: 71000, loss: 36.97388689641892\n",
            "Iteration: 72000, loss: 1.4379751015121978\n",
            "Iteration: 73000, loss: 137.50915352469735\n",
            "Iteration: 74000, loss: 4.669104682875358\n",
            "Iteration: 75000, loss: 0.031216470767080333\n",
            "Iteration: 76000, loss: 50.20022192795879\n",
            "Iteration: 77000, loss: 9.840274089783934\n",
            "Iteration: 78000, loss: 583.8560442737951\n",
            "Iteration: 79000, loss: 10.76653846442575\n",
            "Iteration: 80000, loss: 358.3475320091023\n",
            "Iteration: 81000, loss: 22.224466907421405\n",
            "Iteration: 82000, loss: 70.84829059532704\n",
            "Iteration: 83000, loss: 89.93712093490869\n",
            "Iteration: 84000, loss: 2.365855478769262\n",
            "Iteration: 85000, loss: 2.2780826705047765\n",
            "Iteration: 86000, loss: 47.55348617422745\n",
            "Iteration: 87000, loss: 1.396465741841344\n",
            "Iteration: 88000, loss: 20.169390700181832\n",
            "Iteration: 89000, loss: 12.794353872172398\n",
            "Iteration: 90000, loss: 37.236771928330086\n",
            "Iteration: 91000, loss: 53.35513202833173\n",
            "Iteration: 92000, loss: 10.584618541275036\n",
            "Iteration: 93000, loss: 3.338414785170076\n",
            "Iteration: 94000, loss: 14.019621426027772\n",
            "Iteration: 95000, loss: 1.631070642628017\n",
            "Iteration: 96000, loss: 19.1486548660836\n",
            "Iteration: 97000, loss: 231.08264676108607\n",
            "Iteration: 98000, loss: 2.2803269313709267\n",
            "Iteration: 99000, loss: 15.779028601013334\n",
            "Iteration: 100000, loss: 0.041319557284360715\n",
            "Iteration: 101000, loss: 56.76147157211201\n",
            "Iteration: 102000, loss: 4.775814605070794\n",
            "Iteration: 103000, loss: 83.0855863058293\n",
            "Iteration: 104000, loss: 77.13214629200759\n",
            "Iteration: 105000, loss: 22.472986382973627\n",
            "Iteration: 106000, loss: 84.8037103358308\n",
            "Iteration: 107000, loss: 3.3195289600745275\n",
            "Iteration: 108000, loss: 24.995764308338515\n",
            "Iteration: 109000, loss: 0.039341145965501347\n",
            "Iteration: 110000, loss: 0.12233168809403948\n",
            "Iteration: 111000, loss: 0.16264882820958845\n",
            "Iteration: 112000, loss: 4.073312386110623\n",
            "Iteration: 113000, loss: 34.08817022158706\n",
            "Iteration: 114000, loss: 8.934688002724721\n",
            "Iteration: 115000, loss: 5.740508638273981\n",
            "Iteration: 116000, loss: 37.16376072599048\n",
            "Iteration: 117000, loss: 41.238488299397915\n",
            "Iteration: 118000, loss: 0.07068873017789368\n",
            "Iteration: 119000, loss: 6.102340782014327\n",
            "Iteration: 120000, loss: 0.07920076857462215\n",
            "Iteration: 121000, loss: 5.68242634253515\n",
            "Iteration: 122000, loss: 9.751432187993604\n",
            "Iteration: 123000, loss: 71.48053997895464\n",
            "Iteration: 124000, loss: 22.339610488928354\n",
            "Iteration: 125000, loss: 28.694890560824383\n",
            "Iteration: 126000, loss: 1.4096506983504444\n",
            "Iteration: 127000, loss: 27.747157682013658\n",
            "Iteration: 128000, loss: 52.03808245226843\n",
            "Iteration: 129000, loss: 1.0184597126730806\n",
            "Iteration: 130000, loss: 41.25316408275382\n",
            "Iteration: 131000, loss: 497.5502897977022\n",
            "Iteration: 132000, loss: 12.825752325698788\n",
            "Iteration: 133000, loss: 29.510873540588545\n",
            "Iteration: 134000, loss: 465.2042085329516\n",
            "Iteration: 135000, loss: 4.654080958755928\n",
            "Iteration: 136000, loss: 3.9168858540477043\n",
            "Iteration: 137000, loss: 29.796696213219104\n",
            "Iteration: 138000, loss: 0.8095827806362057\n",
            "Iteration: 139000, loss: 121.8183904316242\n",
            "Iteration: 140000, loss: 9.195972348079259\n",
            "Iteration: 141000, loss: 63.08394312534007\n",
            "Iteration: 142000, loss: 3.7843717068181473\n",
            "Iteration: 143000, loss: 11.962485928509569\n",
            "Iteration: 144000, loss: 490.27774833932756\n",
            "Iteration: 145000, loss: 252.83883157008174\n",
            "Iteration: 146000, loss: 252.2937487572558\n",
            "Iteration: 147000, loss: 21.735134648282934\n",
            "Iteration: 148000, loss: 79.12867340308364\n",
            "Iteration: 149000, loss: 6.972837749084286\n",
            "Iteration: 150000, loss: 441.64312379287077\n",
            "Iteration: 151000, loss: 2.0820919037500594\n",
            "Iteration: 152000, loss: 2.9443875205275876\n",
            "Iteration: 153000, loss: 38.36761623319677\n",
            "Iteration: 154000, loss: 0.0026011356805573653\n",
            "Iteration: 155000, loss: 8.303555823736302\n",
            "Iteration: 156000, loss: 30.499359041163856\n",
            "Iteration: 157000, loss: 10.381239161392399\n",
            "Iteration: 158000, loss: 6.9150021262578765\n",
            "Iteration: 159000, loss: 0.22107256824347776\n",
            "Iteration: 160000, loss: 410.864255752358\n",
            "Iteration: 161000, loss: 4.755356634713522\n",
            "Iteration: 162000, loss: 3.614658719473918\n",
            "Iteration: 163000, loss: 13.801904941827981\n",
            "Iteration: 164000, loss: 5.854740302079485\n",
            "Iteration: 165000, loss: 14.296359994830278\n",
            "Iteration: 166000, loss: 0.12140183827623577\n",
            "Iteration: 167000, loss: 2.8793121164120343\n",
            "Iteration: 168000, loss: 408.9191537213347\n",
            "Iteration: 169000, loss: 4.516293821329313\n",
            "Iteration: 170000, loss: 390.84530648566636\n",
            "Iteration: 171000, loss: 7.204067567220269\n",
            "Iteration: 172000, loss: 0.005567218249098607\n",
            "Iteration: 173000, loss: 1.0441592790001315\n",
            "Iteration: 174000, loss: 6.903900854984152\n",
            "Iteration: 175000, loss: 15.167805835029592\n",
            "Iteration: 176000, loss: 2.2858696004266053\n",
            "Iteration: 177000, loss: 76.36504128947485\n",
            "Iteration: 178000, loss: 19.810047686985325\n",
            "Iteration: 179000, loss: 461.3749447226081\n",
            "Iteration: 180000, loss: 4.712595552324667\n",
            "Iteration: 181000, loss: 44.83692313964565\n",
            "Iteration: 182000, loss: 0.1041383984649066\n",
            "Iteration: 183000, loss: 14.907177002897672\n",
            "Iteration: 184000, loss: 17.249024360356014\n",
            "Iteration: 185000, loss: 7.7846955806944\n",
            "Iteration: 186000, loss: 21.922948914118592\n",
            "Iteration: 187000, loss: 9.39452259078716\n",
            "Iteration: 188000, loss: 5.253453274308066\n",
            "Iteration: 189000, loss: 0.7316896114473082\n",
            "Iteration: 190000, loss: 12.394630124942953\n",
            "Iteration: 191000, loss: 8.99754891349194\n",
            "Iteration: 192000, loss: 7.497780373118104\n",
            "Iteration: 193000, loss: 10.842630294937912\n",
            "Iteration: 194000, loss: 3.0558991294113342\n",
            "Iteration: 195000, loss: 4.875777775690007\n",
            "Iteration: 196000, loss: 3.775412766939885\n",
            "Iteration: 197000, loss: 92.46360623727298\n",
            "Iteration: 198000, loss: 60.224678738787574\n",
            "Iteration: 199000, loss: 2.622318366824885\n",
            "Iteration: 200000, loss: 0.0011829433680550553\n",
            "Iteration: 201000, loss: 36.76938566293784\n",
            "Iteration: 202000, loss: 0.22572991424696237\n",
            "Iteration: 203000, loss: 58.384185070478196\n",
            "Iteration: 204000, loss: 1.7405365904449448\n",
            "Iteration: 205000, loss: 28.084117744420308\n",
            "Iteration: 206000, loss: 5.004703771346254\n",
            "Iteration: 207000, loss: 1.5512130039723275\n",
            "Iteration: 208000, loss: 38.76905887184353\n",
            "Iteration: 209000, loss: 168.66151769965\n",
            "Iteration: 210000, loss: 0.04398615075572099\n",
            "Iteration: 211000, loss: 10.00666516571462\n",
            "Iteration: 212000, loss: 20.368369868116574\n",
            "Iteration: 213000, loss: 62.32399952841725\n",
            "Iteration: 214000, loss: 0.5606561536981767\n",
            "Iteration: 215000, loss: 0.8884909025085564\n",
            "Iteration: 216000, loss: 15.574358777126081\n",
            "Iteration: 217000, loss: 0.0002059647126091873\n",
            "Iteration: 218000, loss: 1.4191770996305328\n",
            "Iteration: 219000, loss: 2.1273591525827578\n",
            "Iteration: 220000, loss: 3.171396096091687\n",
            "Iteration: 221000, loss: 0.7303814759354312\n",
            "Iteration: 222000, loss: 61.71887013848127\n",
            "Iteration: 223000, loss: 11.341437232882615\n",
            "Iteration: 224000, loss: 1.5706260410707822\n",
            "Iteration: 225000, loss: 1.9016384648718092\n",
            "Iteration: 226000, loss: 0.0016871895561020257\n",
            "Iteration: 227000, loss: 1.0995490470208047\n",
            "Iteration: 228000, loss: 19.626233397482526\n",
            "Iteration: 229000, loss: 287.3444135763791\n",
            "Iteration: 230000, loss: 1.3417516703508594\n",
            "Iteration: 231000, loss: 608.0885561861522\n",
            "Iteration: 232000, loss: 1.5555211692337276\n",
            "Iteration: 233000, loss: 8.788796967293633\n",
            "Iteration: 234000, loss: 12.500595757859438\n",
            "Iteration: 235000, loss: 6.5551052454740075\n",
            "Iteration: 236000, loss: 16.164484371336723\n",
            "Iteration: 237000, loss: 0.08085327803783678\n",
            "Iteration: 238000, loss: 0.22337495306997887\n",
            "Iteration: 239000, loss: 3.8964270706536714\n",
            "Iteration: 240000, loss: 0.49329359572700876\n",
            "Iteration: 241000, loss: 59.80801304233493\n",
            "Iteration: 242000, loss: 74.69147523197434\n",
            "Iteration: 243000, loss: 15.48980693977206\n",
            "Iteration: 244000, loss: 6.620662143240841\n",
            "Iteration: 245000, loss: 1.0502698475255117\n",
            "Iteration: 246000, loss: 0.15670622578578963\n",
            "Iteration: 247000, loss: 2.354832543684986\n",
            "Iteration: 248000, loss: 34.24050221372184\n",
            "Iteration: 249000, loss: 0.24076965155893276\n",
            "Iteration: 250000, loss: 1.2405463406466446\n",
            "Iteration: 251000, loss: 1.7708194282605207\n",
            "Iteration: 252000, loss: 12.566598053590546\n",
            "Iteration: 253000, loss: 0.24860888421974023\n",
            "Iteration: 254000, loss: 2.7281125219313163\n",
            "Iteration: 255000, loss: 8.038540768640635\n",
            "Iteration: 256000, loss: 6.34302901399374\n",
            "Iteration: 257000, loss: 26.308569503702095\n",
            "Iteration: 258000, loss: 1.7023712811663412\n",
            "Iteration: 259000, loss: 119.05073552014082\n",
            "Iteration: 260000, loss: 0.0019297641335383692\n",
            "Iteration: 261000, loss: 0.9881431447476571\n",
            "Iteration: 262000, loss: 5.297684530822053\n",
            "Iteration: 263000, loss: 1.0898511593731703\n",
            "Iteration: 264000, loss: 15.969936549103\n",
            "Iteration: 265000, loss: 0.12248933694762541\n",
            "Iteration: 266000, loss: 34.8456646240605\n",
            "Iteration: 267000, loss: 1.3840969149555824\n",
            "Iteration: 268000, loss: 119.5327750589463\n",
            "Iteration: 269000, loss: 1.7476299472522174\n",
            "Iteration: 270000, loss: 122.05218666814312\n",
            "Iteration: 271000, loss: 0.18707063391411743\n",
            "Iteration: 272000, loss: 109.15225046866139\n",
            "Iteration: 273000, loss: 1.7751015935437306\n",
            "Iteration: 274000, loss: 35.463228866186874\n",
            "Iteration: 275000, loss: 0.638098321406669\n",
            "Iteration: 276000, loss: 32.8779103341901\n",
            "Iteration: 277000, loss: 16.0199261771834\n",
            "Iteration: 278000, loss: 9.663272313737304\n",
            "Iteration: 279000, loss: 1.2779795806549334\n",
            "Iteration: 280000, loss: 1.2186516836182766\n",
            "Iteration: 281000, loss: 5.950726070350405\n",
            "Iteration: 282000, loss: 0.2570278911335741\n",
            "Iteration: 283000, loss: 99.97949913678465\n",
            "Iteration: 284000, loss: 4.10936888506531\n",
            "Iteration: 285000, loss: 0.11153096356219339\n",
            "Iteration: 286000, loss: 22.576026016802523\n",
            "Iteration: 287000, loss: 9.031744449393674\n",
            "Iteration: 288000, loss: 0.21525060332429696\n",
            "Iteration: 289000, loss: 2.529865618994712\n",
            "Iteration: 290000, loss: 1.4807819458470397\n",
            "Iteration: 291000, loss: 251.6122869224042\n",
            "Iteration: 292000, loss: 10.158384745323296\n",
            "Iteration: 293000, loss: 21.57843501693215\n",
            "Iteration: 294000, loss: 140.6350245492152\n",
            "Iteration: 295000, loss: 88.74772965733032\n",
            "Iteration: 296000, loss: 1.569363513999094\n",
            "Iteration: 297000, loss: 65.1322997230775\n",
            "Iteration: 298000, loss: 6.646473210151653\n",
            "Iteration: 299000, loss: 6.033850661249573\n",
            "Iteration: 300000, loss: 9.815345438457436\n",
            "Iteration: 301000, loss: 0.057386710746836184\n",
            "Iteration: 302000, loss: 12.905526184096335\n",
            "Iteration: 303000, loss: 1.2113846018569874\n",
            "Iteration: 304000, loss: 81.38175685714307\n",
            "Iteration: 305000, loss: 13.034182441986726\n",
            "Iteration: 306000, loss: 15.300595975601796\n",
            "Iteration: 307000, loss: 10.48812860976342\n",
            "Iteration: 308000, loss: 99.54062074278744\n",
            "Iteration: 309000, loss: 5.14371634819613\n",
            "Iteration: 310000, loss: 0.7216264417871306\n",
            "Iteration: 311000, loss: 5.9964252795871795\n",
            "Iteration: 312000, loss: 15.522456242780711\n",
            "Iteration: 313000, loss: 42.37543966053187\n",
            "Iteration: 314000, loss: 0.8434435225395294\n",
            "Iteration: 315000, loss: 0.7364389676505012\n",
            "Iteration: 316000, loss: 1.8195948008677194\n",
            "Iteration: 317000, loss: 62.53382687961509\n",
            "Iteration: 318000, loss: 0.5645980298076229\n",
            "Iteration: 319000, loss: 0.9562388440818252\n",
            "Iteration: 320000, loss: 1.699046409616666\n",
            "Iteration: 321000, loss: 1.7243007299374398\n",
            "Iteration: 322000, loss: 0.9572174867539034\n",
            "Iteration: 323000, loss: 1.196810984236929\n",
            "Iteration: 324000, loss: 70.03598201328738\n",
            "Iteration: 325000, loss: 0.02314273254277613\n",
            "Iteration: 326000, loss: 53.91046992256251\n",
            "Iteration: 327000, loss: 9.478501007356638\n",
            "Iteration: 328000, loss: 8.439010569853162\n",
            "Iteration: 329000, loss: 10.313468042278002\n",
            "Iteration: 330000, loss: 47.87577699649963\n",
            "Iteration: 331000, loss: 0.055546342024024906\n",
            "Iteration: 332000, loss: 4.490571161239493\n",
            "Iteration: 333000, loss: 0.3637943033288558\n",
            "Iteration: 334000, loss: 19.19229622638369\n",
            "Iteration: 335000, loss: 229.875065935795\n",
            "Iteration: 336000, loss: 133.36186278672162\n",
            "Iteration: 337000, loss: 0.01592242873309536\n",
            "Iteration: 338000, loss: 2.6729155451040096\n",
            "Iteration: 339000, loss: 3.8647139358888505\n",
            "Iteration: 340000, loss: 23.34774034871655\n",
            "Iteration: 341000, loss: 1.4044269271652856\n",
            "Iteration: 342000, loss: 121.01773130150508\n",
            "Iteration: 343000, loss: 37.06661577042331\n",
            "Iteration: 344000, loss: 2.9874117187250246\n",
            "Iteration: 345000, loss: 4.786230362894371\n",
            "Iteration: 346000, loss: 1.073509207837172\n",
            "Iteration: 347000, loss: 0.12190187162587725\n",
            "Iteration: 348000, loss: 46.55498097163801\n",
            "Iteration: 349000, loss: 1.164967408041535\n",
            "Iteration: 350000, loss: 0.3997283238886756\n",
            "Iteration: 351000, loss: 3.5711005341809443\n",
            "Iteration: 352000, loss: 0.13809311937064808\n",
            "Iteration: 353000, loss: 1.4228460652640285\n",
            "Iteration: 354000, loss: 8.379159266434819\n",
            "Iteration: 355000, loss: 5.127682009394848\n",
            "Iteration: 356000, loss: 13.528113309482931\n",
            "Iteration: 357000, loss: 10.69338400386047\n",
            "Iteration: 358000, loss: 0.8598380896802824\n",
            "Iteration: 359000, loss: 62.44498675174467\n",
            "Iteration: 360000, loss: 1.9756624494931567\n",
            "Iteration: 361000, loss: 13.718554919560843\n",
            "Iteration: 362000, loss: 54.931222463784025\n",
            "Iteration: 363000, loss: 3.0712771886989367\n",
            "Iteration: 364000, loss: 2.619329520219576\n",
            "Iteration: 365000, loss: 61.82935286104095\n",
            "Iteration: 366000, loss: 0.3037567842499245\n",
            "Iteration: 367000, loss: 6.58885497348322\n",
            "Iteration: 368000, loss: 14.78684031488433\n",
            "Iteration: 369000, loss: 2.101620285283464\n",
            "Iteration: 370000, loss: 4.292259849171116\n",
            "Iteration: 371000, loss: 3.9436816667540393\n",
            "Iteration: 372000, loss: 0.525916378414133\n",
            "Iteration: 373000, loss: 0.23642935759650785\n",
            "Iteration: 374000, loss: 19.07225608171344\n",
            "Iteration: 375000, loss: 0.9718614372708179\n",
            "Iteration: 376000, loss: 9.453930496754937\n",
            "Iteration: 377000, loss: 1.358780748050661\n",
            "Iteration: 378000, loss: 0.06498761302874279\n",
            "Iteration: 379000, loss: 0.6943810422984991\n",
            "Iteration: 380000, loss: 3.6080874365955684\n",
            "Iteration: 381000, loss: 34.2797811637215\n",
            "Iteration: 382000, loss: 1.6755200538223638\n",
            "Iteration: 383000, loss: 109.99099809435064\n",
            "Iteration: 384000, loss: 2.1841614625658385\n",
            "Iteration: 385000, loss: 1.0489723766612649\n",
            "Iteration: 386000, loss: 0.005365381606652936\n",
            "Iteration: 387000, loss: 1.5483614003277864\n",
            "Iteration: 388000, loss: 0.04358670269300378\n",
            "Iteration: 389000, loss: 10.49502200466764\n",
            "Iteration: 390000, loss: 96.65741357951583\n",
            "Iteration: 391000, loss: 0.5380710240446743\n",
            "Iteration: 392000, loss: 0.10856280819941183\n",
            "Iteration: 393000, loss: 1.2573918226842213\n",
            "Iteration: 394000, loss: 1.0355782560057778\n",
            "Iteration: 395000, loss: 0.2958278742485106\n",
            "Iteration: 396000, loss: 0.02949749955514373\n",
            "Iteration: 397000, loss: 30.97913042143814\n",
            "Iteration: 398000, loss: 3.284306224520992\n",
            "Iteration: 399000, loss: 0.07987281767240859\n",
            "Iteration: 400000, loss: 0.23730650179999618\n",
            "Iteration: 401000, loss: 8.665932766328972\n",
            "Iteration: 402000, loss: 1.5712740033451498\n",
            "Iteration: 403000, loss: 4.359176762180563\n",
            "Iteration: 404000, loss: 0.025169349274215533\n",
            "Iteration: 405000, loss: 43.744815273832174\n",
            "Iteration: 406000, loss: 1.5937061561977255\n",
            "Iteration: 407000, loss: 38.5473701758727\n",
            "Iteration: 408000, loss: 10.100603997833291\n",
            "Iteration: 409000, loss: 0.3525392123564152\n",
            "Iteration: 410000, loss: 11.423341840115501\n",
            "Iteration: 411000, loss: 24.715597557700193\n",
            "Iteration: 412000, loss: 0.32334376348181343\n",
            "Iteration: 413000, loss: 8.250657748128127\n",
            "Iteration: 414000, loss: 1.6575320889109901\n",
            "Iteration: 415000, loss: 5.550997588340947\n",
            "Iteration: 416000, loss: 5.57121440562494\n",
            "Iteration: 417000, loss: 0.04249199136093015\n",
            "Iteration: 418000, loss: 0.602966877746375\n",
            "Iteration: 419000, loss: 4.581322818151476\n",
            "Iteration: 420000, loss: 40.53622054573645\n",
            "Iteration: 421000, loss: 3.5790343819739507\n",
            "Iteration: 422000, loss: 0.7482102240265326\n",
            "Iteration: 423000, loss: 2.7397538513548567\n",
            "Iteration: 424000, loss: 0.4595956902914282\n",
            "Iteration: 425000, loss: 16.95033073819984\n",
            "Iteration: 426000, loss: 23.982113671803734\n",
            "Iteration: 427000, loss: 7.417232950158351\n",
            "Iteration: 428000, loss: 94.22057096428723\n",
            "Iteration: 429000, loss: 1.7768982240626743\n",
            "Iteration: 430000, loss: 0.8891867509544574\n",
            "Iteration: 431000, loss: 47.181999434889576\n",
            "Iteration: 432000, loss: 1.009209116007966\n",
            "Iteration: 433000, loss: 0.7340396560674607\n",
            "Iteration: 434000, loss: 0.17190074597701516\n",
            "Iteration: 435000, loss: 143.89241455301692\n",
            "Iteration: 436000, loss: 0.36454207461994553\n",
            "Iteration: 437000, loss: 1.1467456197168409\n",
            "Iteration: 438000, loss: 2.490900414417235\n",
            "Iteration: 439000, loss: 0.013635767316823321\n",
            "Iteration: 440000, loss: 20.515221977946368\n",
            "Iteration: 441000, loss: 23.287116779061957\n",
            "Iteration: 442000, loss: 10.974389738361843\n",
            "Iteration: 443000, loss: 23.15157079634795\n",
            "Iteration: 444000, loss: 1.161553311026295\n",
            "Iteration: 445000, loss: 0.010000384596168295\n",
            "Iteration: 446000, loss: 0.7249190689440831\n",
            "Iteration: 447000, loss: 1.0204765597819658\n",
            "Iteration: 448000, loss: 15.838902575249739\n",
            "Iteration: 449000, loss: 1.776275538134282\n",
            "Iteration: 450000, loss: 4.323953551250093\n",
            "Iteration: 451000, loss: 9.026257467082104\n",
            "Iteration: 452000, loss: 43.66246777133276\n",
            "Iteration: 453000, loss: 33.42119499963947\n",
            "Iteration: 454000, loss: 0.01007427000310148\n",
            "Iteration: 455000, loss: 1.6965855278820292\n",
            "Iteration: 456000, loss: 8.782717556996813\n",
            "Iteration: 457000, loss: 30.27426486970272\n",
            "Iteration: 458000, loss: 3.5782985121515174\n",
            "Iteration: 459000, loss: 2.527496398945063\n",
            "Iteration: 460000, loss: 0.01233277573016264\n",
            "Iteration: 461000, loss: 6.086997599622416\n",
            "Iteration: 462000, loss: 2.018500046532063\n",
            "Iteration: 463000, loss: 2.5699913005272355\n",
            "Iteration: 464000, loss: 34.22042407194167\n",
            "Iteration: 465000, loss: 7.248391371157784\n",
            "Iteration: 466000, loss: 0.05332731678558541\n",
            "Iteration: 467000, loss: 0.3750395981108333\n",
            "Iteration: 468000, loss: 2.187784795997835\n",
            "Iteration: 469000, loss: 6.361877509916086\n",
            "Iteration: 470000, loss: 15.562137488211192\n",
            "Iteration: 471000, loss: 346.8690225232244\n",
            "Iteration: 472000, loss: 1.3530335017736497\n",
            "Iteration: 473000, loss: 2.7635949977300442\n",
            "Iteration: 474000, loss: 124.96466048518707\n",
            "Iteration: 475000, loss: 0.1332035343050943\n",
            "Iteration: 476000, loss: 0.5333857352222483\n",
            "Iteration: 477000, loss: 26.12236324193847\n",
            "Iteration: 478000, loss: 5.754018886365792\n",
            "Iteration: 479000, loss: 17.199350054173788\n",
            "Iteration: 480000, loss: 3.0432021368286235\n",
            "Iteration: 481000, loss: 41.841953214704745\n",
            "Iteration: 482000, loss: 4.74534993227935\n",
            "Iteration: 483000, loss: 0.0875213217887695\n",
            "Iteration: 484000, loss: 0.006320116737557446\n",
            "Iteration: 485000, loss: 42.02867538567456\n",
            "Iteration: 486000, loss: 0.14333894358656551\n",
            "Iteration: 487000, loss: 0.9913471413646305\n",
            "Iteration: 488000, loss: 21.763527408403416\n",
            "Iteration: 489000, loss: 8.399006807514345\n",
            "Iteration: 490000, loss: 0.31838495910231684\n",
            "Iteration: 491000, loss: 3.464567361259776\n",
            "Iteration: 492000, loss: 5.493684212463821\n",
            "Iteration: 493000, loss: 18.511187227112764\n",
            "Iteration: 494000, loss: 0.0048751368532501315\n",
            "Iteration: 495000, loss: 4.194731652635008\n",
            "Iteration: 496000, loss: 16.384127356719684\n",
            "Iteration: 497000, loss: 0.5080302496209222\n",
            "Iteration: 498000, loss: 0.4219541715260049\n",
            "Iteration: 499000, loss: 0.09246319443645552\n",
            "Iteration: 500000, loss: 0.013697899710868976\n",
            "Iteration: 501000, loss: 17.404714520558215\n",
            "Iteration: 502000, loss: 30.757074815920884\n",
            "Iteration: 503000, loss: 58.812608546292125\n",
            "Iteration: 504000, loss: 0.07770644336849278\n",
            "Iteration: 505000, loss: 44.452228478123985\n",
            "Iteration: 506000, loss: 70.18036810857423\n",
            "Iteration: 507000, loss: 114.74388270136689\n",
            "Iteration: 508000, loss: 15.391136881537328\n",
            "Iteration: 509000, loss: 130.37619088689675\n",
            "Iteration: 510000, loss: 3.134782832580041\n",
            "Iteration: 511000, loss: 7.771075139730822\n",
            "Iteration: 512000, loss: 42.13460996646309\n",
            "Iteration: 513000, loss: 13.012285727746853\n",
            "Iteration: 514000, loss: 0.018482527149390256\n",
            "Iteration: 515000, loss: 0.5135244691935648\n",
            "Iteration: 516000, loss: 4.795404118857456\n",
            "Iteration: 517000, loss: 66.271236869753\n",
            "Iteration: 518000, loss: 80.99070008024073\n",
            "Iteration: 519000, loss: 9.4906728776442\n",
            "Iteration: 520000, loss: 9.425971219062605\n",
            "Iteration: 521000, loss: 311.24274926142425\n",
            "Iteration: 522000, loss: 8.990422193827934\n",
            "Iteration: 523000, loss: 3.0089230444273123\n",
            "Iteration: 524000, loss: 5.933089871837707\n",
            "Iteration: 525000, loss: 5.398620030273169\n",
            "Iteration: 526000, loss: 1.6064110824301443\n",
            "Iteration: 527000, loss: 5.705145883479138\n",
            "Iteration: 528000, loss: 24.469291529869302\n",
            "Iteration: 529000, loss: 2.2558581029905533\n",
            "Iteration: 530000, loss: 9.350693669050314\n",
            "Iteration: 531000, loss: 0.6597205029289915\n",
            "Iteration: 532000, loss: 499.0355800713875\n",
            "Iteration: 533000, loss: 49.68869831200575\n",
            "Iteration: 534000, loss: 0.27430295743610744\n",
            "Iteration: 535000, loss: 1.7266852474138232\n",
            "Iteration: 536000, loss: 2.12269210809119\n",
            "Iteration: 537000, loss: 60.539729752405854\n",
            "Iteration: 538000, loss: 3.416868919380451\n",
            "Iteration: 539000, loss: 0.16519906398175951\n",
            "Iteration: 540000, loss: 3.4100332760693624\n",
            "Iteration: 541000, loss: 8.016909151610673\n",
            "Iteration: 542000, loss: 8.960631963714045\n",
            "Iteration: 543000, loss: 9.853159538654094\n",
            "Iteration: 544000, loss: 61.49627694649772\n",
            "Iteration: 545000, loss: 2.835874534549542\n",
            "Iteration: 546000, loss: 24.286924698494943\n",
            "Iteration: 547000, loss: 3.806656270209703\n",
            "Iteration: 548000, loss: 1.7662297823362854\n",
            "Iteration: 549000, loss: 0.3278732310051373\n",
            "Iteration: 550000, loss: 16.319445118290314\n",
            "Iteration: 551000, loss: 289.4146791105635\n",
            "Iteration: 552000, loss: 2.6246779042750146\n",
            "Iteration: 553000, loss: 2.121639188006213\n",
            "Iteration: 554000, loss: 9.139605877897338\n",
            "Iteration: 555000, loss: 4.756307647297272\n",
            "Iteration: 556000, loss: 9.057946590380947\n",
            "Iteration: 557000, loss: 1.4167841063132502\n",
            "Iteration: 558000, loss: 20.872250898435137\n",
            "Iteration: 559000, loss: 9.271305859380053\n",
            "Iteration: 560000, loss: 281.65639655207553\n",
            "Iteration: 561000, loss: 0.026877564938876653\n",
            "Iteration: 562000, loss: 8.191836805194622\n",
            "Iteration: 563000, loss: 0.11162602530171595\n",
            "Iteration: 564000, loss: 15.503405541456523\n",
            "Iteration: 565000, loss: 0.46594549401206653\n",
            "Iteration: 566000, loss: 1.7785906840097165\n",
            "Iteration: 567000, loss: 6.1736832629832135\n",
            "Iteration: 568000, loss: 38.817728617580144\n",
            "Iteration: 569000, loss: 2.966680171061333\n",
            "Iteration: 570000, loss: 25.596624865450924\n",
            "Iteration: 571000, loss: 91.7720399332029\n",
            "Iteration: 572000, loss: 32.5713651652118\n",
            "Iteration: 573000, loss: 479.487149640375\n",
            "Iteration: 574000, loss: 8.382844112163218\n",
            "Iteration: 575000, loss: 1.0871199599578019\n",
            "Iteration: 576000, loss: 11.999743228016792\n",
            "Iteration: 577000, loss: 8.340132600324074\n",
            "Iteration: 578000, loss: 0.022807589223985163\n",
            "Iteration: 579000, loss: 0.4516497919350218\n",
            "Iteration: 580000, loss: 22.548004247140828\n",
            "Iteration: 581000, loss: 53.89547197379777\n",
            "Iteration: 582000, loss: 48.69919158497787\n",
            "Iteration: 583000, loss: 19.460388670497903\n",
            "Iteration: 584000, loss: 6.630843042514221\n",
            "Iteration: 585000, loss: 0.6147203649597163\n",
            "Iteration: 586000, loss: 0.09408688055595826\n",
            "Iteration: 587000, loss: 0.09470483708288893\n",
            "Iteration: 588000, loss: 0.3188168497762612\n",
            "Iteration: 589000, loss: 5.010972653242605\n",
            "Iteration: 590000, loss: 0.012008416329463343\n",
            "Iteration: 591000, loss: 13.945158903623074\n",
            "Iteration: 592000, loss: 2.964127622382408\n",
            "Iteration: 593000, loss: 21.189581249225668\n",
            "Iteration: 594000, loss: 106.65369478964014\n",
            "Iteration: 595000, loss: 4.9081197698880095\n",
            "Iteration: 596000, loss: 2.373910908753319\n",
            "Iteration: 597000, loss: 5.013211569472059\n",
            "Iteration: 598000, loss: 1.5985497445952264\n",
            "Iteration: 599000, loss: 21.026057163346486\n",
            "Iteration: 600000, loss: 42.380871505483825\n",
            "Iteration: 601000, loss: 2.3468606012939324\n",
            "Iteration: 602000, loss: 0.06166351906750768\n",
            "Iteration: 603000, loss: 5.630947811371321\n",
            "Iteration: 604000, loss: 1.0504005243606371\n",
            "Iteration: 605000, loss: 28.54429282097587\n",
            "Iteration: 606000, loss: 2.313733348256349\n",
            "Iteration: 607000, loss: 0.006198845454604821\n",
            "Iteration: 608000, loss: 7.506851496442625\n",
            "Iteration: 609000, loss: 5.9663004418400805\n",
            "Iteration: 610000, loss: 8.927870301347621\n",
            "Iteration: 611000, loss: 5.01495088780884\n",
            "Iteration: 612000, loss: 59.97018435770324\n",
            "Iteration: 613000, loss: 1.8600420560515447\n",
            "Iteration: 614000, loss: 1.3479217934778203\n",
            "Iteration: 615000, loss: 2.570765584849355\n",
            "Iteration: 616000, loss: 1.3381532684639388\n",
            "Iteration: 617000, loss: 0.26397462376971925\n",
            "Iteration: 618000, loss: 37.66061248278693\n",
            "Iteration: 619000, loss: 3.9894134194994444\n",
            "Iteration: 620000, loss: 0.8765219672139772\n",
            "Iteration: 621000, loss: 3.3773878126972403\n",
            "Iteration: 622000, loss: 13.656045390353974\n",
            "Iteration: 623000, loss: 0.11426349082608882\n",
            "Iteration: 624000, loss: 69.61205266557988\n",
            "Iteration: 625000, loss: 7.545770733076317\n",
            "Iteration: 626000, loss: 5.183644832644157\n",
            "Iteration: 627000, loss: 1.0616152892262265\n",
            "Iteration: 628000, loss: 1.4398121272405564\n",
            "Iteration: 629000, loss: 0.009976621139318215\n",
            "Iteration: 630000, loss: 0.8022521391106328\n",
            "Iteration: 631000, loss: 3.3982426824659826\n",
            "Iteration: 632000, loss: 1.2471542646982323\n",
            "Iteration: 633000, loss: 18.030982957548883\n",
            "Iteration: 634000, loss: 0.984737443713942\n",
            "Iteration: 635000, loss: 21.23468870368779\n",
            "Iteration: 636000, loss: 2.506116502717275\n",
            "Iteration: 637000, loss: 4.889249805401274\n",
            "Iteration: 638000, loss: 226.97882174463766\n",
            "Iteration: 639000, loss: 0.5118220639441595\n",
            "Iteration: 640000, loss: 49.505163666559056\n",
            "Iteration: 641000, loss: 0.5028657478776912\n",
            "Iteration: 642000, loss: 0.3689868500501147\n",
            "Iteration: 643000, loss: 3.5708172513451215\n",
            "Iteration: 644000, loss: 1.0624424972918307\n",
            "Iteration: 645000, loss: 4.358240790507005\n",
            "Iteration: 646000, loss: 11.123032170238886\n",
            "Iteration: 647000, loss: 35.376682758542636\n",
            "Iteration: 648000, loss: 0.2679642852635451\n",
            "Iteration: 649000, loss: 1.5579061150437652\n",
            "Iteration: 650000, loss: 0.00010590844048974216\n",
            "Iteration: 651000, loss: 4.5309885469477535\n",
            "Iteration: 652000, loss: 1.0103853593204173\n",
            "Iteration: 653000, loss: 0.15098679566288925\n",
            "Iteration: 654000, loss: 0.3473958711975061\n",
            "Iteration: 655000, loss: 0.20940218185504775\n",
            "Iteration: 656000, loss: 8.952498980177946\n",
            "Iteration: 657000, loss: 1.4081437263607337\n",
            "Iteration: 658000, loss: 1.1872071733867067\n",
            "Iteration: 659000, loss: 0.05545405030068528\n",
            "Iteration: 660000, loss: 52.519113378524644\n",
            "Iteration: 661000, loss: 0.011581880472117457\n",
            "Iteration: 662000, loss: 0.02266094859502066\n",
            "Iteration: 663000, loss: 3.370614189049022\n",
            "Iteration: 664000, loss: 2.444323757312954\n",
            "Iteration: 665000, loss: 0.038527265914734\n",
            "Iteration: 666000, loss: 2.2946710837567488\n",
            "Iteration: 667000, loss: 1.1508053916870025\n",
            "Iteration: 668000, loss: 13.992030666929532\n",
            "Iteration: 669000, loss: 11.977021318217632\n",
            "Iteration: 670000, loss: 2.315915136572054\n",
            "Iteration: 671000, loss: 8.02924271030554\n",
            "Iteration: 672000, loss: 14.638034600581753\n",
            "Iteration: 673000, loss: 4.231246171933825\n",
            "Iteration: 674000, loss: 20.844463963276272\n",
            "Iteration: 675000, loss: 2.9727954642610004\n",
            "Iteration: 676000, loss: 202.66280795585857\n",
            "Iteration: 677000, loss: 1.1331457994103828\n",
            "Iteration: 678000, loss: 17.092204146689262\n",
            "Iteration: 679000, loss: 2.6858233777045255\n",
            "Iteration: 680000, loss: 1.0836875436102762\n",
            "Iteration: 681000, loss: 0.012371619167902066\n",
            "Iteration: 682000, loss: 0.18571309684833165\n",
            "Iteration: 683000, loss: 15.517700439484724\n",
            "Iteration: 684000, loss: 16.47289966250666\n",
            "Iteration: 685000, loss: 2.0472318248547845\n",
            "Iteration: 686000, loss: 7.0916689215625\n",
            "Iteration: 687000, loss: 5.760954912152935\n",
            "Iteration: 688000, loss: 63.51994355123475\n",
            "Iteration: 689000, loss: 4.734113136498665\n",
            "Iteration: 690000, loss: 10.605799583086355\n",
            "Iteration: 691000, loss: 1.4641860856044029\n",
            "Iteration: 692000, loss: 9.532419831027633\n",
            "Iteration: 693000, loss: 0.401036772317058\n",
            "Iteration: 694000, loss: 4.078486331619259\n",
            "Iteration: 695000, loss: 10.248553439174488\n",
            "Iteration: 696000, loss: 0.05153101137384808\n",
            "Iteration: 697000, loss: 1.4447191161249378\n",
            "Iteration: 698000, loss: 0.7657833329905022\n",
            "Iteration: 699000, loss: 2.9273104874824174\n",
            "Iteration: 700000, loss: 12.10121832360986\n",
            "Iteration: 701000, loss: 8.916881102947197\n",
            "Iteration: 702000, loss: 1.9703513716642767\n",
            "Iteration: 703000, loss: 0.007989368006903533\n",
            "Iteration: 704000, loss: 18.007449691162453\n",
            "Iteration: 705000, loss: 0.0017924672484930166\n",
            "Iteration: 706000, loss: 2.2590505163068877\n",
            "Iteration: 707000, loss: 12.83613774943297\n",
            "Iteration: 708000, loss: 19.035813883201445\n",
            "Iteration: 709000, loss: 4.789374461164524\n",
            "Iteration: 710000, loss: 0.047247349204352834\n",
            "Iteration: 711000, loss: 2.191897318560835\n",
            "Iteration: 712000, loss: 7.528893202564607\n",
            "Iteration: 713000, loss: 2.9928479059596755\n",
            "Iteration: 714000, loss: 14.804666990297703\n",
            "Iteration: 715000, loss: 0.17421173677682783\n",
            "Iteration: 716000, loss: 3.254062785272884\n",
            "Iteration: 717000, loss: 21.482756304789017\n",
            "Iteration: 718000, loss: 1.1181711180616603\n",
            "Iteration: 719000, loss: 42.40346682188606\n",
            "Iteration: 720000, loss: 0.8380458773770394\n",
            "Iteration: 721000, loss: 1.021491556760569\n",
            "Iteration: 722000, loss: 0.451533113840241\n",
            "Iteration: 723000, loss: 7.77269155877144\n",
            "Iteration: 724000, loss: 43.57240186722293\n",
            "Iteration: 725000, loss: 9.413921592516791\n",
            "Iteration: 726000, loss: 6.412054463397135\n",
            "Iteration: 727000, loss: 0.9699023271201128\n",
            "Iteration: 728000, loss: 2.0827484891562773\n",
            "Iteration: 729000, loss: 0.5727523822001215\n",
            "Iteration: 730000, loss: 0.6257994695279276\n",
            "Iteration: 731000, loss: 3.914870701914676\n",
            "Iteration: 732000, loss: 0.5638384667691263\n",
            "Iteration: 733000, loss: 3.7562628041208486\n",
            "Iteration: 734000, loss: 77.9588699965189\n",
            "Iteration: 735000, loss: 1.3794907144328048\n",
            "Iteration: 736000, loss: 0.08168550854629766\n",
            "Iteration: 737000, loss: 1.2505747379425995\n",
            "Iteration: 738000, loss: 2.33296054804666\n",
            "Iteration: 739000, loss: 26.43028909418254\n",
            "Iteration: 740000, loss: 0.3083132612882314\n",
            "Iteration: 741000, loss: 19.03243788465825\n",
            "Iteration: 742000, loss: 11.178093902765026\n",
            "Iteration: 743000, loss: 1.5378913253984474\n",
            "Iteration: 744000, loss: 24.242395269770057\n",
            "Iteration: 745000, loss: 0.0007817008820740004\n",
            "Iteration: 746000, loss: 0.8092469299751508\n",
            "Iteration: 747000, loss: 0.7366250416110669\n",
            "Iteration: 748000, loss: 0.20445430953101332\n",
            "Iteration: 749000, loss: 2.152787633536527\n",
            "Iteration: 750000, loss: 0.17941764759010906\n",
            "Iteration: 751000, loss: 7.911008016826292\n",
            "Iteration: 752000, loss: 11.489167768907375\n",
            "Iteration: 753000, loss: 5.579896731465581\n",
            "Iteration: 754000, loss: 1.0802236069521927\n",
            "Iteration: 755000, loss: 22.86554218864522\n",
            "Iteration: 756000, loss: 3.6369171628279466\n",
            "Iteration: 757000, loss: 18.191240982709477\n",
            "Iteration: 758000, loss: 1.0660169349355553\n",
            "Iteration: 759000, loss: 6.840831444697767\n",
            "Iteration: 760000, loss: 1.6521821538438515\n",
            "Iteration: 761000, loss: 26.068194023982414\n",
            "Iteration: 762000, loss: 9.78282819015575\n",
            "Iteration: 763000, loss: 1.8723467144662378\n",
            "Iteration: 764000, loss: 2.225324638149078\n",
            "Iteration: 765000, loss: 2.796671401583896\n",
            "Iteration: 766000, loss: 8.506707431323976\n",
            "Iteration: 767000, loss: 0.432327096927919\n",
            "Iteration: 768000, loss: 0.6276820250805469\n",
            "Iteration: 769000, loss: 0.039194216250263855\n",
            "Iteration: 770000, loss: 1.1979746184567166\n",
            "Iteration: 771000, loss: 0.09790745358863191\n",
            "Iteration: 772000, loss: 0.20458177999264976\n",
            "Iteration: 773000, loss: 18.38592709281853\n",
            "Iteration: 774000, loss: 6.0766761227183075\n",
            "Iteration: 775000, loss: 132.66045210454348\n",
            "Iteration: 776000, loss: 0.6820947993656874\n",
            "Iteration: 777000, loss: 25.667180716452034\n",
            "Iteration: 778000, loss: 34.74505517428359\n",
            "Iteration: 779000, loss: 0.2765909027363799\n",
            "Iteration: 780000, loss: 4.368252775159453\n",
            "Iteration: 781000, loss: 2.6346177590019177\n",
            "Iteration: 782000, loss: 0.1252752624412276\n",
            "Iteration: 783000, loss: 72.11688880891829\n",
            "Iteration: 784000, loss: 0.2074326130495328\n",
            "Iteration: 785000, loss: 0.6714845747311639\n",
            "Iteration: 786000, loss: 16.547067326163948\n",
            "Iteration: 787000, loss: 7.528294656768639\n",
            "Iteration: 788000, loss: 1.3094436364899644\n",
            "Iteration: 789000, loss: 4.990876048953354\n",
            "Iteration: 790000, loss: 1.4949164826930261\n",
            "Iteration: 791000, loss: 0.6574528629461329\n",
            "Iteration: 792000, loss: 3.5843730339308695\n",
            "Iteration: 793000, loss: 0.1118699906297705\n",
            "Iteration: 794000, loss: 0.6947745840897829\n",
            "Iteration: 795000, loss: 4.917524029471728\n",
            "Iteration: 796000, loss: 0.41210462025711275\n",
            "Iteration: 797000, loss: 1.2305780316139039\n",
            "Iteration: 798000, loss: 4.216022272619911\n",
            "Iteration: 799000, loss: 81.60468198407483\n",
            "Iteration: 800000, loss: 0.08384634725373749\n",
            "Iteration: 801000, loss: 22.876144790751674\n",
            "Iteration: 802000, loss: 2.8986405935624675\n",
            "Iteration: 803000, loss: 43.44486860834063\n",
            "Iteration: 804000, loss: 18.150962367861318\n",
            "Iteration: 805000, loss: 1.7989390789093636\n",
            "Iteration: 806000, loss: 23.418775775775146\n",
            "Iteration: 807000, loss: 2.813526750181847\n",
            "Iteration: 808000, loss: 0.08796895389825692\n",
            "Iteration: 809000, loss: 8.17247998129905\n",
            "Iteration: 810000, loss: 4.90255701948748\n",
            "Iteration: 811000, loss: 5.149384750747172\n",
            "Iteration: 812000, loss: 6.96630667257591\n",
            "Iteration: 813000, loss: 5.393174620205446\n",
            "Iteration: 814000, loss: 7.197057207357526\n",
            "Iteration: 815000, loss: 0.3674037985619796\n",
            "Iteration: 816000, loss: 9.669271294635413\n",
            "Iteration: 817000, loss: 1.0315022142326378\n",
            "Iteration: 818000, loss: 30.323517318902557\n",
            "Iteration: 819000, loss: 0.37083356335355255\n",
            "Iteration: 820000, loss: 0.019663007620196484\n",
            "Iteration: 821000, loss: 0.8090552373193661\n",
            "Iteration: 822000, loss: 10.288219801039547\n",
            "Iteration: 823000, loss: 0.05195015489316789\n",
            "Iteration: 824000, loss: 31.978837624694865\n",
            "Iteration: 825000, loss: 0.7530181471698951\n",
            "Iteration: 826000, loss: 9.268395424522948\n",
            "Iteration: 827000, loss: 0.09314461509382126\n",
            "Iteration: 828000, loss: 3.1085325617148385\n",
            "Iteration: 829000, loss: 6.153433012290214\n",
            "Iteration: 830000, loss: 0.2761984496005285\n",
            "Iteration: 831000, loss: 25.277568466440282\n",
            "Iteration: 832000, loss: 1.0915413989012428\n",
            "Iteration: 833000, loss: 16.25994742102537\n",
            "Iteration: 834000, loss: 15.360933987932905\n",
            "Iteration: 835000, loss: 0.6242572944309319\n",
            "Iteration: 836000, loss: 6.989869743871276\n",
            "Iteration: 837000, loss: 49.85184505248009\n",
            "Iteration: 838000, loss: 27.804997561119766\n",
            "Iteration: 839000, loss: 263.48856696198953\n",
            "Iteration: 840000, loss: 0.5716597608635327\n",
            "Iteration: 841000, loss: 1.0105334462884652\n",
            "Iteration: 842000, loss: 18.65004290469364\n",
            "Iteration: 843000, loss: 0.6033070222369326\n",
            "Iteration: 844000, loss: 5.971805532180347\n",
            "Iteration: 845000, loss: 10.516364605238365\n",
            "Iteration: 846000, loss: 29.894086801940237\n",
            "Iteration: 847000, loss: 20.474146843943817\n",
            "Iteration: 848000, loss: 23.81995032328116\n",
            "Iteration: 849000, loss: 22.30201988522402\n",
            "Iteration: 850000, loss: 3.4598365784520846\n",
            "Iteration: 851000, loss: 1.105575527341459\n",
            "Iteration: 852000, loss: 9.314735068162838\n",
            "Iteration: 853000, loss: 18.04001545316013\n",
            "Iteration: 854000, loss: 1.0314344644829485\n",
            "Iteration: 855000, loss: 0.5826617381646653\n",
            "Iteration: 856000, loss: 188.37778634704893\n",
            "Iteration: 857000, loss: 0.42085692912087136\n",
            "Iteration: 858000, loss: 0.012903957605509433\n",
            "Iteration: 859000, loss: 2.7937817003948733\n",
            "Iteration: 860000, loss: 0.8912119784870441\n",
            "Iteration: 861000, loss: 0.6215500450732397\n",
            "Iteration: 862000, loss: 0.05039786226088196\n",
            "Iteration: 863000, loss: 25.364270692523956\n",
            "Iteration: 864000, loss: 17.786370479989955\n",
            "Iteration: 865000, loss: 5.1977883848864295\n",
            "Iteration: 866000, loss: 24.365418845218386\n",
            "Iteration: 867000, loss: 1.1129022582743957\n",
            "Iteration: 868000, loss: 41.12066783993082\n",
            "Iteration: 869000, loss: 2.0825604986496864\n",
            "Iteration: 870000, loss: 6.8474845694216055\n",
            "Iteration: 871000, loss: 0.4029978257238335\n",
            "Iteration: 872000, loss: 14.248296300587507\n",
            "Iteration: 873000, loss: 1.4805367476052937\n",
            "Iteration: 874000, loss: 23.013978702620157\n",
            "Iteration: 875000, loss: 13.199813355209164\n",
            "Iteration: 876000, loss: 7.420514515355587\n",
            "Iteration: 877000, loss: 1.2467738778566597\n",
            "Iteration: 878000, loss: 0.05081296058065554\n",
            "Iteration: 879000, loss: 15.3423532723687\n",
            "Iteration: 880000, loss: 0.28541937681199686\n",
            "Iteration: 881000, loss: 49.4132159892744\n",
            "Iteration: 882000, loss: 2.003827191212906\n",
            "Iteration: 883000, loss: 0.01000599508139845\n",
            "Iteration: 884000, loss: 5.306202116885806\n",
            "Iteration: 885000, loss: 0.4059050531206791\n",
            "Iteration: 886000, loss: 6.436070783991397\n",
            "Iteration: 887000, loss: 24.51876572048986\n",
            "Iteration: 888000, loss: 37.34480032091241\n",
            "Iteration: 889000, loss: 0.050778012017067234\n",
            "Iteration: 890000, loss: 5.245185757961523\n",
            "Iteration: 891000, loss: 0.19110572909427387\n",
            "Iteration: 892000, loss: 0.3085575433856192\n",
            "Iteration: 893000, loss: 0.6634865305049354\n",
            "Iteration: 894000, loss: 11.48510064328259\n",
            "Iteration: 895000, loss: 1.281464009091276\n",
            "Iteration: 896000, loss: 2.9087318169563146\n",
            "Iteration: 897000, loss: 5.016334609183489\n",
            "Iteration: 898000, loss: 0.18928058566132958\n",
            "Iteration: 899000, loss: 0.016380207655807916\n",
            "Iteration: 900000, loss: 1.2482009032216417\n",
            "Iteration: 901000, loss: 1.0535129771869443\n",
            "Iteration: 902000, loss: 3.163861724244134\n",
            "Iteration: 903000, loss: 0.29264163384167735\n",
            "Iteration: 904000, loss: 1.6807193700808207\n",
            "Iteration: 905000, loss: 2.168341443041109\n",
            "Iteration: 906000, loss: 0.2740046984837756\n",
            "Iteration: 907000, loss: 1.0074553460056401\n",
            "Iteration: 908000, loss: 0.47164348477214246\n",
            "Iteration: 909000, loss: 0.033940634312465065\n",
            "Iteration: 910000, loss: 137.31525953786914\n",
            "Iteration: 911000, loss: 40.451255802866214\n",
            "Iteration: 912000, loss: 0.9526377394382727\n",
            "Iteration: 913000, loss: 0.008845167400023111\n",
            "Iteration: 914000, loss: 1.4068551609529187\n",
            "Iteration: 915000, loss: 1.1320397342595803\n",
            "Iteration: 916000, loss: 12.989509217831023\n",
            "Iteration: 917000, loss: 12.446431244428936\n",
            "Iteration: 918000, loss: 3.327011558192583\n",
            "Iteration: 919000, loss: 1.2636635850342388\n",
            "Iteration: 920000, loss: 17.26287357938564\n",
            "Iteration: 921000, loss: 0.6420015255278894\n",
            "Iteration: 922000, loss: 2.8180329292493302e-05\n",
            "Iteration: 923000, loss: 6.276873834606055\n",
            "Iteration: 924000, loss: 6.656427761783894\n",
            "Iteration: 925000, loss: 0.22243591107446758\n",
            "Iteration: 926000, loss: 0.020403167956344066\n",
            "Iteration: 927000, loss: 10.887430890523795\n",
            "Iteration: 928000, loss: 0.1909538412237411\n",
            "Iteration: 929000, loss: 4.888196991017046\n",
            "Iteration: 930000, loss: 26.049236810740688\n",
            "Iteration: 931000, loss: 27.75145399936848\n",
            "Iteration: 932000, loss: 3.6242961131669076\n",
            "Iteration: 933000, loss: 33.09941418922062\n",
            "Iteration: 934000, loss: 0.08926000341853975\n",
            "Iteration: 935000, loss: 0.8153760551453324\n",
            "Iteration: 936000, loss: 0.5707718078964027\n",
            "Iteration: 937000, loss: 5.2438243670844615\n",
            "Iteration: 938000, loss: 0.5989100232234815\n",
            "Iteration: 939000, loss: 8.885070859422095\n",
            "Iteration: 940000, loss: 5.717486468767026\n",
            "Iteration: 941000, loss: 2.0605836392447845\n",
            "Iteration: 942000, loss: 0.00010355552491454543\n",
            "Iteration: 943000, loss: 19.786038962101248\n",
            "Iteration: 944000, loss: 3.369159470474096\n",
            "Iteration: 945000, loss: 9.36087764052526\n",
            "Iteration: 946000, loss: 22.16014444859486\n",
            "Iteration: 947000, loss: 0.01617266630886653\n",
            "Iteration: 948000, loss: 0.07395590091344448\n",
            "Iteration: 949000, loss: 24.645255706818542\n",
            "Iteration: 950000, loss: 1.6438250735210274\n",
            "Iteration: 951000, loss: 1.5548583641448093\n",
            "Iteration: 952000, loss: 1.9586510558597827\n",
            "Iteration: 953000, loss: 0.011452276871441024\n",
            "Iteration: 954000, loss: 0.39104125670075085\n",
            "Iteration: 955000, loss: 0.8011698966630186\n",
            "Iteration: 956000, loss: 0.7104060517256466\n",
            "Iteration: 957000, loss: 3.434608554995164\n",
            "Iteration: 958000, loss: 64.61811399019929\n",
            "Iteration: 959000, loss: 2.927662077031824\n",
            "Iteration: 960000, loss: 30.847401261141307\n",
            "Iteration: 961000, loss: 8.290207037786852\n",
            "Iteration: 962000, loss: 4.6014155644649355\n",
            "Iteration: 963000, loss: 1.0166688805129453\n",
            "Iteration: 964000, loss: 1.1882727076813875\n",
            "Iteration: 965000, loss: 15.592091252043206\n",
            "Iteration: 966000, loss: 7.704672624985582\n",
            "Iteration: 967000, loss: 1.8158523496594703\n",
            "Iteration: 968000, loss: 0.04749780449738017\n",
            "Iteration: 969000, loss: 5.991558856256883\n",
            "Iteration: 970000, loss: 0.1289620644457883\n",
            "Iteration: 971000, loss: 6.338315337617483\n",
            "Iteration: 972000, loss: 2.8137276812569363\n",
            "Iteration: 973000, loss: 0.0845334874890335\n",
            "Iteration: 974000, loss: 0.03867827349068163\n",
            "Iteration: 975000, loss: 0.197541456488118\n",
            "Iteration: 976000, loss: 0.9803634905103864\n",
            "Iteration: 977000, loss: 0.045947984984089994\n",
            "Iteration: 978000, loss: 10.879879632181868\n",
            "Iteration: 979000, loss: 0.023293079615668486\n",
            "Iteration: 980000, loss: 0.000877588841319122\n",
            "Iteration: 981000, loss: 4.81528682410741\n",
            "Iteration: 982000, loss: 1.1420857081322073\n",
            "Iteration: 983000, loss: 11.144889943511897\n",
            "Iteration: 984000, loss: 3.1733486167151983\n",
            "Iteration: 985000, loss: 0.013279276779710593\n",
            "Iteration: 986000, loss: 22.41972651330409\n",
            "Iteration: 987000, loss: 1.6200388962455123\n",
            "Iteration: 988000, loss: 1.441665036044771\n",
            "Iteration: 989000, loss: 0.029844148371737992\n",
            "Iteration: 990000, loss: 0.6380460463056606\n",
            "Iteration: 991000, loss: 3.35691991806119\n",
            "Iteration: 992000, loss: 14.08961020995217\n",
            "Iteration: 993000, loss: 7.0447183126087145\n",
            "Iteration: 994000, loss: 0.253112279617138\n",
            "Iteration: 995000, loss: 2.513383129698598\n",
            "Iteration: 996000, loss: 0.2406692529029431\n",
            "Iteration: 997000, loss: 3.282321068546953\n",
            "Iteration: 998000, loss: 67.03488280571948\n",
            "Iteration: 999000, loss: 1.3477385253463228\n",
            "Mean Error in percentage: 11.173442668%\n"
          ]
        }
      ],
      "source": [
        "i_list_2c = []\n",
        "loss_list_2c = []\n",
        "\n",
        "num_iterations = 1000000\n",
        "def q2c(X_train, Y_train, learning_rate):\n",
        "\n",
        "  # three layers: Layer 1 with 13 output neurons with sigmoid activation.\n",
        "  # Layer 2 with 13 output neurons and sigmoid activation. \n",
        "  # Layer 3 with one output neuron and linear activation. use mean squared loss\n",
        "\n",
        "  layer_list = [Dense((13,1), 13,learning_rate, Sigmoid()), Dense((13,1), 13, learning_rate, Sigmoid()), Dense((13,1), 1, learning_rate)]\n",
        "  mse = MeanSquaredLossLayer()\n",
        "  nn = Neural_Network(layer_list, mse)\n",
        "\n",
        "  # Train the model for specified number of iterations\n",
        "  for i in range(num_iterations) :\n",
        "    size = len(X_train)\n",
        "    index = np.random.randint(low = 0,high = size)\n",
        "    _x = X_train[index:index+1].T\n",
        "    _y = Y_train[index:index+1].reshape(1,1)\n",
        "    loss = nn.forward(_x, _y)\n",
        "    if(i%1000==0) :\n",
        "      i_list_2c.append(i)\n",
        "      loss_list_2c.append(loss)\n",
        "      print(f\"Iteration: {i}, loss: {loss}\")\n",
        "    nn.backward()\n",
        "  \n",
        "  return nn.layer_list\n",
        "\n",
        "learning_rate = 0.00001\n",
        "final_layer_list = q2c(X_train, y_train, learning_rate)\n",
        "final_loss = testing(X_test, y_test, final_layer_list)\n",
        "# Printing the final mean squared loss\n",
        "print(\"Mean Error in percentage: {:.9f}%\".format(final_loss * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvQDbf5UpfmZ"
      },
      "source": [
        "## Plot of the Average Loss V/S Number of Iterations for 2.c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "n2NBrG2XpgDz",
        "outputId": "fd11fada-a243-4d6b-82c6-3ae9cedb264b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "avg_loss_2c = []\n",
        "for i in range(0, len(i_list_2c), 10):\n",
        "    avg_2c = sum(loss_list_2c[i:i+10]) / 10\n",
        "    avg_loss_2c.append(avg_2c)\n",
        "\n",
        "# Plot the average loss on the y-axis and the iteration number on the x-axis\n",
        "plt.plot(list(range(1, len(avg_loss_2c)+1)), avg_loss_2c, color='red', linestyle='-')\n",
        "# plt.plot(i_list, avg_loss, color='red', linestyle='--')\n",
        "\n",
        "plt.xlabel('Iteration number (in thousands)')\n",
        "plt.ylabel('Average loss')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suioQMNo3i4w"
      },
      "source": [
        "## Question 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFVTBmPZ3brw"
      },
      "source": [
        "## Loading the mnist (sklearn) dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5hUL8lE7mXmn"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Load the digits dataset into the variable \"mnist\"\n",
        "mnist = load_digits()\n",
        "\n",
        "# Store the features (pixel values) in the variable X_\n",
        "X_ = (mnist.data)\n",
        "# Store the target (digit labels) in the variable Y_\n",
        "Y_ = (mnist.target)\n",
        "\n",
        "X_train, X_test, Y_train_, Y_test_ = train_test_split(X_, Y_, test_size=0.10, random_state=42)\n",
        "\n",
        "# Convert the training target labels into a one-hot encoded representation\n",
        "Y_train = np.zeros((Y_train_.shape[0], 10))\n",
        "Y_train[np.arange(Y_train_.shape[0]), Y_train_] = 1\n",
        "\n",
        "# Convert the testing target labels into a one-hot encoded representation\n",
        "Y_test = np.zeros((Y_test_.shape[0], 10))\n",
        "Y_test[np.arange(Y_test_.shape[0]), Y_test_] = 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0UlO6-v39wl"
      },
      "source": [
        "### Testing function for calculating the accuracy of our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VB0f7iTKg6za"
      },
      "outputs": [],
      "source": [
        "def testing_accuracy2(X_test, Y_test, layer_list):\n",
        "  correct=0\n",
        "  for i in range (len(X_test)):\n",
        "    y_pred = X_test[i:i+1].T\n",
        "    for layer_i in layer_list:\n",
        "      y_pred = layer_i.forward(y_pred)\n",
        "\n",
        "    # Get the index of the maximum prediction value\n",
        "    mxi = y_pred.argmax(axis=0)\n",
        "\n",
        "    # Check if the prediction matches the ground truth with a confidence of 0.9\n",
        "    if(Y_test[i][mxi[0]]>0.9) :\n",
        "      # If the prediction matches, increase the correct count\n",
        "      correct += 1\n",
        "  # Return the accuracy as the ratio of correct predictions to the total number of samples\n",
        "  return correct / (len(Y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jde78ccD3oQb"
      },
      "source": [
        "##Question 3a: \n",
        "##Two layers. Layer 1 with 89 output neurons with tanh activation. Layer 2 with ten output neuron and sigmoid activation. use mean squared loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppMAI_8QHAaH",
        "outputId": "b7e2d7d2-f503-466f-a148-5d0ec6867228"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 0, loss: 0.494376191301623\n",
            "Iteration: 2000, loss: 0.25682223767488704\n",
            "Iteration: 4000, loss: 0.055328459532889826\n",
            "Iteration: 6000, loss: 0.10002319645684721\n",
            "Iteration: 8000, loss: 0.10212132519113408\n",
            "Iteration: 10000, loss: 0.16431788202501768\n",
            "Iteration: 12000, loss: 0.10005830479079506\n",
            "Iteration: 14000, loss: 0.10002702324874804\n",
            "Iteration: 16000, loss: 0.1000000058316433\n",
            "Iteration: 18000, loss: 0.0999870970833433\n",
            "Iteration: 20000, loss: 0.02374690847112147\n",
            "Iteration: 22000, loss: 0.09992954910724818\n",
            "Iteration: 24000, loss: 0.0001259368527392687\n",
            "Iteration: 26000, loss: 0.08866089620597556\n",
            "Iteration: 28000, loss: 0.09999981255665272\n",
            "Iteration: 30000, loss: 0.09992108620389716\n",
            "Iteration: 32000, loss: 0.1000005267877219\n",
            "Iteration: 34000, loss: 0.10212784750880351\n",
            "Iteration: 36000, loss: 0.0021334986021039363\n",
            "Iteration: 38000, loss: 0.003554503563827366\n",
            "Iteration: 40000, loss: 0.08956862227177362\n",
            "Iteration: 42000, loss: 0.0009238516297367683\n",
            "Iteration: 44000, loss: 0.10002273117359109\n",
            "Iteration: 46000, loss: 0.10056377096579469\n",
            "Iteration: 48000, loss: 1.1465990783237927e-06\n",
            "Iteration: 50000, loss: 0.10002207006429183\n",
            "Iteration: 52000, loss: 1.6294333189770486e-05\n",
            "Iteration: 54000, loss: 0.10010158096832895\n",
            "Iteration: 56000, loss: 2.3173037889339676e-05\n",
            "Iteration: 58000, loss: 0.09998797432863912\n",
            "Iteration: 60000, loss: 0.10004424590327025\n",
            "Iteration: 62000, loss: 0.1011183780224331\n",
            "Iteration: 64000, loss: 0.1004946748542553\n",
            "Iteration: 66000, loss: 4.5232309428670364e-06\n",
            "Iteration: 68000, loss: 0.0999913400803302\n",
            "Iteration: 70000, loss: 1.0498166861017628e-05\n",
            "Iteration: 72000, loss: 0.006196698410594841\n",
            "Iteration: 74000, loss: 0.10289112533251717\n",
            "Iteration: 76000, loss: 0.10528713140018679\n",
            "Iteration: 78000, loss: 0.10002064203031216\n",
            "Iteration: 80000, loss: 0.00037798378628512753\n",
            "Iteration: 82000, loss: 0.10000079334071865\n",
            "Iteration: 84000, loss: 0.09999390977353242\n",
            "Iteration: 86000, loss: 0.09999705742777758\n",
            "Iteration: 88000, loss: 0.10000002632077518\n",
            "Iteration: 90000, loss: 5.702174291537392e-06\n",
            "Iteration: 92000, loss: 1.0935952714047614e-06\n",
            "Iteration: 94000, loss: 0.10000445277578907\n",
            "Iteration: 96000, loss: 0.003143062315571459\n",
            "Iteration: 98000, loss: 0.10000129391414445\n",
            "Iteration: 100000, loss: 0.10000847993581286\n",
            "Iteration: 102000, loss: 0.00022218636915980828\n",
            "Iteration: 104000, loss: 0.09999999810133553\n",
            "Iteration: 106000, loss: 4.723787615356299e-06\n",
            "Iteration: 108000, loss: 0.10191985155810765\n",
            "Iteration: 110000, loss: 0.00046637192759161987\n",
            "Iteration: 112000, loss: 0.10000077275894821\n",
            "Iteration: 114000, loss: 0.10001519840557524\n",
            "Iteration: 116000, loss: 0.00042870687122845976\n",
            "Iteration: 118000, loss: 4.802912106185839e-08\n",
            "Iteration: 120000, loss: 7.840907748614651e-07\n",
            "Iteration: 122000, loss: 0.10635768266899426\n",
            "Iteration: 124000, loss: 0.001116012517093108\n",
            "Iteration: 126000, loss: 8.382724955790201e-07\n",
            "Iteration: 128000, loss: 0.10016228985610749\n",
            "Iteration: 130000, loss: 0.09999903792206544\n",
            "Iteration: 132000, loss: 9.798082253743242e-09\n",
            "Iteration: 134000, loss: 4.4960104204162227e-07\n",
            "Iteration: 136000, loss: 0.100002248874862\n",
            "Iteration: 138000, loss: 0.10034992979429631\n",
            "Iteration: 140000, loss: 9.11259368906599e-09\n",
            "Iteration: 142000, loss: 0.098167075164383\n",
            "Iteration: 144000, loss: 0.10326266725268356\n",
            "Iteration: 146000, loss: 0.10000000065755629\n",
            "Iteration: 148000, loss: 0.100000001008556\n",
            "Iteration: 150000, loss: 0.10000520492566425\n",
            "Iteration: 152000, loss: 4.690272861487894e-07\n",
            "Iteration: 154000, loss: 0.10013444853435502\n",
            "Iteration: 156000, loss: 0.10002457509819733\n",
            "Iteration: 158000, loss: 0.10001250126819568\n",
            "Iteration: 160000, loss: 0.006390435916315368\n",
            "Iteration: 162000, loss: 9.004621120407118e-05\n",
            "Iteration: 164000, loss: 0.10000750765285515\n",
            "Iteration: 166000, loss: 9.882782034252507e-09\n",
            "Iteration: 168000, loss: 0.0019915380543543536\n",
            "Iteration: 170000, loss: 0.02235617931893056\n",
            "Iteration: 172000, loss: 0.10018478358412868\n",
            "Iteration: 174000, loss: 0.000533191250949644\n",
            "Iteration: 176000, loss: 6.057227866819255e-09\n",
            "Iteration: 178000, loss: 0.026983650112213358\n",
            "Iteration: 180000, loss: 0.10046275576142497\n",
            "Iteration: 182000, loss: 1.77139456048871e-08\n",
            "Iteration: 184000, loss: 0.10002870943794326\n",
            "Iteration: 186000, loss: 0.10109395310148872\n",
            "Iteration: 188000, loss: 1.923656033733918e-07\n",
            "Iteration: 190000, loss: 8.179323244363517e-11\n",
            "Iteration: 192000, loss: 0.10000011755905798\n",
            "Iteration: 194000, loss: 8.249099430126537e-08\n",
            "Iteration: 196000, loss: 0.09947980871697057\n",
            "Iteration: 198000, loss: 0.0004955143878337678\n",
            "Iteration: 200000, loss: 2.302095051111973e-06\n",
            "Iteration: 202000, loss: 0.104727954328802\n",
            "Iteration: 204000, loss: 0.107112423889776\n",
            "Iteration: 206000, loss: 0.10002854750398289\n",
            "Iteration: 208000, loss: 6.281060451255518e-06\n",
            "Iteration: 210000, loss: 0.10017596824524164\n",
            "Iteration: 212000, loss: 6.166640685098383e-05\n",
            "Iteration: 214000, loss: 1.9691146027214818e-07\n",
            "Iteration: 216000, loss: 0.0645269901206779\n",
            "Iteration: 218000, loss: 0.10000117642419508\n",
            "Iteration: 220000, loss: 4.4652617261051025e-08\n",
            "Iteration: 222000, loss: 0.16807887624888015\n",
            "Iteration: 224000, loss: 0.0999996401214876\n",
            "Iteration: 226000, loss: 0.10014226188229822\n",
            "Iteration: 228000, loss: 0.10000001197217953\n",
            "Iteration: 230000, loss: 0.0003355466554590787\n",
            "Iteration: 232000, loss: 0.0012240944191212412\n",
            "Iteration: 234000, loss: 3.1061988375912425e-05\n",
            "Iteration: 236000, loss: 0.10274345617762051\n",
            "Iteration: 238000, loss: 0.0015624253449951562\n",
            "Iteration: 240000, loss: 0.11116811017221666\n",
            "Iteration: 242000, loss: 0.10006583976634878\n",
            "Iteration: 244000, loss: 0.09675919762694833\n",
            "Iteration: 246000, loss: 0.0003002123390095773\n",
            "Iteration: 248000, loss: 5.207888856284362e-05\n",
            "Iteration: 250000, loss: 9.311525220786907e-10\n",
            "Iteration: 252000, loss: 0.10163074027751393\n",
            "Iteration: 254000, loss: 3.833603959200587e-06\n",
            "Iteration: 256000, loss: 0.10000012093190577\n",
            "Iteration: 258000, loss: 9.936646138430891e-06\n",
            "Iteration: 260000, loss: 4.781177116343152e-08\n",
            "Iteration: 262000, loss: 0.00012415835746094008\n",
            "Iteration: 264000, loss: 0.10019781384975208\n",
            "Iteration: 266000, loss: 0.00019731681492627856\n",
            "Iteration: 268000, loss: 0.00010678079112074578\n",
            "Iteration: 270000, loss: 0.10000040180819159\n",
            "Iteration: 272000, loss: 0.10000002452905721\n",
            "Iteration: 274000, loss: 1.056307681553095e-06\n",
            "Iteration: 276000, loss: 0.0033637453079753974\n",
            "Iteration: 278000, loss: 0.0023047430344116573\n",
            "Iteration: 280000, loss: 0.09849085975989892\n",
            "Iteration: 282000, loss: 4.050282684381161e-08\n",
            "Iteration: 284000, loss: 0.10000007443973521\n",
            "Iteration: 286000, loss: 5.313606803016305e-07\n",
            "Iteration: 288000, loss: 0.09516947892814691\n",
            "Iteration: 290000, loss: 0.0968799409532652\n",
            "Iteration: 292000, loss: 7.823648787493127e-09\n",
            "Iteration: 294000, loss: 0.1000035124839388\n",
            "Iteration: 296000, loss: 2.1143526819451338e-05\n",
            "Iteration: 298000, loss: 6.552890099314993e-06\n",
            "Iteration: 300000, loss: 7.619543834988455e-08\n",
            "Iteration: 302000, loss: 0.007941400264373802\n",
            "Iteration: 304000, loss: 0.00024362946854662084\n",
            "Iteration: 306000, loss: 0.11939030798746464\n",
            "Iteration: 308000, loss: 0.0015668273725100198\n",
            "Iteration: 310000, loss: 0.09999998352362802\n",
            "Iteration: 312000, loss: 5.335972854202103e-07\n",
            "Iteration: 314000, loss: 0.10003216731840489\n",
            "Iteration: 316000, loss: 7.499008891271395e-05\n",
            "Iteration: 318000, loss: 1.514371516010152e-08\n",
            "Iteration: 320000, loss: 1.1616584802696345e-08\n",
            "Iteration: 322000, loss: 4.780785213663078e-08\n",
            "Iteration: 324000, loss: 0.00043385537026159637\n",
            "Iteration: 326000, loss: 0.00032756144077940275\n",
            "Iteration: 328000, loss: 1.056800937165912e-06\n",
            "Iteration: 330000, loss: 0.10000063642808599\n",
            "Iteration: 332000, loss: 7.60633705521638e-05\n",
            "Iteration: 334000, loss: 1.3859481403422316e-05\n",
            "Iteration: 336000, loss: 0.10000197882994526\n",
            "Iteration: 338000, loss: 7.849731143247651e-07\n",
            "Iteration: 340000, loss: 0.10134555467523185\n",
            "Iteration: 342000, loss: 0.10008153320012295\n",
            "Iteration: 344000, loss: 0.10097301817970736\n",
            "Iteration: 346000, loss: 0.08532666339385928\n",
            "Iteration: 348000, loss: 0.10001441460027423\n",
            "Iteration: 350000, loss: 4.3143184677156646e-07\n",
            "Iteration: 352000, loss: 0.00023558736840309193\n",
            "Iteration: 354000, loss: 2.895083628277068e-06\n",
            "Iteration: 356000, loss: 0.09910284550949491\n",
            "Iteration: 358000, loss: 0.10018165410345899\n",
            "Iteration: 360000, loss: 9.663543262811193e-08\n",
            "Iteration: 362000, loss: 0.0003495843962147808\n",
            "Iteration: 364000, loss: 0.1000100655923734\n",
            "Iteration: 366000, loss: 0.0013772726753953734\n",
            "Iteration: 368000, loss: 0.000534174616854855\n",
            "Iteration: 370000, loss: 2.4165168900385118e-06\n",
            "Iteration: 372000, loss: 0.10000947396951605\n",
            "Iteration: 374000, loss: 0.0002559376311998688\n",
            "Iteration: 376000, loss: 3.278461415161207e-05\n",
            "Iteration: 378000, loss: 1.3013825640266727e-05\n",
            "Iteration: 380000, loss: 0.007646906039023582\n",
            "Iteration: 382000, loss: 0.10003147148395601\n",
            "Iteration: 384000, loss: 0.10770339308976443\n",
            "Iteration: 386000, loss: 0.1009433573467898\n",
            "Iteration: 388000, loss: 0.09999996324282986\n",
            "Iteration: 390000, loss: 1.975081696014301e-06\n",
            "Iteration: 392000, loss: 0.10052017985449975\n",
            "Iteration: 394000, loss: 0.0978944718213659\n",
            "Iteration: 396000, loss: 0.008894880349738328\n",
            "Iteration: 398000, loss: 0.02101618223684684\n",
            "Iteration: 400000, loss: 2.6625196815598454e-07\n",
            "Iteration: 402000, loss: 6.579169857769625e-05\n",
            "Iteration: 404000, loss: 1.1507823332218623e-06\n",
            "Iteration: 406000, loss: 0.0016795190391547417\n",
            "Iteration: 408000, loss: 0.09600711286104507\n",
            "Iteration: 410000, loss: 4.778729722724146e-07\n",
            "Iteration: 412000, loss: 0.09938103462699385\n",
            "Iteration: 414000, loss: 1.4940375775260314e-06\n",
            "Iteration: 416000, loss: 0.10030011792275015\n",
            "Iteration: 418000, loss: 4.278056311747704e-07\n",
            "Iteration: 420000, loss: 0.10000068599427121\n",
            "Iteration: 422000, loss: 7.848959841798556e-07\n",
            "Iteration: 424000, loss: 0.10008376721182513\n",
            "Iteration: 426000, loss: 0.015927254670993436\n",
            "Iteration: 428000, loss: 0.009734269128052293\n",
            "Iteration: 430000, loss: 0.10190082187680609\n",
            "Iteration: 432000, loss: 2.6544211148178845e-10\n",
            "Iteration: 434000, loss: 0.0005156633123186939\n",
            "Iteration: 436000, loss: 0.0002295921809244681\n",
            "Iteration: 438000, loss: 0.09998579288033986\n",
            "Iteration: 440000, loss: 0.10016152281523656\n",
            "Iteration: 442000, loss: 0.10058694623220679\n",
            "Iteration: 444000, loss: 7.3905689093956785e-06\n",
            "Iteration: 446000, loss: 0.0017397749828258278\n",
            "Iteration: 448000, loss: 5.7598966462516075e-05\n",
            "Iteration: 450000, loss: 0.10000201911807054\n",
            "Iteration: 452000, loss: 2.2711750414015363e-06\n",
            "Iteration: 454000, loss: 4.431876121405664e-09\n",
            "Iteration: 456000, loss: 0.09999765537201019\n",
            "Iteration: 458000, loss: 0.014578343648445239\n",
            "Iteration: 460000, loss: 0.022593203275229668\n",
            "Iteration: 462000, loss: 0.1000930097038755\n",
            "Iteration: 464000, loss: 0.1186511568000054\n",
            "Iteration: 466000, loss: 2.0759598054481415e-05\n",
            "Iteration: 468000, loss: 4.7114130598903224e-07\n",
            "Iteration: 470000, loss: 0.10111638831412877\n",
            "Iteration: 472000, loss: 4.4879188363592073e-05\n",
            "Iteration: 474000, loss: 0.10000009930213341\n",
            "Iteration: 476000, loss: 1.1115201251473877e-06\n",
            "Iteration: 478000, loss: 0.03819699000120856\n",
            "Iteration: 480000, loss: 0.10000000995219596\n",
            "Iteration: 482000, loss: 0.0016263177096144741\n",
            "Iteration: 484000, loss: 0.10005190277262624\n",
            "Iteration: 486000, loss: 8.467011954687079e-08\n",
            "Iteration: 488000, loss: 2.890892743904095e-08\n",
            "Iteration: 490000, loss: 2.4299545100716196e-06\n",
            "Iteration: 492000, loss: 0.003930068890892305\n",
            "Iteration: 494000, loss: 0.0065694428165562965\n",
            "Iteration: 496000, loss: 0.10199475821324591\n",
            "Iteration: 498000, loss: 1.1899147833405256e-05\n",
            "Iteration: 500000, loss: 1.8581110529649124e-08\n",
            "Iteration: 502000, loss: 2.3752230126408734e-06\n",
            "Iteration: 504000, loss: 0.0003402901442605883\n",
            "Iteration: 506000, loss: 0.10000261918202709\n",
            "Iteration: 508000, loss: 0.10000080890456767\n",
            "Iteration: 510000, loss: 0.0012170169504553189\n",
            "Iteration: 512000, loss: 0.10001742455848066\n",
            "Iteration: 514000, loss: 0.000462239251826105\n",
            "Iteration: 516000, loss: 0.00013928672891823463\n",
            "Iteration: 518000, loss: 0.0012384154104129008\n",
            "Iteration: 520000, loss: 0.00016637598279453016\n",
            "Iteration: 522000, loss: 0.10003623552760546\n",
            "Iteration: 524000, loss: 0.07772117704544432\n",
            "Iteration: 526000, loss: 0.10000015265535503\n",
            "Iteration: 528000, loss: 2.0527354665676937e-07\n",
            "Iteration: 530000, loss: 0.10000006337520399\n",
            "Iteration: 532000, loss: 9.512727211441394e-08\n",
            "Iteration: 534000, loss: 1.8700572874645237e-05\n",
            "Iteration: 536000, loss: 0.0009182003285199882\n",
            "Iteration: 538000, loss: 0.10001711998820859\n",
            "Iteration: 540000, loss: 0.10006001108742923\n",
            "Iteration: 542000, loss: 2.2428410278072777e-06\n",
            "Iteration: 544000, loss: 2.2517656600703077e-06\n",
            "Iteration: 546000, loss: 8.498376766109633e-05\n",
            "Iteration: 548000, loss: 1.8194323328449127e-05\n",
            "Iteration: 550000, loss: 4.3822452173575506e-07\n",
            "Iteration: 552000, loss: 4.767544335812844e-05\n",
            "Iteration: 554000, loss: 0.0033563351530445525\n",
            "Iteration: 556000, loss: 2.1866569016943688e-05\n",
            "Iteration: 558000, loss: 0.003197296419046189\n",
            "Iteration: 560000, loss: 0.0007942785014713012\n",
            "Iteration: 562000, loss: 4.850257901644059e-06\n",
            "Iteration: 564000, loss: 3.527916647819125e-07\n",
            "Iteration: 566000, loss: 0.09989644999861101\n",
            "Iteration: 568000, loss: 6.074889087979309e-05\n",
            "Iteration: 570000, loss: 0.0010750420258608011\n",
            "Iteration: 572000, loss: 2.21125620363112e-07\n",
            "Iteration: 574000, loss: 2.637796072681225e-07\n",
            "Iteration: 576000, loss: 5.93097920604346e-06\n",
            "Iteration: 578000, loss: 0.015787077067191523\n",
            "Iteration: 580000, loss: 3.9263392999350316e-06\n",
            "Iteration: 582000, loss: 0.1000399838381721\n",
            "Iteration: 584000, loss: 1.5108481505439323e-06\n",
            "Iteration: 586000, loss: 0.09970427966124833\n",
            "Iteration: 588000, loss: 4.072890161032707e-08\n",
            "Iteration: 590000, loss: 5.162407613356309e-06\n",
            "Iteration: 592000, loss: 8.074990524406257e-07\n",
            "Iteration: 594000, loss: 6.468407053342617e-08\n",
            "Iteration: 596000, loss: 4.222455608596535e-09\n",
            "Iteration: 598000, loss: 7.488998141618098e-09\n",
            "Iteration: 600000, loss: 2.572717165738221e-06\n",
            "Iteration: 602000, loss: 0.000996215770114445\n",
            "Iteration: 604000, loss: 4.439836976235587e-06\n",
            "Iteration: 606000, loss: 5.939673613509419e-09\n",
            "Iteration: 608000, loss: 1.4941875891209942e-06\n",
            "Iteration: 610000, loss: 0.10000531261938254\n",
            "Iteration: 612000, loss: 0.005779015218242098\n",
            "Iteration: 614000, loss: 5.901568607521859e-07\n",
            "Iteration: 616000, loss: 0.000792320856967194\n",
            "Iteration: 618000, loss: 0.10000002511025438\n",
            "Iteration: 620000, loss: 6.281195364448843e-05\n",
            "Iteration: 622000, loss: 0.00037006122919493336\n",
            "Iteration: 624000, loss: 0.00010612246819238118\n",
            "Iteration: 626000, loss: 0.1000023449695812\n",
            "Iteration: 628000, loss: 7.332621830364711e-05\n",
            "Iteration: 630000, loss: 0.0017046845376922012\n",
            "Iteration: 632000, loss: 9.816121405123177e-05\n",
            "Iteration: 634000, loss: 4.042989677162748e-08\n",
            "Iteration: 636000, loss: 9.828200220012376e-08\n",
            "Iteration: 638000, loss: 9.590219752435342e-08\n",
            "Iteration: 640000, loss: 1.301925333684219e-07\n",
            "Iteration: 642000, loss: 2.6635254872685594e-07\n",
            "Iteration: 644000, loss: 0.00012451322065660966\n",
            "Iteration: 646000, loss: 4.5269657472272974e-06\n",
            "Iteration: 648000, loss: 2.6573091002705457e-05\n",
            "Iteration: 650000, loss: 8.536876691990767e-06\n",
            "Iteration: 652000, loss: 0.00030099105032228824\n",
            "Iteration: 654000, loss: 3.811898139827372e-06\n",
            "Iteration: 656000, loss: 0.00011541121441928763\n",
            "Iteration: 658000, loss: 0.0003512250458632061\n",
            "Iteration: 660000, loss: 6.754409412684684e-06\n",
            "Iteration: 662000, loss: 1.5913464190370533e-06\n",
            "Iteration: 664000, loss: 1.0413383201175048e-06\n",
            "Iteration: 666000, loss: 5.6381376966516155e-06\n",
            "Iteration: 668000, loss: 2.142571738072832e-06\n",
            "Iteration: 670000, loss: 0.0010887503154657263\n",
            "Iteration: 672000, loss: 3.1004940960836852e-09\n",
            "Iteration: 674000, loss: 0.0003546637453860292\n",
            "Iteration: 676000, loss: 3.5614220803127994e-05\n",
            "Iteration: 678000, loss: 5.829829513117872e-10\n",
            "Iteration: 680000, loss: 2.004106997664988e-05\n",
            "Iteration: 682000, loss: 1.5705610808582796e-07\n",
            "Iteration: 684000, loss: 1.3805332614916998e-05\n",
            "Iteration: 686000, loss: 0.00030047704338521713\n",
            "Iteration: 688000, loss: 6.29638936149065e-05\n",
            "Iteration: 690000, loss: 5.010772622122969e-08\n",
            "Iteration: 692000, loss: 1.1450209554196086e-06\n",
            "Iteration: 694000, loss: 0.10353276521625543\n",
            "Iteration: 696000, loss: 0.0006549961017272006\n",
            "Iteration: 698000, loss: 0.0017659842943326693\n",
            "Iteration: 700000, loss: 0.10000058728135197\n",
            "Iteration: 702000, loss: 0.0010991639041599637\n",
            "Iteration: 704000, loss: 5.21293535494217e-06\n",
            "Iteration: 706000, loss: 4.501589311597224e-10\n",
            "Iteration: 708000, loss: 7.013185705220363e-07\n",
            "Iteration: 710000, loss: 9.837183458346131e-05\n",
            "Iteration: 712000, loss: 4.9312610814125806e-05\n",
            "Iteration: 714000, loss: 0.0008631755208875347\n",
            "Iteration: 716000, loss: 0.10000004944449589\n",
            "Iteration: 718000, loss: 1.2745224511110731e-05\n",
            "Iteration: 720000, loss: 0.014611036479114228\n",
            "Iteration: 722000, loss: 0.00083546780895862\n",
            "Iteration: 724000, loss: 1.964485341911147e-06\n",
            "Iteration: 726000, loss: 0.0026589606901415218\n",
            "Iteration: 728000, loss: 4.2336609644475915e-09\n",
            "Iteration: 730000, loss: 0.10000000764389112\n",
            "Iteration: 732000, loss: 0.10372036700198306\n",
            "Iteration: 734000, loss: 0.10000007233440711\n",
            "Iteration: 736000, loss: 2.414473510144514e-06\n",
            "Iteration: 738000, loss: 0.09520688660662285\n",
            "Iteration: 740000, loss: 0.00011445911708698751\n",
            "Iteration: 742000, loss: 8.686672225022217e-05\n",
            "Iteration: 744000, loss: 0.10003921721387934\n",
            "Iteration: 746000, loss: 0.0005862603978673747\n",
            "Iteration: 748000, loss: 8.6777097684528e-05\n",
            "Iteration: 750000, loss: 0.10000426906719737\n",
            "Iteration: 752000, loss: 0.08180705703088388\n",
            "Iteration: 754000, loss: 0.10000280581048197\n",
            "Iteration: 756000, loss: 1.2450634157173964e-06\n",
            "Iteration: 758000, loss: 2.346052994352275e-07\n",
            "Iteration: 760000, loss: 0.1015631755942783\n",
            "Iteration: 762000, loss: 1.6291399163780426e-07\n",
            "Iteration: 764000, loss: 0.00016012230375130689\n",
            "Iteration: 766000, loss: 0.00017131701199651092\n",
            "Iteration: 768000, loss: 2.2659710967169236e-09\n",
            "Iteration: 770000, loss: 3.968127070331209e-08\n",
            "Iteration: 772000, loss: 1.921322063975541e-06\n",
            "Iteration: 774000, loss: 0.1002316218726246\n",
            "Iteration: 776000, loss: 0.004939447662477732\n",
            "Iteration: 778000, loss: 0.10000059034200873\n",
            "Iteration: 780000, loss: 0.09779092687527849\n",
            "Iteration: 782000, loss: 0.00047663499202710795\n",
            "Iteration: 784000, loss: 0.00043753209537114774\n",
            "Iteration: 786000, loss: 6.630276049817351e-07\n",
            "Iteration: 788000, loss: 0.002747566795237302\n",
            "Iteration: 790000, loss: 1.2366814667129399e-05\n",
            "Iteration: 792000, loss: 0.0008533382516347172\n",
            "Iteration: 794000, loss: 0.0007821914743845971\n",
            "Iteration: 796000, loss: 0.0006470614938197322\n",
            "Iteration: 798000, loss: 2.121197275735574e-08\n",
            "Iteration: 800000, loss: 9.846447588844338e-05\n",
            "Iteration: 802000, loss: 0.10017304788221788\n",
            "Iteration: 804000, loss: 2.050948138584014e-07\n",
            "Iteration: 806000, loss: 0.00034017587151719783\n",
            "Iteration: 808000, loss: 0.0008709329086180785\n",
            "Iteration: 810000, loss: 1.2651960110756464e-06\n",
            "Iteration: 812000, loss: 1.261628606635296e-05\n",
            "Iteration: 814000, loss: 0.10047733165833406\n",
            "Iteration: 816000, loss: 7.490082065567514e-07\n",
            "Iteration: 818000, loss: 5.533490179032456e-05\n",
            "Iteration: 820000, loss: 0.00010770399694349253\n",
            "Iteration: 822000, loss: 2.4420029521465326e-09\n",
            "Iteration: 824000, loss: 0.000382517620812009\n",
            "Iteration: 826000, loss: 2.8686100314115002e-05\n",
            "Iteration: 828000, loss: 0.10010882486807535\n",
            "Iteration: 830000, loss: 0.10006697169395044\n",
            "Iteration: 832000, loss: 3.1355462164391076e-06\n",
            "Iteration: 834000, loss: 0.0004098928504765835\n",
            "Iteration: 836000, loss: 0.10000000418502489\n",
            "Iteration: 838000, loss: 0.1000022998857533\n",
            "Iteration: 840000, loss: 0.0005359226469194051\n",
            "Iteration: 842000, loss: 0.0002737208750616764\n",
            "Iteration: 844000, loss: 1.607467394078575e-11\n",
            "Iteration: 846000, loss: 7.315408788784798e-05\n",
            "Iteration: 848000, loss: 0.10000179820855068\n",
            "Iteration: 850000, loss: 1.9623324543836797e-10\n",
            "Iteration: 852000, loss: 0.10008088520045491\n",
            "Iteration: 854000, loss: 0.002357707909273709\n",
            "Iteration: 856000, loss: 9.828163394752018e-08\n",
            "Iteration: 858000, loss: 0.0012042615653000618\n",
            "Iteration: 860000, loss: 2.1283285013930503e-08\n",
            "Iteration: 862000, loss: 3.1519740773678056e-05\n",
            "Iteration: 864000, loss: 0.00010269062558982152\n",
            "Iteration: 866000, loss: 0.1000000337990791\n",
            "Iteration: 868000, loss: 0.00031489293952279875\n",
            "Iteration: 870000, loss: 0.10000114740250773\n",
            "Iteration: 872000, loss: 0.10000001103118317\n",
            "Iteration: 874000, loss: 0.10000829077155829\n",
            "Iteration: 876000, loss: 4.480582969023366e-05\n",
            "Iteration: 878000, loss: 0.10000194130341551\n",
            "Iteration: 880000, loss: 0.0007949354301144652\n",
            "Iteration: 882000, loss: 4.526790300134899e-08\n",
            "Iteration: 884000, loss: 5.709884614651724e-10\n",
            "Iteration: 886000, loss: 2.9830415131265994e-06\n",
            "Iteration: 888000, loss: 0.0672144563607821\n",
            "Iteration: 890000, loss: 0.0028268507549690887\n",
            "Iteration: 892000, loss: 0.00448684109091479\n",
            "Iteration: 894000, loss: 0.10588805818677352\n",
            "Iteration: 896000, loss: 0.0009523399471649778\n",
            "Iteration: 898000, loss: 0.10000087301723873\n",
            "Iteration: 900000, loss: 1.008263606094253e-07\n",
            "Iteration: 902000, loss: 0.000591983194048197\n",
            "Iteration: 904000, loss: 0.00011454745794434518\n",
            "Iteration: 906000, loss: 0.1000195728254087\n",
            "Iteration: 908000, loss: 9.527332528873257e-09\n",
            "Iteration: 910000, loss: 0.10000043341218516\n",
            "Iteration: 912000, loss: 0.05448758266436662\n",
            "Iteration: 914000, loss: 5.991324541958144e-10\n",
            "Iteration: 916000, loss: 3.3190160330194817e-06\n",
            "Iteration: 918000, loss: 0.10020321178807345\n",
            "Iteration: 920000, loss: 2.6464381076654144e-07\n",
            "Iteration: 922000, loss: 6.610220868495007e-10\n",
            "Iteration: 924000, loss: 0.00066229068740435\n",
            "Iteration: 926000, loss: 0.0009317515763776768\n",
            "Iteration: 928000, loss: 6.119331817059091e-07\n",
            "Iteration: 930000, loss: 0.0004578806229983376\n",
            "Iteration: 932000, loss: 4.6995203236739455e-08\n",
            "Iteration: 934000, loss: 1.6679000789864597e-05\n",
            "Iteration: 936000, loss: 0.10030569987217822\n",
            "Iteration: 938000, loss: 0.001673501310583299\n",
            "Iteration: 940000, loss: 0.0017218608050571904\n",
            "Iteration: 942000, loss: 0.0022158592980620175\n",
            "Iteration: 944000, loss: 1.7191825556614337e-10\n",
            "Iteration: 946000, loss: 0.003437245984471303\n",
            "Iteration: 948000, loss: 1.018321793633759e-08\n",
            "Iteration: 950000, loss: 1.4502443779846852e-06\n",
            "Iteration: 952000, loss: 0.00041181697747170125\n",
            "Iteration: 954000, loss: 0.10000227970222528\n",
            "Iteration: 956000, loss: 1.554793648852604e-08\n",
            "Iteration: 958000, loss: 1.1700505253217028e-09\n",
            "Iteration: 960000, loss: 0.00027808882158919036\n",
            "Iteration: 962000, loss: 0.0001677227724092523\n",
            "Iteration: 964000, loss: 0.0017721842122626612\n",
            "Iteration: 966000, loss: 9.230055109456716e-09\n",
            "Iteration: 968000, loss: 4.7259884968225575e-05\n",
            "Iteration: 970000, loss: 0.000333048799758699\n",
            "Iteration: 972000, loss: 8.359533710391882e-05\n",
            "Iteration: 974000, loss: 6.049587994297075e-06\n",
            "Iteration: 976000, loss: 1.4455419456328715e-07\n",
            "Iteration: 978000, loss: 5.041845885993839e-07\n",
            "Iteration: 980000, loss: 0.00019080965236872985\n",
            "Iteration: 982000, loss: 9.839435979844184e-08\n",
            "Iteration: 984000, loss: 0.10004789567714605\n",
            "Iteration: 986000, loss: 1.2207069337802135e-09\n",
            "Iteration: 988000, loss: 0.100000117138546\n",
            "Iteration: 990000, loss: 0.0002932483788940199\n",
            "Iteration: 992000, loss: 5.4618734042054975e-05\n",
            "Iteration: 994000, loss: 0.0006231318563860945\n",
            "Iteration: 996000, loss: 3.402220697513055e-05\n",
            "Iteration: 998000, loss: 0.10000012223241388\n",
            "Accuracy of the Model: 72.777777778%\n"
          ]
        }
      ],
      "source": [
        "i_list = []\n",
        "loss_list = []\n",
        "num_iterations = 1000000\n",
        "\n",
        "def q3a(X_train, Y_train, learning_rate):\n",
        "  \n",
        "  # two layers. Layer 1 with 89 output neurons with tanh activation. \n",
        "  # Layer 2 with ten output neuron and sigmoid activation. use mean squared loss.\n",
        "\n",
        "  # Initialize the neural network with the layer list and the mean squared loss layer\n",
        "  layer_list = [Dense((64,1), 89, learning_rate, Tanh()), Dense((89,1), 10, learning_rate, Sigmoid())]\n",
        "  mse = MeanSquaredLossLayer()\n",
        "  nn = Neural_Network(layer_list, mse)\n",
        "\n",
        "  for i in range(1000000) :\n",
        "    size = len(X_train)\n",
        "    index = np.random.randint(low = 0,high = size)\n",
        "    _x = X_train[index:index+1].T\n",
        "    _y = Y_train[index:index+1].reshape(10, 1)      \n",
        "    loss = nn.forward(_x, _y)\n",
        "    if(i%2000==0) :\n",
        "      i_list.append(i)\n",
        "      loss_list.append(loss)\n",
        "      print(f\"Iteration: {i}, loss: {loss}\")\n",
        "    \n",
        "    # Perform the backward pass to update the weights\n",
        "    nn.backward()\n",
        "  # Return the final list of layers\n",
        "  return nn.layer_list\n",
        "\n",
        "\n",
        "learning_rate = 0.005\n",
        "# Call the function to train the neural network\n",
        "final_layer_list = q3a(X_train, Y_train, learning_rate)\n",
        "# Calculate the accuracy of the trained model on the test data\n",
        "accuracy_3a = testing_accuracy2(X_test, Y_test, final_layer_list)\n",
        "# Print the accuracy\n",
        "print(\"Accuracy of the Model: {:.9f}%\".format(accuracy_3a * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5CeoWZe3-en"
      },
      "source": [
        "### Plot of the Average Loss V/S Number of Iterations for 3.a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "c-jqUqZTw2bt",
        "outputId": "5a01dc96-00b4-4db2-bd55-c4ec53319ffb"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "avg_loss = []\n",
        "for i in range(0, len(i_list), 5):\n",
        "    avg = sum(loss_list[i:i+5]) / 5\n",
        "    avg_loss.append(avg)\n",
        "\n",
        "# Plot the average loss on the y-axis and the iteration number on the x-axis\n",
        "plt.plot(list(range(1, len(avg_loss)+1)), avg_loss, color='red', linestyle='-')\n",
        "# plt.plot(i_list, avg_loss, color='red', linestyle='--')\n",
        "\n",
        "plt.xlabel('Iteration number (in thousands)')\n",
        "plt.ylabel('Average loss')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZn49fOS5UJP"
      },
      "source": [
        "## Question 3b:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtttcJZ85X9W"
      },
      "source": [
        "##two layers. Layer 1 with 89 output neurons with tanh activation. Layer 2 with ten output neuron and linear activation. use softmax with cross entropy loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnqFHquzGs12",
        "outputId": "305cd2ef-a3fa-4751-8106-231c05c55630"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 0, loss: 30.78849868826918\n",
            "Iteration: 4000, loss: 0.19845802081241964\n",
            "Iteration: 8000, loss: 0.0002481118988891641\n",
            "Iteration: 12000, loss: 0.0035597307965355486\n",
            "Iteration: 16000, loss: 7.449445899307113e-05\n",
            "Iteration: 20000, loss: 0.0014522429776547495\n",
            "Iteration: 24000, loss: 0.0064908614794951525\n",
            "Iteration: 28000, loss: 0.4792890071890399\n",
            "Iteration: 32000, loss: 0.21176853155981598\n",
            "Iteration: 36000, loss: 0.023238046686166827\n",
            "Iteration: 40000, loss: 0.024919592930813667\n",
            "Iteration: 44000, loss: 0.0748681757240151\n",
            "Iteration: 48000, loss: 0.23769765806288362\n",
            "Iteration: 52000, loss: 0.001917783429206833\n",
            "Iteration: 56000, loss: 0.03942215379141056\n",
            "Iteration: 60000, loss: 0.025301774826134774\n",
            "Iteration: 64000, loss: 0.0026511760731970572\n",
            "Iteration: 68000, loss: 1.35879110101132\n",
            "Iteration: 72000, loss: 4.169767505724567\n",
            "Iteration: 76000, loss: 10.803750826768132\n",
            "Iteration: 80000, loss: 0.21390524825562632\n",
            "Iteration: 84000, loss: 2.7721024834869212e-05\n",
            "Iteration: 88000, loss: 0.001652174439103244\n",
            "Iteration: 92000, loss: 0.010069953694484397\n",
            "Iteration: 96000, loss: 0.0019059748395228574\n",
            "Iteration: 100000, loss: 0.00029518058233802733\n",
            "Iteration: 104000, loss: 0.0019155195612968411\n",
            "Iteration: 108000, loss: 0.015124936767119086\n",
            "Iteration: 112000, loss: 0.005637640592008992\n",
            "Iteration: 116000, loss: 0.08442994152508759\n",
            "Iteration: 120000, loss: 0.02235875100039387\n",
            "Iteration: 124000, loss: 0.08087313905749906\n",
            "Iteration: 128000, loss: 0.008152260491074786\n",
            "Iteration: 132000, loss: 0.0025054035294822838\n",
            "Iteration: 136000, loss: 0.12597716681010185\n",
            "Iteration: 140000, loss: 0.0024662043565526763\n",
            "Iteration: 144000, loss: 0.13538874479220123\n",
            "Iteration: 148000, loss: 0.0016754643952773469\n",
            "Iteration: 152000, loss: 5.556103953227828e-07\n",
            "Iteration: 156000, loss: 0.059150789756115554\n",
            "Iteration: 160000, loss: 0.0023682744859585456\n",
            "Iteration: 164000, loss: 0.0001700347119820089\n",
            "Iteration: 168000, loss: 0.0032670628623220743\n",
            "Iteration: 172000, loss: 0.002144008691597635\n",
            "Iteration: 176000, loss: 0.12789432155236274\n",
            "Iteration: 180000, loss: 0.0013710337015648516\n",
            "Iteration: 184000, loss: 0.027891330560298327\n",
            "Iteration: 188000, loss: 0.011723191140252463\n",
            "Iteration: 192000, loss: 0.002407065440560364\n",
            "Iteration: 196000, loss: 0.1023009917874442\n",
            "Iteration: 200000, loss: 1.1851752039535493e-06\n",
            "Iteration: 204000, loss: 0.001975669195346765\n",
            "Iteration: 208000, loss: 0.020448385344606364\n",
            "Iteration: 212000, loss: 0.09965410899140535\n",
            "Iteration: 216000, loss: 0.007028708125590361\n",
            "Iteration: 220000, loss: 0.004813779025664436\n",
            "Iteration: 224000, loss: 0.35146089985418294\n",
            "Iteration: 228000, loss: 0.0004411900189342003\n",
            "Iteration: 232000, loss: 0.0018327673369689247\n",
            "Iteration: 236000, loss: 0.08579637657010188\n",
            "Iteration: 240000, loss: 4.3333555462251915e-06\n",
            "Iteration: 244000, loss: 0.01886541078511992\n",
            "Iteration: 248000, loss: 0.00037412884488718554\n",
            "Iteration: 252000, loss: 4.507611190579097e-06\n",
            "Iteration: 256000, loss: 0.038654297116549295\n",
            "Iteration: 260000, loss: 0.09305286644762163\n",
            "Iteration: 264000, loss: 9.156679629646057e-06\n",
            "Iteration: 268000, loss: 0.0008609544094996421\n",
            "Iteration: 272000, loss: 0.07792237607196988\n",
            "Iteration: 276000, loss: 0.01106049317098703\n",
            "Iteration: 280000, loss: 0.002427846580709499\n",
            "Iteration: 284000, loss: 0.32728340414621426\n",
            "Iteration: 288000, loss: 0.056638175173339564\n",
            "Iteration: 292000, loss: 0.032775988660681404\n",
            "Iteration: 296000, loss: 0.09810698869396772\n",
            "Iteration: 300000, loss: 0.00044401442360808314\n",
            "Iteration: 304000, loss: 0.030489648019864986\n",
            "Iteration: 308000, loss: 0.00025453787087364733\n",
            "Iteration: 312000, loss: 3.385916077998863e-06\n",
            "Iteration: 316000, loss: 1.7967440188009458e-06\n",
            "Iteration: 320000, loss: 0.0016012333168951675\n",
            "Iteration: 324000, loss: 1.786779177598497e-06\n",
            "Iteration: 328000, loss: 0.0038596302606368244\n",
            "Iteration: 332000, loss: 0.0016212232665140562\n",
            "Iteration: 336000, loss: 3.677598979848362\n",
            "Iteration: 340000, loss: 0.16706360959958444\n",
            "Iteration: 344000, loss: 0.021014460968458742\n",
            "Iteration: 348000, loss: 0.00033836442758359875\n",
            "Iteration: 352000, loss: 0.0614650271173117\n",
            "Iteration: 356000, loss: 0.003286799309032489\n",
            "Iteration: 360000, loss: 0.00012738584026854904\n",
            "Iteration: 364000, loss: 0.006168294563520221\n",
            "Iteration: 368000, loss: 0.032066876621871666\n",
            "Iteration: 372000, loss: 3.551189294367243e-05\n",
            "Iteration: 376000, loss: 2.9123227836378015e-05\n",
            "Iteration: 380000, loss: 5.410334811520641e-05\n",
            "Iteration: 384000, loss: 0.007364559381268839\n",
            "Iteration: 388000, loss: 0.0010636306141251636\n",
            "Iteration: 392000, loss: 0.002255200429091133\n",
            "Iteration: 396000, loss: 0.0019586809511632918\n",
            "Iteration: 400000, loss: 0.0015018750699189395\n",
            "Iteration: 404000, loss: 8.582300467561524e-05\n",
            "Iteration: 408000, loss: 0.0003922622139328527\n",
            "Iteration: 412000, loss: 1.0986761693424172e-05\n",
            "Iteration: 416000, loss: 1.670696515437593e-05\n",
            "Iteration: 420000, loss: 0.008888698171105124\n",
            "Iteration: 424000, loss: 0.001997233986886943\n",
            "Iteration: 428000, loss: 0.0018313450262959668\n",
            "Iteration: 432000, loss: 0.0051653815851101246\n",
            "Iteration: 436000, loss: 0.00032864554675495774\n",
            "Iteration: 440000, loss: 0.00916244837540692\n",
            "Iteration: 444000, loss: 0.011281521232254427\n",
            "Iteration: 448000, loss: 0.0011559840401866956\n",
            "Iteration: 452000, loss: 0.043503533502000834\n",
            "Iteration: 456000, loss: 0.0017359600087799342\n",
            "Iteration: 460000, loss: 0.0014674474487695475\n",
            "Iteration: 464000, loss: 2.6415029759844715e-05\n",
            "Iteration: 468000, loss: 1.0659559897184235e-05\n",
            "Iteration: 472000, loss: 0.004775524468738716\n",
            "Iteration: 476000, loss: 0.0006210474554920509\n",
            "Iteration: 480000, loss: 0.009004559148445043\n",
            "Iteration: 484000, loss: 0.00020172850656164151\n",
            "Iteration: 488000, loss: 0.006828753129926006\n",
            "Iteration: 492000, loss: 0.009850271213160544\n",
            "Iteration: 496000, loss: 0.0006774365009369255\n",
            "Iteration: 500000, loss: 0.002996883033056463\n",
            "Iteration: 504000, loss: 0.00043362525164062753\n",
            "Iteration: 508000, loss: 0.002502918341428236\n",
            "Iteration: 512000, loss: 3.950674394027193e-05\n",
            "Iteration: 516000, loss: 0.03570576379754928\n",
            "Iteration: 520000, loss: 0.04911477540863374\n",
            "Iteration: 524000, loss: 0.005381488200911104\n",
            "Iteration: 528000, loss: 0.042045141746995154\n",
            "Iteration: 532000, loss: 0.00846517024176242\n",
            "Iteration: 536000, loss: 0.6243660646950356\n",
            "Iteration: 540000, loss: 1.4127516193656762e-06\n",
            "Iteration: 544000, loss: 0.0005747972138367395\n",
            "Iteration: 548000, loss: 3.614488754583759e-08\n",
            "Iteration: 552000, loss: 0.0008933141527284463\n",
            "Iteration: 556000, loss: 9.501270040927332e-07\n",
            "Iteration: 560000, loss: 0.1615414038742253\n",
            "Iteration: 564000, loss: 0.0022099972481140263\n",
            "Iteration: 568000, loss: 0.0009016963944369824\n",
            "Iteration: 572000, loss: 0.006345338948597323\n",
            "Iteration: 576000, loss: 0.0057036362242832905\n",
            "Iteration: 580000, loss: 0.007108322954885582\n",
            "Iteration: 584000, loss: 4.106390008797726e-06\n",
            "Iteration: 588000, loss: 0.0014389270000819444\n",
            "Iteration: 592000, loss: 0.002371212564414212\n",
            "Iteration: 596000, loss: 0.0003592063090643823\n",
            "Iteration: 600000, loss: 0.00023279875137517228\n",
            "Iteration: 604000, loss: 0.00029581249974902434\n",
            "Iteration: 608000, loss: 1.760818831376155\n",
            "Iteration: 612000, loss: 0.0003374708174725705\n",
            "Iteration: 616000, loss: 4.4007618325595424e-05\n",
            "Iteration: 620000, loss: 0.0009591417178504266\n",
            "Iteration: 624000, loss: 0.013095223623915613\n",
            "Iteration: 628000, loss: 0.0025184532192561544\n",
            "Iteration: 632000, loss: 0.007194634602265167\n",
            "Iteration: 636000, loss: 0.06493506295968789\n",
            "Iteration: 640000, loss: 0.0002684954572234812\n",
            "Iteration: 644000, loss: 1.9475212356790476e-05\n",
            "Iteration: 648000, loss: 0.04895180523553644\n",
            "Iteration: 652000, loss: 0.0013161841987040439\n",
            "Iteration: 656000, loss: 0.035554858871928406\n",
            "Iteration: 660000, loss: 0.00011259332700697925\n",
            "Iteration: 664000, loss: 0.015126218506760394\n",
            "Iteration: 668000, loss: 1.4372605727968515e-05\n",
            "Iteration: 672000, loss: 0.25441424743811875\n",
            "Iteration: 676000, loss: 0.00038708521344217275\n",
            "Iteration: 680000, loss: 0.0025979293878721907\n",
            "Iteration: 684000, loss: 0.0003985077320999655\n",
            "Iteration: 688000, loss: 0.00018003066421753692\n",
            "Iteration: 692000, loss: 0.06541399550747375\n",
            "Iteration: 696000, loss: 0.0007558589419007799\n",
            "Iteration: 700000, loss: 5.695167265688051\n",
            "Iteration: 704000, loss: 0.000656022305570277\n",
            "Iteration: 708000, loss: 0.0003913400441201686\n",
            "Iteration: 712000, loss: 0.015016955760883062\n",
            "Iteration: 716000, loss: 0.0003260263317789244\n",
            "Iteration: 720000, loss: 0.02435277004555867\n",
            "Iteration: 724000, loss: 0.0001072043878965363\n",
            "Iteration: 728000, loss: 0.09127770787101215\n",
            "Iteration: 732000, loss: 0.0005806516566438917\n",
            "Iteration: 736000, loss: 0.0014654949676224831\n",
            "Iteration: 740000, loss: 3.81896363542527e-05\n",
            "Iteration: 744000, loss: 0.15640681241607743\n",
            "Iteration: 748000, loss: 0.0007185857595967636\n",
            "Iteration: 752000, loss: 0.28772331366703513\n",
            "Iteration: 756000, loss: 0.00043561339118265284\n",
            "Iteration: 760000, loss: 0.0007339132373982681\n",
            "Iteration: 764000, loss: 0.0013197093869328769\n",
            "Iteration: 768000, loss: 0.003893539642271434\n",
            "Iteration: 772000, loss: 0.008875451726385045\n",
            "Iteration: 776000, loss: 0.34150835968961746\n",
            "Iteration: 780000, loss: 2.052880548891953\n",
            "Iteration: 784000, loss: 0.0012488254496511115\n",
            "Iteration: 788000, loss: 0.002596062249076185\n",
            "Iteration: 792000, loss: 0.0069317516157593635\n",
            "Iteration: 796000, loss: 0.00043137629810911755\n",
            "Iteration: 800000, loss: 0.00013441764286845366\n",
            "Iteration: 804000, loss: 0.00040481832394232217\n",
            "Iteration: 808000, loss: 3.743262231547051e-05\n",
            "Iteration: 812000, loss: 0.0004746930385449666\n",
            "Iteration: 816000, loss: 3.7933674675240968e-06\n",
            "Iteration: 820000, loss: 0.030110618431678303\n",
            "Iteration: 824000, loss: 0.0006541548584339164\n",
            "Iteration: 828000, loss: 0.0002510914401956055\n",
            "Iteration: 832000, loss: 0.00586069134320946\n",
            "Iteration: 836000, loss: 0.0015336666795674019\n",
            "Iteration: 840000, loss: 0.00029866128772420083\n",
            "Iteration: 844000, loss: 0.006499978790467343\n",
            "Iteration: 848000, loss: 0.00015324353956727314\n",
            "Iteration: 852000, loss: 0.00018339409325029973\n",
            "Iteration: 856000, loss: 0.005647547974880124\n",
            "Iteration: 860000, loss: 0.03343875233993381\n",
            "Iteration: 864000, loss: 0.002626532394190601\n",
            "Iteration: 868000, loss: 0.004280838137612907\n",
            "Iteration: 872000, loss: 0.00020688902746263723\n",
            "Iteration: 876000, loss: 0.00031678244956318246\n",
            "Iteration: 880000, loss: 0.5677176243216533\n",
            "Iteration: 884000, loss: 0.0031946980523036143\n",
            "Iteration: 888000, loss: 2.3365774764783122e-05\n",
            "Iteration: 892000, loss: 1.260372189554053e-05\n",
            "Iteration: 896000, loss: 0.007024082320827089\n",
            "Iteration: 900000, loss: 0.0028547326514441534\n",
            "Iteration: 904000, loss: 0.0002607198078282192\n",
            "Iteration: 908000, loss: 0.0008820263185085162\n",
            "Iteration: 912000, loss: 1.030645889259835e-05\n",
            "Iteration: 916000, loss: 0.0036721629720527843\n",
            "Iteration: 920000, loss: 0.038016465353487426\n",
            "Iteration: 924000, loss: 0.02452069122555436\n",
            "Iteration: 928000, loss: 0.00013977486792350735\n",
            "Iteration: 932000, loss: 0.003449095760668147\n",
            "Iteration: 936000, loss: 0.19285314065670667\n",
            "Iteration: 940000, loss: 7.633787846665957e-05\n",
            "Iteration: 944000, loss: 0.0013916154381411643\n",
            "Iteration: 948000, loss: 0.005215028513819789\n",
            "Iteration: 952000, loss: 0.0009055672855944467\n",
            "Iteration: 956000, loss: 0.13792072040975198\n",
            "Iteration: 960000, loss: 0.44711157754928554\n",
            "Iteration: 964000, loss: 0.10454870666613683\n",
            "Iteration: 968000, loss: 0.21237044571127378\n",
            "Iteration: 972000, loss: 0.0008794649028303635\n",
            "Iteration: 976000, loss: 0.0024090067583067785\n",
            "Iteration: 980000, loss: 0.19827566059364105\n",
            "Iteration: 984000, loss: 0.00014095055300006662\n",
            "Iteration: 988000, loss: 7.473180758827686e-05\n",
            "Iteration: 992000, loss: 4.913145873846082\n",
            "Iteration: 996000, loss: 0.00028584048795528397\n",
            "Accuracy of the Model: 89.444444444%\n"
          ]
        }
      ],
      "source": [
        "num_iterations = 1000000\n",
        "i_list_3b = []\n",
        "loss_list_3b = []\n",
        "\n",
        "def q3b(X_train, Y_train, learning_rate):\n",
        "  # two layers. Layer 1 with 89 output neurons with tanh activation. \n",
        "  # Layer 2 with ten output neuron and linear activation. use softmax with cross\n",
        "  # entropy loss.\n",
        "\n",
        "\n",
        "  layer_list = [Dense((64,1), 89, learning_rate*10, Tanh()), Dense((89,1), 10, learning_rate), SoftmaxLayer()]\n",
        "  cel = CrossEntropyLayer()\n",
        "  nn = Neural_Network(layer_list, cel)\n",
        "  for i in range(num_iterations) :\n",
        "    size = len(X_train)\n",
        "    index = np.random.randint(low = 0,high = size)\n",
        "    _x = X_train[index:index+1].T\n",
        "    _y = Y_train[index:index+1].reshape(10,1)\n",
        "    loss = nn.forward(_x, _y)\n",
        "    if(i%4000==0) :\n",
        "      i_list_3b.append(i)\n",
        "      loss_list_3b.append(loss)\n",
        "      print(f\"Iteration: {i}, loss: {loss}\")\n",
        "    # Perform the backward pass to update the weights\n",
        "    nn.backward()\n",
        "  return nn.layer_list\n",
        "\n",
        "\n",
        "learning_rate = 0.0001\n",
        "# Call the function to train the neural network\n",
        "final_layer_list = q3b(X_train, Y_train, learning_rate)\n",
        "# Calculate the accuracy of the trained model on the test data\n",
        "accuracy_3b = testing_accuracy2(X_test, Y_test, final_layer_list)\n",
        "# Print the accuracy\n",
        "print(\"Accuracy of the Model: {:.9f}%\".format(accuracy_3b * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCgQx9GF3wkD"
      },
      "source": [
        "### Plot of the Average Loss V/S Number of Iterations for 3.b\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "QXd78hpy19-J",
        "outputId": "fd8f159a-4307-442e-9dc2-44cf125a97c9"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "avg_loss_3b = []\n",
        "for i in range(0, len(i_list_3b), 2):\n",
        "    avg_3b = sum(loss_list_3b[i:i+2]) / 2\n",
        "    avg_loss_3b.append(avg_3b)\n",
        "\n",
        "# Plot the average loss on the y-axis and the iteration number on the x-axis\n",
        "plt.plot(list(range(1, len(avg_loss_3b)+1)), avg_loss_3b, color='red', linestyle='-')\n",
        "# plt.plot(i_list, avg_loss, color='red', linestyle='--')\n",
        "\n",
        "plt.xlabel('Iteration number (in thousands)')\n",
        "plt.ylabel('Average loss')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbUhm-tlkL9j"
      },
      "source": [
        "## Q.4 Implement the convolution layer for 1 channel input and (n >= 1) channel output. Implement both forward and backward passes. Implement the flatten operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPbzIuNhe6Az"
      },
      "source": [
        "## Q.5 (extra credit bonus:) generalize this for any number of input and any number of output channel. Implement both forward and backward passes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5xRt9dme-Ub"
      },
      "source": [
        "*Here we have implemented the 5th Question (for the general case i.e for any number of input and any number of output channel), keeping the Question.4 as the default case in the class implemented in Question 5.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3QYHLr2qbxk"
      },
      "source": [
        "## Cross-Correlate2d Function From Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_FyIpBuLqemM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import signal\n",
        "# Function to calculate the 2D cross-correlation of an image with a kernel.\n",
        "\n",
        "def correlate2d(image, kernel, mode='valid'):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    image (ndarray): Input image.\n",
        "    kernel (ndarray): Input kernel.\n",
        "    mode (str, optional): Mode for correlation calculation.\n",
        "        'valid': only valid positions (no padding).\n",
        "        'same': pads the input so the output has the same shape as the input.\n",
        "        'full': padding such that the output size is max(M, N) - min(M, N) + 1.\n",
        "    \"\"\"\n",
        "    image_rows, image_cols = image.shape\n",
        "    kernel_rows, kernel_cols = kernel.shape\n",
        "\n",
        "    # Perform the operation depending upon the mode selected\n",
        "\n",
        "    if mode == 'valid':\n",
        "        result = np.zeros((image_rows - kernel_rows + 1, image_cols - kernel_cols + 1))\n",
        "    elif mode == 'same':\n",
        "        result = np.zeros(image.shape)\n",
        "    elif mode == 'full':\n",
        "        result = np.zeros((image_rows + kernel_rows - 1, image_cols + kernel_cols - 1))\n",
        "    else:\n",
        "        raise ValueError(\"mode can only be 'valid', 'same' or 'full'\")\n",
        "\n",
        "    # Padding for image\n",
        "    if mode == 'same' or mode == 'full':\n",
        "        padding_rows = kernel_rows // 2\n",
        "        padding_cols = kernel_cols // 2\n",
        "        image = np.pad(image, ((padding_rows, padding_rows), (padding_cols, padding_cols)), 'constant')\n",
        "\n",
        "    # Matrix Multiplication\n",
        "    for i in range(result.shape[0]):\n",
        "        for j in range(result.shape[1]):\n",
        "            result[i][j] = (image[i:i+kernel_rows, j:j+kernel_cols] * kernel).sum()\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TItx_wEgqUky"
      },
      "source": [
        "## Convolve2d Function From Scratch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "W3TZYxFWqZHm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def convolve2d(image, kernel, mode='full'):\n",
        "    # Flip the kernel along both the horizontal and vertical axes\n",
        "    flipped_kernel = np.fliplr(np.flipud(kernel))\n",
        "    \n",
        "    # Call the convolve2d function with the flipped kernel\n",
        "    result = correlate2d(image, flipped_kernel, mode)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLpVxQyD6iMM"
      },
      "source": [
        "## Padding Matrix With Zeros Function "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kIN1zuuwdKrQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function takes an input matrix and pads it with zeros.\n",
        "# The input matrix is padded on all sides with a size of pad_size.\n",
        "def pad_with_zeros(input_matrix, pad_size):\n",
        "    return np.pad(input_matrix, [(pad_size, pad_size), (pad_size, pad_size)], 'constant')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyjO_m5HZ9iX"
      },
      "source": [
        "## Flatten Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LV1dPwrzaA4c"
      },
      "outputs": [],
      "source": [
        "class Flatten:\n",
        "    def __init__(self):\n",
        "        # Initialize an attribute to store the input shape\n",
        "        self.input_shape = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Store the input shape for the backward pass\n",
        "        self.input_shape = x.shape\n",
        "        # Flatten the input into a 1D array\n",
        "        return x.reshape(x.shape[0], -1)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        # Transpose dout\n",
        "        dout = dout.T\n",
        "        # Reshape dout back to its original shape\n",
        "        return dout.reshape(*self.input_shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3zbErrSBgmuf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class CNN():\n",
        "\n",
        "    # class constructor to initialize the CNN\n",
        "    def __init__(self, input_height, input_width, kernel_size, output_channel, learning_rate, activation=None, input_channel = 1, seed_kernel=None, seed_bias=None):\n",
        "        self.input_height = input_height\n",
        "        self.input_width = input_width\n",
        "        self.output_channel = output_channel\n",
        "        self.input_channel = input_channel\n",
        "        self.kernel_size = kernel_size\n",
        "        self.output_shape = (output_channel, input_height - kernel_size+1, input_width - kernel_size+1)\n",
        "        self.kernels_shape = (output_channel, input_channel, kernel_size, kernel_size)\n",
        "        self.kernels = np.random.randn(*self.kernels_shape)\n",
        "        self.biases = np.random.randn(*self.output_shape)\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        # use provided seeds if they are not None\n",
        "        if(seed_kernel is not None) :\n",
        "          self.kernels = seed_kernel\n",
        "        if(seed_bias is not None) :\n",
        "          self.biases = seed_bias\n",
        "        self.activation = activation\n",
        "\n",
        "    # forward pass of the CNN\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        # create a copy of the biases to store the output\n",
        "        self.output = np.copy(self.biases)\n",
        "        for i in range(self.output_channel):\n",
        "            for j in range(self.input_channel):\n",
        "              # perform cross-correlation between each input channel and corresponding kernel\n",
        "                # self.output[i] += correlate2d(self.input[j], self.kernels[i, j], \"valid\")\n",
        "                self.output[i] += signal.correlate2d(self.input[j], self.kernels[i, j], \"valid\")\n",
        "\n",
        "        # flatten the output tensor\n",
        "        self.output = self.output.reshape(-1,1) #flattening\n",
        "        \n",
        "        # perform activation if provided\n",
        "        if(self.activation is not None) :\n",
        "          self.output = self.activation.forward(self.output)\n",
        "        \n",
        "        # return the output\n",
        "        return self.output\n",
        "\n",
        "    # backward pass of the CNN\n",
        "    def backward(self, output_gradient):\n",
        "      # perform backward activation if provided\n",
        "        if(self.activation is not None) :\n",
        "          output_gradient = self.activation.backward(output_gradient)\n",
        "        \n",
        "        output_gradient = output_gradient.reshape(self.output_channel, self.input_height-self.kernel_size+1, self.input_width - self.kernel_size+1) #reversing flattening\n",
        "        \n",
        "        # initialize the gradient of the kernels and input \n",
        "        kernels_gradient = np.zeros(self.kernels_shape)\n",
        "        input_gradient = np.zeros((self.input_channel,  self.input_height,  self.input_width))\n",
        "\n",
        "        # compute the gradient of the kernels and input\n",
        "        for i in range(self.output_channel):\n",
        "            for j in range(self.input_channel):\n",
        "                kernels_gradient[i, j] = signal.correlate2d(self.input[j], output_gradient[i], \"valid\")\n",
        "                input_gradient[j] += signal.convolve2d(output_gradient[i], self.kernels[i][j], \"full\")\n",
        "\n",
        "        self.kernels -= self.learning_rate * kernels_gradient\n",
        "        self.biases -= self.learning_rate * output_gradient\n",
        "        return input_gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LecA5yA7cjz"
      },
      "source": [
        "## Testing Function for Calculating Accuracy of the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6F5-IQjlyM2e"
      },
      "outputs": [],
      "source": [
        "def testing_accuracy3(X_test, Y_test, layer_list):\n",
        "  correct=0\n",
        "  for i in range (len(X_test)):\n",
        "    y_pred = X_test[i:i+1]\n",
        "    for layer_i in layer_list:\n",
        "      y_pred = layer_i.forward(y_pred)\n",
        "    # Get the index of the maximum prediction value\n",
        "    mxi = y_pred.argmax(axis=0)\n",
        "    # Check if the prediction matches the ground truth with a confidence of 0.9\n",
        "    if(Y_test[i][mxi[0]]>0.9) :\n",
        "      # If the prediction matches, increase the correct count\n",
        "      correct += 1\n",
        "\n",
        "  # Return the accuracy as the ratio of correct predictions to the total number of samples\n",
        "  return correct / (len(Y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0WgCyvTsEwq"
      },
      "source": [
        "## Q.6 Train this CNN on mnist dataset. Layer 1: Convolution layer with 16 out-put channels+flatten+tanh activation. Layer 2: 10 output neuron with linear activation. Softmax cross entropy loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5HLC-kYl1k9B"
      },
      "outputs": [],
      "source": [
        "X_train_6 = X_train.reshape(len(X_train), 8, 8)\n",
        "X_test_6 = X_test.reshape(len(X_test), 8, 8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-Nh_j_psDwX",
        "outputId": "65e5340f-6efe-4459-ae43-50ee09b4d989"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 0, loss: 45.998373262921916\n",
            "Iteration: 2000, loss: 9.106855119438524\n",
            "Iteration: 4000, loss: 10.875693701222671\n",
            "Iteration: 6000, loss: 27.124800654750022\n",
            "Iteration: 8000, loss: 26.402787805337734\n",
            "Iteration: 10000, loss: 28.252534378142357\n",
            "Iteration: 12000, loss: 3.126562782048754\n",
            "Iteration: 14000, loss: 25.11042256118517\n",
            "Iteration: 16000, loss: 24.852429981711527\n",
            "Iteration: 18000, loss: 0.020449790328781702\n",
            "Iteration: 20000, loss: -8.505598447393633e-10\n",
            "Iteration: 22000, loss: 28.682620631726216\n",
            "Iteration: 24000, loss: -9.701928149279935e-10\n",
            "Iteration: 26000, loss: 30.67029899334876\n",
            "Iteration: 28000, loss: 28.06202422401421\n",
            "Iteration: 30000, loss: 27.561893197382542\n",
            "Iteration: 32000, loss: 1.4352017247192283e-06\n",
            "Iteration: 34000, loss: -9.758094332090874e-10\n",
            "Iteration: 36000, loss: 1.8291097771427827\n",
            "Iteration: 38000, loss: 9.025517801744437e-09\n",
            "Iteration: 40000, loss: 3.9918446361611556e-10\n",
            "Iteration: 42000, loss: 20.72732490700872\n",
            "Iteration: 44000, loss: 0.0002077920621648459\n",
            "Iteration: 46000, loss: 15.772057844866511\n",
            "Iteration: 48000, loss: 1.555844124511463\n",
            "Iteration: 50000, loss: 0.49881889840648846\n",
            "Iteration: 52000, loss: 33.72510196511094\n",
            "Iteration: 54000, loss: 15.24055717398586\n",
            "Iteration: 56000, loss: 5.106317128459828\n",
            "Iteration: 58000, loss: -9.928844412816451e-10\n",
            "Iteration: 60000, loss: 2.3988292484645864e-09\n",
            "Iteration: 62000, loss: 29.27382850333806\n",
            "Iteration: 64000, loss: 1.1801338112574318e-07\n",
            "Iteration: 66000, loss: 28.083409495847828\n",
            "Iteration: 68000, loss: 0.2642688648877778\n",
            "Iteration: 70000, loss: 24.53600169573266\n",
            "Iteration: 72000, loss: 1.5698573088167487e-07\n",
            "Iteration: 74000, loss: 7.19538902265896e-07\n",
            "Iteration: 76000, loss: 0.004273277808177673\n",
            "Iteration: 78000, loss: 0.0072018796702794955\n",
            "Iteration: 80000, loss: 0.5664030655258855\n",
            "Iteration: 82000, loss: 0.009322316167183771\n",
            "Iteration: 84000, loss: 1.7048084070602367e-09\n",
            "Iteration: 86000, loss: 0.05169128687182792\n",
            "Iteration: 88000, loss: 4.418266940637019e-07\n",
            "Iteration: 90000, loss: 3.7333555186189638\n",
            "Iteration: 92000, loss: 28.840142449153824\n",
            "Iteration: 94000, loss: 2.150011852304205\n",
            "Iteration: 96000, loss: 0.007494565416091285\n",
            "Iteration: 98000, loss: 0.003149005156400316\n",
            "Iteration: 100000, loss: 27.12228699348981\n",
            "Iteration: 102000, loss: 6.008889462557585e-06\n",
            "Iteration: 104000, loss: 0.0016424622500399544\n",
            "Iteration: 106000, loss: 0.001608993458404974\n",
            "Iteration: 108000, loss: 0.00020576403876579002\n",
            "Iteration: 110000, loss: 8.316247694761802e-10\n",
            "Iteration: 112000, loss: -9.988565529751213e-10\n",
            "Iteration: 114000, loss: 2.6369999354589635\n",
            "Iteration: 116000, loss: 15.704251701893263\n",
            "Iteration: 118000, loss: 3.115855521124318e-06\n",
            "Iteration: 120000, loss: 1.2215543395398158e-08\n",
            "Iteration: 122000, loss: 6.76967513191803e-08\n",
            "Iteration: 124000, loss: 9.15715793441681\n",
            "Iteration: 126000, loss: 12.643501100125723\n",
            "Iteration: 128000, loss: 0.058655167651612106\n",
            "Iteration: 130000, loss: -8.96589469292555e-10\n",
            "Iteration: 132000, loss: 23.219942559872795\n",
            "Iteration: 134000, loss: -7.4702910520493e-10\n",
            "Iteration: 136000, loss: 2.2288025001950398e-08\n",
            "Iteration: 138000, loss: 2.3715115905797386e-05\n",
            "Iteration: 140000, loss: 0.7102985923481506\n",
            "Iteration: 142000, loss: 1.0075818485136514\n",
            "Iteration: 144000, loss: 0.003344263694841801\n",
            "Iteration: 146000, loss: -9.993801341534822e-10\n",
            "Iteration: 148000, loss: 44.35658702942995\n",
            "Iteration: 150000, loss: 6.121107299256895e-09\n",
            "Iteration: 152000, loss: 7.453887927604752e-06\n",
            "Iteration: 154000, loss: -9.99981875032769e-10\n",
            "Iteration: 156000, loss: 1.0772457284654502e-08\n",
            "Iteration: 158000, loss: 24.366169166176324\n",
            "Iteration: 160000, loss: 1.1671543241028104e-07\n",
            "Iteration: 162000, loss: -9.999667759996356e-10\n",
            "Iteration: 164000, loss: 0.13564444185068117\n",
            "Iteration: 166000, loss: 4.186687621077952e-05\n",
            "Iteration: 168000, loss: 3.7740621766883454e-10\n",
            "Iteration: 170000, loss: -9.340088702016808e-10\n",
            "Iteration: 172000, loss: 4.932011643311631e-07\n",
            "Iteration: 174000, loss: 2.552682913935503e-05\n",
            "Iteration: 176000, loss: 12.728749698549082\n",
            "Iteration: 178000, loss: 1.638009815901356e-07\n",
            "Iteration: 180000, loss: 24.907659924462877\n",
            "Iteration: 182000, loss: 26.868184805273163\n",
            "Iteration: 184000, loss: 4.732123826163676e-05\n",
            "Iteration: 186000, loss: -1.5877297514027828e-12\n",
            "Iteration: 188000, loss: 1.3907265717202466\n",
            "Iteration: 190000, loss: 7.25817698137459e-05\n",
            "Iteration: 192000, loss: 0.25181891853324356\n",
            "Iteration: 194000, loss: 3.975507091880661e-07\n",
            "Iteration: 196000, loss: 0.0050911335772965355\n",
            "Iteration: 198000, loss: 0.4505360572859101\n",
            "Iteration: 200000, loss: 24.522970320553146\n",
            "Iteration: 202000, loss: 0.01861659796979014\n",
            "Iteration: 204000, loss: 0.25069066470767\n",
            "Iteration: 206000, loss: 1.844129985294515e-05\n",
            "Iteration: 208000, loss: 3.558311423364215e-05\n",
            "Iteration: 210000, loss: 24.482955479204364\n",
            "Iteration: 212000, loss: 9.22340262321937e-06\n",
            "Iteration: 214000, loss: 4.3586920542637225e-06\n",
            "Iteration: 216000, loss: 25.25837269318598\n",
            "Iteration: 218000, loss: 0.000308258836849898\n",
            "Iteration: 220000, loss: 1.3675275239293436e-06\n",
            "Iteration: 222000, loss: 22.611262690420936\n",
            "Iteration: 224000, loss: 0.012727053889234837\n",
            "Iteration: 226000, loss: -9.160452396196352e-10\n",
            "Iteration: 228000, loss: 1.5593397427615544e-06\n",
            "Iteration: 230000, loss: 31.101797483639974\n",
            "Iteration: 232000, loss: -9.999623351075375e-10\n",
            "Iteration: 234000, loss: 8.938287133895438e-07\n",
            "Iteration: 236000, loss: 25.255892967352548\n",
            "Iteration: 238000, loss: -1.000000082690371e-09\n",
            "Iteration: 240000, loss: -9.981548920236277e-10\n",
            "Iteration: 242000, loss: 9.865675573837792e-08\n",
            "Iteration: 244000, loss: 0.008491977214381517\n",
            "Iteration: 246000, loss: 7.094665272548759e-07\n",
            "Iteration: 248000, loss: 24.704754850939434\n",
            "Iteration: 250000, loss: -9.99938354290208e-10\n",
            "Iteration: 252000, loss: 0.0014273090429172347\n",
            "Iteration: 254000, loss: -3.135511849180274e-10\n",
            "Iteration: 256000, loss: 5.464935133070767e-06\n",
            "Iteration: 258000, loss: 3.434860508541386e-08\n",
            "Iteration: 260000, loss: 0.00032919283794553866\n",
            "Iteration: 262000, loss: 1.325015008730299e-07\n",
            "Iteration: 264000, loss: -9.825169566340856e-10\n",
            "Iteration: 266000, loss: 0.006512054109026277\n",
            "Iteration: 268000, loss: -9.99879290425304e-10\n",
            "Iteration: 270000, loss: 2.1201637121573004e-05\n",
            "Iteration: 272000, loss: 1.0908084225672528e-07\n",
            "Iteration: 274000, loss: 4.5878208869673866e-05\n",
            "Iteration: 276000, loss: 4.654087327098872e-08\n",
            "Iteration: 278000, loss: 3.523276269889306e-08\n",
            "Iteration: 280000, loss: 0.020740959808387433\n",
            "Iteration: 282000, loss: 24.423080022844594\n",
            "Iteration: 284000, loss: 1.855188455287683e-08\n",
            "Iteration: 286000, loss: 2.3960030650329163e-09\n",
            "Iteration: 288000, loss: 13.962904345676263\n",
            "Iteration: 290000, loss: 29.267002527751917\n",
            "Iteration: 292000, loss: -9.998097904639693e-10\n",
            "Iteration: 294000, loss: 12.687474802302605\n",
            "Iteration: 296000, loss: 0.08770248845523745\n",
            "Iteration: 298000, loss: 2.4874582421260817e-09\n",
            "Iteration: 300000, loss: 28.335962788106247\n",
            "Iteration: 302000, loss: 9.269000735016176e-06\n",
            "Iteration: 304000, loss: 2.4014828715039414e-08\n",
            "Iteration: 306000, loss: 0.00018572091670246774\n",
            "Iteration: 308000, loss: 0.020385232490631774\n",
            "Iteration: 310000, loss: 2.5000313776225873e-05\n",
            "Iteration: 312000, loss: -1.000000082690371e-09\n",
            "Iteration: 314000, loss: 5.888305769653063\n",
            "Iteration: 316000, loss: -9.845051440263771e-10\n",
            "Iteration: 318000, loss: 8.857151862457373e-06\n",
            "Iteration: 320000, loss: 3.2739225388554363e-08\n",
            "Iteration: 322000, loss: -9.986973469934057e-10\n",
            "Iteration: 324000, loss: 19.253691976999786\n",
            "Iteration: 326000, loss: 4.907380003469979e-06\n",
            "Iteration: 328000, loss: 0.009092804665249242\n",
            "Iteration: 330000, loss: 1.7693717513730973\n",
            "Iteration: 332000, loss: 0.0013449805231201834\n",
            "Iteration: 334000, loss: -9.994780558242444e-10\n",
            "Iteration: 336000, loss: 2.1226376665317768e-07\n",
            "Iteration: 338000, loss: 3.474366309604006e-07\n",
            "Iteration: 340000, loss: 3.9909626398869547\n",
            "Iteration: 342000, loss: 1.9226080591643918e-07\n",
            "Iteration: 344000, loss: 1.8048090322206507e-07\n",
            "Iteration: 346000, loss: 0.3562899105134862\n",
            "Iteration: 348000, loss: 27.278504407233857\n",
            "Iteration: 350000, loss: 24.206787373793677\n",
            "Iteration: 352000, loss: -3.933481317633313e-10\n",
            "Iteration: 354000, loss: 2.7319257996836813e-10\n",
            "Iteration: 356000, loss: -9.61308366196559e-10\n",
            "Iteration: 358000, loss: 1.8940383803328386e-08\n",
            "Iteration: 360000, loss: 24.634578109969777\n",
            "Iteration: 362000, loss: -9.999996386011612e-10\n",
            "Iteration: 364000, loss: 1.6594380262843552e-07\n",
            "Iteration: 366000, loss: 24.630632813282652\n",
            "Iteration: 368000, loss: 0.00396445389287632\n",
            "Iteration: 370000, loss: 0.07761547480337336\n",
            "Iteration: 372000, loss: 3.1759355904385265\n",
            "Iteration: 374000, loss: 26.35169982536443\n",
            "Iteration: 376000, loss: 0.0001037941480867093\n",
            "Iteration: 378000, loss: 7.4930352930801645\n",
            "Iteration: 380000, loss: 0.006242146950016167\n",
            "Iteration: 382000, loss: 2.634001050489209e-06\n",
            "Iteration: 384000, loss: -9.995262395035084e-10\n",
            "Iteration: 386000, loss: 7.710986300494114e-10\n",
            "Iteration: 388000, loss: 1.992870064279686e-07\n",
            "Iteration: 390000, loss: -6.386789985367889e-10\n",
            "Iteration: 392000, loss: 25.585244045188627\n",
            "Iteration: 394000, loss: 0.08951566168515128\n",
            "Iteration: 396000, loss: 7.859504976372067e-07\n",
            "Iteration: 398000, loss: 0.00023493431502695332\n",
            "Iteration: 400000, loss: -9.924321364214538e-10\n",
            "Iteration: 402000, loss: 0.0003422003824904932\n",
            "Iteration: 404000, loss: 1.6600312779068248e-07\n",
            "Iteration: 406000, loss: 0.0024676709786146046\n",
            "Iteration: 408000, loss: 25.306820525085456\n",
            "Iteration: 410000, loss: 1.1010085624445299e-07\n",
            "Iteration: 412000, loss: 0.0020877473386801257\n",
            "Iteration: 414000, loss: -9.623120078107695e-10\n",
            "Iteration: 416000, loss: 0.011041917040239841\n",
            "Iteration: 418000, loss: -7.357900954823459e-10\n",
            "Iteration: 420000, loss: 0.00028484376457402975\n",
            "Iteration: 422000, loss: 0.007596875511588892\n",
            "Iteration: 424000, loss: 26.211679073225657\n",
            "Iteration: 426000, loss: 2.104496370597024e-05\n",
            "Iteration: 428000, loss: 0.021967197421413592\n",
            "Iteration: 430000, loss: -7.806705281819723e-10\n",
            "Iteration: 432000, loss: -9.730574123758925e-10\n",
            "Iteration: 434000, loss: -6.714407918130598e-10\n",
            "Iteration: 436000, loss: 3.0043599483415417\n",
            "Iteration: 438000, loss: 3.919078963129374e-05\n",
            "Iteration: 440000, loss: 2.788982825718049e-09\n",
            "Iteration: 442000, loss: 25.41232755446894\n",
            "Iteration: 444000, loss: -7.768824472219465e-10\n",
            "Iteration: 446000, loss: 2.626500693899385e-05\n",
            "Iteration: 448000, loss: 6.44531788393523e-07\n",
            "Iteration: 450000, loss: 0.5224568175030471\n",
            "Iteration: 452000, loss: 0.025051357205250376\n",
            "Iteration: 454000, loss: -1.000000082690371e-09\n",
            "Iteration: 456000, loss: -9.99764493364569e-10\n",
            "Iteration: 458000, loss: 0.004537775355231205\n",
            "Iteration: 460000, loss: 18.900582978930423\n",
            "Iteration: 462000, loss: 2.373221478841447e-08\n",
            "Iteration: 464000, loss: 0.0011379725598545457\n",
            "Iteration: 466000, loss: 5.174404275214815e-05\n",
            "Iteration: 468000, loss: -9.990295257223407e-10\n",
            "Iteration: 470000, loss: 3.9062524924717447e-08\n",
            "Iteration: 472000, loss: 5.393222183337243e-07\n",
            "Iteration: 474000, loss: 0.08204147480885951\n",
            "Iteration: 476000, loss: 1.5073602379703309e-09\n",
            "Iteration: 478000, loss: -7.875774476628213e-10\n",
            "Iteration: 480000, loss: 25.212838511057992\n",
            "Iteration: 482000, loss: 24.487470080740607\n",
            "Iteration: 484000, loss: 0.0027369503728372864\n",
            "Iteration: 486000, loss: 0.0009604436676141387\n",
            "Iteration: 488000, loss: 3.8499824586388256e-06\n",
            "Iteration: 490000, loss: 3.6098157645024423e-06\n",
            "Iteration: 492000, loss: 0.012290909865667134\n",
            "Iteration: 494000, loss: 8.704699647427885e-09\n",
            "Iteration: 496000, loss: 1.003133848233725e-07\n",
            "Iteration: 498000, loss: 24.895613668085318\n",
            "Iteration: 500000, loss: 1.637659716966483e-06\n",
            "Iteration: 502000, loss: -8.349627655557482e-10\n",
            "Iteration: 504000, loss: 0.3221050762729228\n",
            "Iteration: 506000, loss: 0.13111519327501428\n",
            "Iteration: 508000, loss: 5.503225661578835e-06\n",
            "Iteration: 510000, loss: 3.709172566445825e-06\n",
            "Iteration: 512000, loss: 3.3217172191297885e-08\n",
            "Iteration: 514000, loss: 2.4310692612973958e-05\n",
            "Iteration: 516000, loss: 8.167711855308076e-06\n",
            "Iteration: 518000, loss: -4.645653860920028e-10\n",
            "Iteration: 520000, loss: 32.928003174163486\n",
            "Iteration: 522000, loss: 5.3293735448925295e-06\n",
            "Iteration: 524000, loss: 4.7448266504226014e-09\n",
            "Iteration: 526000, loss: 0.18839720421083872\n",
            "Iteration: 528000, loss: 9.727573597991785e-06\n",
            "Iteration: 530000, loss: -6.149025732552079e-10\n",
            "Iteration: 532000, loss: 24.46192113543963\n",
            "Iteration: 534000, loss: -9.796419230897613e-10\n",
            "Iteration: 536000, loss: 0.007913845301922483\n",
            "Iteration: 538000, loss: 25.533353769598286\n",
            "Iteration: 540000, loss: 3.4350399950721817e-06\n",
            "Iteration: 542000, loss: 28.760186025070873\n",
            "Iteration: 544000, loss: 4.2996053413604783e-05\n",
            "Iteration: 546000, loss: 0.00011408554357114181\n",
            "Iteration: 548000, loss: -9.995506644100476e-10\n",
            "Iteration: 550000, loss: -2.659831243040636e-10\n",
            "Iteration: 552000, loss: 0.049912346048525784\n",
            "Iteration: 554000, loss: 3.39724058342936e-07\n",
            "Iteration: 556000, loss: 2.4435029142256576e-05\n",
            "Iteration: 558000, loss: -9.941245604000286e-10\n",
            "Iteration: 560000, loss: -9.5587560084821e-10\n",
            "Iteration: 562000, loss: 0.7738805085715585\n",
            "Iteration: 564000, loss: 0.0028829130983550207\n",
            "Iteration: 566000, loss: 1.0763796333191375e-05\n",
            "Iteration: 568000, loss: 24.24487551889038\n",
            "Iteration: 570000, loss: 5.3900907304491184e-05\n",
            "Iteration: 572000, loss: 25.77093610246945\n",
            "Iteration: 574000, loss: -2.0258916755964749e-10\n",
            "Iteration: 576000, loss: 1.412135187799431e-09\n",
            "Iteration: 578000, loss: 6.185753764215019e-05\n",
            "Iteration: 580000, loss: 3.132668690834086e-06\n",
            "Iteration: 582000, loss: 3.093528835012783e-06\n",
            "Iteration: 584000, loss: -9.999974181551121e-10\n",
            "Iteration: 586000, loss: 26.805049996586416\n",
            "Iteration: 588000, loss: 8.61599831453741e-07\n",
            "Iteration: 590000, loss: 1.6825980347588426e-05\n",
            "Iteration: 592000, loss: 0.00044607896736990286\n",
            "Iteration: 594000, loss: 0.20143418320396558\n",
            "Iteration: 596000, loss: 0.5306503514567507\n",
            "Iteration: 598000, loss: 0.00018479836618654454\n",
            "Iteration: 600000, loss: 25.378129359480756\n",
            "Iteration: 602000, loss: 1.4467834182054624e-07\n",
            "Iteration: 604000, loss: 0.14193698207153743\n",
            "Iteration: 606000, loss: -9.847198611593198e-10\n",
            "Iteration: 608000, loss: 2.973023273289274e-05\n",
            "Iteration: 610000, loss: -9.882001882965948e-10\n",
            "Iteration: 612000, loss: 24.40901584161612\n",
            "Iteration: 614000, loss: 8.306252294724085e-06\n",
            "Iteration: 616000, loss: -9.60820090110302e-10\n",
            "Iteration: 618000, loss: 1.8561130282076912e-08\n",
            "Iteration: 620000, loss: -9.999947536198532e-10\n",
            "Iteration: 622000, loss: -9.830158908613e-10\n",
            "Iteration: 624000, loss: 24.28092761950557\n",
            "Iteration: 626000, loss: 13.57353970662577\n",
            "Iteration: 628000, loss: -9.93881199513056e-10\n",
            "Iteration: 630000, loss: 0.007098169014806657\n",
            "Iteration: 632000, loss: -9.49219813816197e-10\n",
            "Iteration: 634000, loss: 0.15048920906507016\n",
            "Iteration: 636000, loss: 13.291480055191098\n",
            "Iteration: 638000, loss: 4.4632402355616085e-05\n",
            "Iteration: 640000, loss: -6.301529297899374e-10\n",
            "Iteration: 642000, loss: 0.014329745087249622\n",
            "Iteration: 644000, loss: -9.999032712426334e-10\n",
            "Iteration: 646000, loss: 9.498926762047433e-05\n",
            "Iteration: 648000, loss: -9.997931371186016e-10\n",
            "Iteration: 650000, loss: 3.140172481288402e-05\n",
            "Iteration: 652000, loss: -9.996039551152243e-10\n",
            "Iteration: 654000, loss: -9.989442605940578e-10\n",
            "Iteration: 656000, loss: 0.007743789610677437\n",
            "Iteration: 658000, loss: 0.03516220070083563\n",
            "Iteration: 660000, loss: 0.0008361965809770023\n",
            "Iteration: 662000, loss: -9.998664118382196e-10\n",
            "Iteration: 664000, loss: 1.4386846171770788e-09\n",
            "Iteration: 666000, loss: 2.397507587214007e-07\n",
            "Iteration: 668000, loss: 0.003312099606845816\n",
            "Iteration: 670000, loss: 0.9785479051262095\n",
            "Iteration: 672000, loss: 9.814268227014874e-07\n",
            "Iteration: 674000, loss: 9.518947982019807e-05\n",
            "Iteration: 676000, loss: 0.00019055058236468108\n",
            "Iteration: 678000, loss: 1.6398662144349586e-06\n",
            "Iteration: 680000, loss: 3.240554516683924e-08\n",
            "Iteration: 682000, loss: 2.2665936820799757e-08\n",
            "Iteration: 684000, loss: 0.0003775298261264565\n",
            "Iteration: 686000, loss: 0.0002980373370835946\n",
            "Iteration: 688000, loss: 8.859820588042728e-07\n",
            "Iteration: 690000, loss: 1.1798675891949632e-06\n",
            "Iteration: 692000, loss: -9.999936433968288e-10\n",
            "Iteration: 694000, loss: 2.305413946643066e-06\n",
            "Iteration: 696000, loss: -9.174918602204572e-10\n",
            "Iteration: 698000, loss: 0.005638952662482984\n",
            "Iteration: 700000, loss: 0.03091768035620368\n",
            "Iteration: 702000, loss: -9.998912808339685e-10\n",
            "Iteration: 704000, loss: 9.998442649696568e-08\n",
            "Iteration: 706000, loss: -6.064833079735326e-10\n",
            "Iteration: 708000, loss: 0.0035681599794018786\n",
            "Iteration: 710000, loss: 4.439517427254356e-09\n",
            "Iteration: 712000, loss: 2.524558453980996e-09\n",
            "Iteration: 714000, loss: 27.507240384354255\n",
            "Iteration: 716000, loss: 0.0021967564837795123\n",
            "Iteration: 718000, loss: 1.1691784789996214e-08\n",
            "Iteration: 720000, loss: 0.09994816828570766\n",
            "Iteration: 722000, loss: 7.402429986740754e-08\n",
            "Iteration: 724000, loss: -9.889773444137615e-10\n",
            "Iteration: 726000, loss: 1.0811747528261795e-08\n",
            "Iteration: 728000, loss: 24.521286098421786\n",
            "Iteration: 730000, loss: 0.0049788433015793\n",
            "Iteration: 732000, loss: -9.998877281202901e-10\n",
            "Iteration: 734000, loss: 0.005699283438907604\n",
            "Iteration: 736000, loss: 7.093782465455198e-09\n",
            "Iteration: 738000, loss: -9.99021532116564e-10\n",
            "Iteration: 740000, loss: 2.6375752211578065e-08\n",
            "Iteration: 742000, loss: 5.060257818439521e-06\n",
            "Iteration: 744000, loss: -9.268719125103385e-10\n",
            "Iteration: 746000, loss: 0.0001665729970111814\n",
            "Iteration: 748000, loss: 4.0245229696150824e-05\n",
            "Iteration: 750000, loss: -9.999985283781368e-10\n",
            "Iteration: 752000, loss: -9.794365318302154e-10\n",
            "Iteration: 754000, loss: -9.521634591433873e-10\n",
            "Iteration: 756000, loss: 4.199791297238708e-09\n",
            "Iteration: 758000, loss: 24.643724555095492\n",
            "Iteration: 760000, loss: -1.000000082690371e-09\n",
            "Iteration: 762000, loss: 5.2934595828176785e-06\n",
            "Iteration: 764000, loss: -9.970690939056516e-10\n",
            "Iteration: 766000, loss: 7.156341361500495e-07\n",
            "Iteration: 768000, loss: -9.999983063335318e-10\n",
            "Iteration: 770000, loss: 2.189854510616248e-08\n",
            "Iteration: 772000, loss: -9.99991200906175e-10\n",
            "Iteration: 774000, loss: 0.00011190313635791432\n",
            "Iteration: 776000, loss: -9.99997862244322e-10\n",
            "Iteration: 778000, loss: -9.999887584155209e-10\n",
            "Iteration: 780000, loss: -9.999998606457661e-10\n",
            "Iteration: 782000, loss: -9.999943095306436e-10\n",
            "Iteration: 784000, loss: -9.970335667688677e-10\n",
            "Iteration: 786000, loss: 0.09506678032407953\n",
            "Iteration: 788000, loss: 0.001254653042801872\n",
            "Iteration: 790000, loss: -9.952116907856363e-10\n",
            "Iteration: 792000, loss: 2.173003140257512e-07\n",
            "Iteration: 794000, loss: 1.6878965839539024e-07\n",
            "Iteration: 796000, loss: -9.884690843131365e-10\n",
            "Iteration: 798000, loss: 1.6379112037165684\n",
            "Iteration: 800000, loss: 0.010967405321281872\n",
            "Iteration: 802000, loss: -9.999983063335318e-10\n",
            "Iteration: 804000, loss: 7.552293141225841e-09\n",
            "Iteration: 806000, loss: 0.0633334870927267\n",
            "Iteration: 808000, loss: 3.2756527927606785e-06\n",
            "Iteration: 810000, loss: -9.999940874860387e-10\n",
            "Iteration: 812000, loss: -8.219160907048649e-10\n",
            "Iteration: 814000, loss: 24.490103279933727\n",
            "Iteration: 816000, loss: 0.0015493046198744804\n",
            "Iteration: 818000, loss: 1.102896774853206e-09\n",
            "Iteration: 820000, loss: 0.2286921762046612\n",
            "Iteration: 822000, loss: -9.998588623216528e-10\n",
            "Iteration: 824000, loss: 26.074476839209613\n",
            "Iteration: 826000, loss: 2.8238748039843482e-09\n",
            "Iteration: 828000, loss: 4.382836700342303e-07\n",
            "Iteration: 830000, loss: 24.241142265089646\n",
            "Iteration: 832000, loss: 0.00019251710919348413\n",
            "Iteration: 834000, loss: -8.957787844404185e-10\n",
            "Iteration: 836000, loss: 0.007585175441197215\n",
            "Iteration: 838000, loss: -9.936726996290504e-10\n",
            "Iteration: 840000, loss: -7.452731764696602e-10\n",
            "Iteration: 842000, loss: 3.8558596383642944e-07\n",
            "Iteration: 844000, loss: 5.503746015686848e-07\n",
            "Iteration: 846000, loss: 24.40064639841756\n",
            "Iteration: 848000, loss: 0.0006202037294695807\n",
            "Iteration: 850000, loss: 7.714725020712386e-07\n",
            "Iteration: 852000, loss: 9.341175869918378e-05\n",
            "Iteration: 854000, loss: 4.8296869916321735e-09\n",
            "Iteration: 856000, loss: 24.421592252796753\n",
            "Iteration: 858000, loss: 0.00013072216417421646\n",
            "Iteration: 860000, loss: 6.127866482697889e-05\n",
            "Iteration: 862000, loss: 0.09798269786455019\n",
            "Iteration: 864000, loss: -9.998757377116254e-10\n",
            "Iteration: 866000, loss: 1.0701022468957479e-07\n",
            "Iteration: 868000, loss: 0.011713838738448439\n",
            "Iteration: 870000, loss: 0.0002325269704215433\n",
            "Iteration: 872000, loss: -9.772986863741951e-10\n",
            "Iteration: 874000, loss: 8.862664064279707\n",
            "Iteration: 876000, loss: 0.0027023541528926522\n",
            "Iteration: 878000, loss: 1.4821964603158883e-08\n",
            "Iteration: 880000, loss: 1.3431471899829458e-06\n",
            "Iteration: 882000, loss: -9.910618991645942e-10\n",
            "Iteration: 884000, loss: -9.999916449953847e-10\n",
            "Iteration: 886000, loss: -9.715879211806285e-10\n",
            "Iteration: 888000, loss: -9.664256061611524e-10\n",
            "Iteration: 890000, loss: 2.0038299911359473e-06\n",
            "Iteration: 892000, loss: 0.0009680736845492737\n",
            "Iteration: 894000, loss: -8.854603716503837e-10\n",
            "Iteration: 896000, loss: 1.184581809319506e-06\n",
            "Iteration: 898000, loss: 0.004381006938116357\n",
            "Iteration: 900000, loss: 24.849491132477475\n",
            "Iteration: 902000, loss: 24.426518434402354\n",
            "Iteration: 904000, loss: -1.000000082690371e-09\n",
            "Iteration: 906000, loss: 6.717774684031701e-09\n",
            "Iteration: 908000, loss: 9.69287761988155e-10\n",
            "Iteration: 910000, loss: 0.004648180855815115\n",
            "Iteration: 912000, loss: 4.377420654924217e-09\n",
            "Iteration: 914000, loss: 1.988320696572745e-08\n",
            "Iteration: 916000, loss: 1.0255944801093334e-07\n",
            "Iteration: 918000, loss: 9.50400330618338e-06\n",
            "Iteration: 920000, loss: 3.7305725214085644e-10\n",
            "Iteration: 922000, loss: 5.7983395971736735e-09\n",
            "Iteration: 924000, loss: 3.533773277818473e-07\n",
            "Iteration: 926000, loss: 2.2882560315279283e-09\n",
            "Iteration: 928000, loss: 0.06621402710859907\n",
            "Iteration: 930000, loss: -9.998557536971842e-10\n",
            "Iteration: 932000, loss: 0.004649612801884827\n",
            "Iteration: 934000, loss: 2.0596092218491775e-05\n",
            "Iteration: 936000, loss: -6.522183903619941e-10\n",
            "Iteration: 938000, loss: 0.00346612523779552\n",
            "Iteration: 940000, loss: 0.02499306534555957\n",
            "Iteration: 942000, loss: 5.862183365732174e-06\n",
            "Iteration: 944000, loss: 0.00013427509611520415\n",
            "Iteration: 946000, loss: 24.36467840351145\n",
            "Iteration: 948000, loss: 7.998972212513457e-07\n",
            "Iteration: 950000, loss: 5.097519002177805e-07\n",
            "Iteration: 952000, loss: 24.570054874470873\n",
            "Iteration: 954000, loss: 25.078646966317184\n",
            "Iteration: 956000, loss: -9.999991945119515e-10\n",
            "Iteration: 958000, loss: 0.009760382664480453\n",
            "Iteration: 960000, loss: 0.10017711139007993\n",
            "Iteration: 962000, loss: 0.035325539920722744\n",
            "Iteration: 964000, loss: 0.024382964192777946\n",
            "Iteration: 966000, loss: 5.228415482171633e-10\n",
            "Iteration: 968000, loss: 2.93152298034832e-08\n",
            "Iteration: 970000, loss: -9.99961446929118e-10\n",
            "Iteration: 972000, loss: 24.41075147793623\n",
            "Iteration: 974000, loss: 0.0011308735024783195\n",
            "Iteration: 976000, loss: 25.873963191989247\n",
            "Iteration: 978000, loss: -7.000546808551394e-10\n",
            "Iteration: 980000, loss: 24.360107925780433\n",
            "Iteration: 982000, loss: 0.0012282599733814147\n",
            "Iteration: 984000, loss: 24.424920739737075\n",
            "Iteration: 986000, loss: 7.351177170034957e-07\n",
            "Iteration: 988000, loss: 0.0002805420304576668\n",
            "Iteration: 990000, loss: -9.923517562744786e-10\n",
            "Iteration: 992000, loss: 0.005945512623906584\n",
            "Iteration: 994000, loss: -9.961746982371023e-10\n",
            "Iteration: 996000, loss: 0.07622079991611484\n",
            "Iteration: 998000, loss: 5.485339708276169e-05\n",
            "Accuracy of the Model: 84.444444444%\n"
          ]
        }
      ],
      "source": [
        "num_iterations = 1000000\n",
        "i_list_6 = []\n",
        "loss_list_6 = []\n",
        "accuracy_list_6 = []\n",
        "\n",
        "def q6(X_train, Y_train, learning_rate):\n",
        "  \n",
        "  # Layer 1: Convolution layer with 16 out-put channels+flatten+tanh activation. \n",
        "  # Layer 2: 10 output neuron with linear activation. Softmax cross entropy loss\n",
        "  layer_list = [CNN(8, 8, 2, 16, learning_rate, Tanh()), Dense((16*7*7,1), 10, learning_rate), SoftmaxLayer()]\n",
        "  cel = CrossEntropyLayer()\n",
        "  nn = Neural_Network(layer_list, cel)\n",
        "  for i in range(num_iterations) :\n",
        "    size = len(X_train)\n",
        "    index = np.random.randint(low = 0,high = size)\n",
        "    _x = X_train[index:index+1]\n",
        "    _y = Y_train[index].reshape(10,1)\n",
        "    loss = nn.forward(_x, _y)\n",
        "    if(i%2000==0) :\n",
        "      i_list_6.append(i)\n",
        "      loss_list_6.append(loss)\n",
        "      new_layer_list = nn.layer_list\n",
        "      accuracy_list_6.append(100*testing_accuracy3(X_test_6[0:30], Y_test[0:30], new_layer_list))\n",
        "      print(f\"Iteration: {i}, loss: {loss}\")\n",
        "    nn.backward()\n",
        "  return nn.layer_list\n",
        "\n",
        "learning_rate = 0.0001\n",
        "# Call the function to train the neural network\n",
        "final_layer_list = q6(X_train_6, Y_train, learning_rate)\n",
        "# Calculate the accuracy of the trained model on the test data\n",
        "accuracy_6 = testing_accuracy3(X_test_6, Y_test, final_layer_list)\n",
        "# Print the accuracy\n",
        "print(\"Accuracy of the Model: {:.9f}%\".format(accuracy_6 * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04Aj5gNl4n78"
      },
      "source": [
        "## Plot of the Average Loss V/S Number of Iterations for Q.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLzfmp3w4oUG"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Plot the accuracy on the y-axis and the iteration number on the x-axis\n",
        "plt.plot(list(range(1, len(accuracy_list_6)+1)), accuracy_list_6, color='red', linestyle='-')\n",
        "plt.xlabel('Iteration number (in thousands)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}