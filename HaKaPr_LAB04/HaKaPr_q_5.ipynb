{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Unsupervised labeling) This exercise concerns the classdemo.py file shared with you. We saw that EM algorithm learnt the hidden parameters fairly well. This question asks you to classify every point to each coin. Record the error rate of this “classifier”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# library, which provides a progress bar that shows the progress of the loop\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "1. theta_A and theta_B are the bias values of two coins in a Bernoulli distribution\n",
    "2. N is the number of times that a coin is flipped and the outcome (either \"head\" or \"tail\") is recorded i.e number of trials.\n",
    "3. D is the total number of trials or flips of a coin in a Bernoulli distribution\n",
    "4. x[i] represents the number of \"head\" outcomes observed in the i-th trial or flip of a coin.\n",
    "5. z is to store the hidden label that indicates which coin was used to generate the data for each observation. z[i] is 0 if coin A was used for the i-th observation, and 1 if coin B was used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generation(N, D, bias_A, bias_B):\n",
    "    # Initialize arrays to store the generated data\n",
    "    x = np.zeros(N)  # Number of heads\n",
    "    z = np.zeros(N)  # Hidden label (0 for A, 1 for B)\n",
    "\n",
    "    # Loop over the samples and generate data\n",
    "    for i in range(N):\n",
    "        # Randomly choose between coin A and coin B\n",
    "        if np.random.uniform() < 0.5:\n",
    "            # Generate data from coin A and store the number of heads\n",
    "            x[i] = np.random.binomial(D, bias_A)\n",
    "        else:\n",
    "            # Generate data from coin B and store the number of heads\n",
    "            x[i] = np.random.binomial(D, bias_B)\n",
    "            # Set the hidden label to 1 to indicate that coin B was used\n",
    "            z[i] = 1\n",
    "\n",
    "    return x, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Values as per Classdemo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_A = 0.4  # Bias of coin A\n",
    "bias_B = 0.6  # Bias of coin B\n",
    "D = 100  # X's, D represents the number of tosses in each experiment\n",
    "N = 10000  # Total Samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Number of Heads and Hidden Label (0 for A, 1 for B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, z = data_generation(N, D, bias_A, bias_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation maximisation of Bernoulli trails\n",
    "\n",
    "The probability of a Bernoulli distribution in given by  \n",
    "$$p(\\textbf{x}|\\boldsymbol{\\mu}) = \\prod_{i=1}^{784}\\mu_i^{x_i}(1-\\mu_i)^{(1-x_i)}$$\n",
    "$$p(x|\\mu) = \\mu^{x}(1-\\mu)^{(D-x)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E-step\n",
    "\n",
    "Computing the responsibilities using the current parameter values. Probability of coin A or B for each trial, $\\pi_A=\\pi_B=0.5$\n",
    "\n",
    "$$ \\gamma(z_{nA}) = \\frac{\\pi_Ap(x_n|\\mu_A)}{ \\pi_Ap(x_n|\\mu_A) + \\pi_Bp(x_n|\\mu_B) } $$\n",
    "\n",
    "$$ \\gamma(z_{nA}) = \\frac{\\pi_A\\mu_A^{x_n}(1-\\mu_A)^{(D-x_n)}}{ \\pi_A\\mu_A^{x_n}(1-\\mu_A)^{(D-x_n)} + \\pi_B\\mu_B^{x_n}(1-\\mu_B)^{(D-x_n)} } $$\n",
    "\n",
    "$$ \\gamma(z_{nB}) = 1 - \\gamma(z_{nA})$$\n",
    "\n",
    "## M - step\n",
    "\n",
    "Re-estimate the parameters using the current responsibilities  \n",
    "$$\\mu_A^{new}=\\frac{1}{N_A}\\sum_{n=1}^{N}\\gamma(z_{nA})x_n,\\space\\space\\space\\space\\space\\space\\space \\mu_B^{new}=\\frac{1}{N_B}\\sum_{n=1}^{N}\\gamma(z_{nB})x_n, $$  \n",
    "\n",
    "$$\\pi_A^{new}=\\frac{N_A}{N} \\space\\space\\space\\space\\space\\space\\space \\pi_B^{new}=\\frac{N_B}{N}$$  \n",
    "\n",
    "where,  \n",
    "$$N_A=\\sum_{n=1}^N\\gamma(z_{nA}) \\space\\space\\space\\space\\space\\space\\space N_B=\\sum_{n=1}^N\\gamma(z_{nB})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the parameters of two coins A and B that were used to generate a dataset x of length N, where each x_i is the number of heads observed in D tosses of the i-th coin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Returns:\n",
    "\n",
    "# theta_A: float representing the estimated bias of coin A\n",
    "# theta_B: float representing the estimated bias of coin B\n",
    "\n",
    "def Expectation_Maximization_Algorithm(x, D, N, repeatcount):\n",
    "\n",
    "    # INITIALIZE BIAS FOR COIN A AND COIN B\n",
    "    theta_A = 0.51  # beta or uniform choices are ok\n",
    "    theta_B = 0.534\n",
    "    gamma_A = np.zeros(N)\n",
    "    gamma_B = np.zeros(N)\n",
    "    for i in tqdm(range(repeatcount)):\n",
    "        # print(i)\n",
    "\n",
    "        #********************************#\n",
    "        # EStep starts\n",
    "        #********************************#\n",
    "        \n",
    "        # In the E step, we estimate the \"responsibilities\" of each coin\n",
    "        # for generating each experiment. We calculate gamma_A[j] and gamma_B[j]\n",
    "        # for each experiment j as per the formulas given in Bishop's book\n",
    "        \n",
    "        for j in range(N):\n",
    "            gamma_A[j] = 0.5 * np.power(theta_A, x[j]) * np.power(1-theta_A, D-x[j])\n",
    "            gamma_A[j] /= (0.5*np.power(theta_A, x[j]) * np.power(1-theta_A, D-x[j]) + 0.5 * np.power(theta_B, x[j]) * np.power(1-theta_B, D-x[j]))\n",
    "            gamma_B[j] = 1 - gamma_A[j]\n",
    "\n",
    "        # ********************************#\n",
    "        # EStep ends\n",
    "        # ********************************#\n",
    "\n",
    "        # In the M step, we update the estimates of the biases of the coins\n",
    "        # using the estimated responsibilities (gamma_A and gamma_B) calculated\n",
    "        # in the E step. We calculate the new values of theta_A and theta_B\n",
    "        # using the formulas given in Bishop's book\n",
    "\n",
    "        # ********************************#\n",
    "        # MStep starts\n",
    "        # ********************************#\n",
    "        theta_A_numerator = theta_A_denominator = theta_B_numerator = theta_B_denominator = 0\n",
    "        for k in range(N):\n",
    "            theta_A_numerator += gamma_A[k]*x[k]\n",
    "            theta_A_denominator += gamma_A[k]*D\n",
    "            theta_B_numerator += gamma_B[k]*x[k]\n",
    "            theta_B_denominator += gamma_B[k]*D\n",
    "        theta_A = theta_A_numerator/theta_A_denominator\n",
    "        theta_B = theta_B_numerator/theta_B_denominator\n",
    "        # ********************************#\n",
    "        # MStep ends\n",
    "        # ********************************#\n",
    "\n",
    "    return theta_A, theta_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing theta's i.e bias values of two coins in a Bernoulli distribution using EM Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:21<00:00, 12.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Biases of Coin A and B are respectively:  0.4004784920169179 0.6005202165558978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 1000\n",
    "theta_A, theta_B = Expectation_Maximization_Algorithm(x, D, N, num_iterations)\n",
    "print(\"Predicted Biases of Coin A and B are respectively: \", theta_A, theta_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Classifier Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_classifier(x, theta_A, theta_B, N):\n",
    "\n",
    "    # classified_list contains the classification results where 1 represents classification to coin B and 0 represents classification to coin A\n",
    "\n",
    "    classified_list = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        # Calculate the probabilities of the observation belonging to each of the coins\n",
    "        \n",
    "        # probability of getting xi given A\n",
    "        prob_A_D = np.multiply(np.power(theta_A, x[i]), np.power(1-theta_A,D-x[i]))\n",
    "        # probability of getting xi given B\n",
    "        prob_B_D = np.multiply(np.power(theta_B, x[i]), np.power(1-theta_B,D-x[i]))\n",
    "\n",
    "        # Classify the observation as belonging to either coin A or coin B based on the\n",
    "        # probability calculation\n",
    "\n",
    "        if(prob_A_D < prob_B_D):\n",
    "            classified_list[i] = 1\n",
    "\n",
    "    # return the classification results\n",
    "    return classified_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class = bayes_classifier(x, theta_A, theta_B, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_data : original classification of the observed data, where 1 represents classification to coin B and 0 represents classification to coin A\n",
    "# predicted_data: the predicted classification of the observed data, where 1 represents classification to coin B and 0 represents classification to coin A\n",
    "\n",
    "def compute_accuracy(true_data, predicted_data, N):\n",
    "    incorrect_counter = 0\n",
    "    # Loop through the original and predicted classifications and count the number of incorrect\n",
    "    # classifications\n",
    "    for i in range(N):\n",
    "        if(true_data[i]!=predicted_data[i]):\n",
    "            incorrect_counter+=1\n",
    "    # Calculate the accuracy of the classification as a percentage\n",
    "    return (1-(incorrect_counter)/N)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is:  97.78999999999999\n"
     ]
    }
   ],
   "source": [
    "accuracy = compute_accuracy(z, predicted_class, N)\n",
    "print(\"The accuracy is: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
